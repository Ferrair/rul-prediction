{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "from keras import Sequential, Input, Model\n",
    "import pandas as pd\n",
    "from keras.layers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import plot_model\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(666)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "def compute_score(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    return np.sum(np.exp(d[d >= 0] / 10) - 1) + np.sum(np.exp(-1 * d[d < 0] / 13) - 1)\n",
    "\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def compute_penalized_rmse(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    positive = np.maximum(diff, 0)\n",
    "    negative = np.minimum(diff, 0)\n",
    "    return np.sqrt(np.mean(np.square(positive) * (1 + lambda_) + np.square(negative)))\n",
    "\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    positive = K.maximum(diff, 0)\n",
    "    negative = K.minimum(diff, 0)\n",
    "    return K.sqrt(K.mean(K.square(positive) * (1 + lambda_) + K.square(negative)))\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def data_generate(item_unit, time_steps, features_column, upper):\n",
    "    data_matrix = item_unit[features_column].values\n",
    "    label_matrix = item_unit[['RUL']].values\n",
    "    num_cycles = data_matrix.shape[0]\n",
    "\n",
    "    for end in range(int(time_steps / 3), num_cycles):\n",
    "        if end < time_steps:  # 需要进行padding\n",
    "            data_item = np.pad(data_matrix[0: end, :], ((time_steps - end, 0), (0, 0)), 'constant')\n",
    "        else:  # 不需要进行padding\n",
    "            data_item = data_matrix[end - time_steps: end, :]\n",
    "        yield data_item, np.minimum(label_matrix[end], upper)\n",
    "\n",
    "\n",
    "def split(train_df, sequence_length, sequence_cols, upper):\n",
    "    seq_array = []\n",
    "    label_array = []\n",
    "\n",
    "    generator = (list(data_generate(train_df[train_df['id'] == id],\n",
    "                                    sequence_length,\n",
    "                                    sequence_cols, \n",
    "                                    upper))\n",
    "                 for id in train_df['id'].unique())\n",
    "    arr = np.array(list(generator))\n",
    "    for item_unit in arr:\n",
    "        for item_slice in item_unit:\n",
    "            seq_array.append(item_slice[0])\n",
    "            label_array.append(item_slice[1])\n",
    "\n",
    "    return np.array(seq_array), np.array(label_array)\n",
    "\n",
    "\n",
    "def data_handle(train_df, test_df, truth_df):\n",
    "    rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "    train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "    train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    # MinMax normalization (from 0 to 1)\n",
    "    train_df['cycle_norm'] = train_df['cycle']\n",
    "    cols_normalize = train_df.columns.difference(['id', 'cycle', 'RUL'])\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                                 columns=cols_normalize,\n",
    "                                 index=train_df.index)\n",
    "    join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "    train_df = join_df.reindex(columns=train_df.columns)\n",
    "\n",
    "    # Test Data\n",
    "    test_df['cycle_norm'] = test_df['cycle']\n",
    "    norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
    "                                columns=cols_normalize,\n",
    "                                index=test_df.index)\n",
    "    test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "    test_df = test_join_df.reindex(columns=test_df.columns)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    # We use the ground truth dataset to generate labels for the test data.\n",
    "    # generate column max for test data\n",
    "    rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    truth_df.columns = ['more']\n",
    "    truth_df['id'] = truth_df.index + 1\n",
    "    truth_df['max'] = rul['max'] + truth_df['more']\n",
    "    truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "    # generate RUL for test data\n",
    "    test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "    test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "    test_df.drop('max', axis=1, inplace=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def read_data(filename='FD001'):\n",
    "    train_df = pd.read_csv('../datasets/phm/train_' + filename + '.txt', sep=\" \", header=None)\n",
    "    train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                        's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                        's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "    train_df = train_df.sort_values(['id', 'cycle'])\n",
    "\n",
    "    test_df = pd.read_csv('../datasets/phm/test_' + filename + '.txt', sep=\" \", header=None)\n",
    "    test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                       's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "    truth_df = pd.read_csv('../datasets/phm/RUL_' + filename + '.txt', sep=\" \", header=None)\n",
    "    truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "    return train_df, test_df, truth_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看RUL分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAD4CAYAAAA5OEWQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAczUlEQVR4nO3df4xdZ33n8ffXkwGmDNsJGzpynLBG2zRawIpDRiErpNUYKgxsdx0iIEFRCzRas1Lo9kfkjY1QCQWUrNwQtqKN1lUoYRsYDCRDCAGXpswiqqYQMwnGpBYupMHjNGmLJ8RkGib2d/+45yY3w52ZO3fuj3Pvfb+k0dx57rlnnvlyzCfPc55zTmQmkiSpHDZ0uwOSJOlZBrMkSSViMEuSVCIGsyRJJWIwS5JUImd0uwMAZ511Vm7evLml+/zpT3/KC1/4wpbucxBYt+ZZu+ZYt+ZZu+aVoXYHDx7858x8ydL2UgTz5s2bue+++1q6z5mZGSYnJ1u6z0Fg3Zpn7Zpj3Zpn7ZpXhtpFxD/Ua3cqW5KkEjGYJUkqEYNZkqQSMZglSSoRg1mSpBIpxapsSZLKaHp2jr0HjnB8foGzx0bYtf18Lr1wU1t/p8EsSVId07Nz7Ln9EAuLpwCYm19gz+2HANoazk5lS5JUx94DR54J5aqFxVPsPXCkrb/XYJYkqY7j8wtram8Vg1mSpDrOHhtZU3urGMySJNWxa/v5jAwPPadtZHiIXdvPb+vvdfGXJEl1VBd4uSpbkqSSuPTCTW0P4qWcypYkqUQMZkmSSmTVYI6IF0TENyPigYg4HBEfKNo/ERE/jIj7i6+tRXtExB9FxNGI+E5EvKrdf4QkSf2ikXPMTwGvzcyTETEMfCMivly8tyszP7dk+zcC5xVfrwZuLr5LkqRVrDpizoqTxY/DxVeu8JEdwCeLz90LjEXExvV3VZKk/heZK2VssVHEEHAQ+GXgjzPz2oj4BPAfqYyo7wF2Z+ZTEXEXcENmfqP47D3AtZl535J97gR2AoyPj180NTXVur8KOHnyJKOjoy3d5yCwbs2zds2xbs2zds0rQ+22bdt2MDMnlrY3dLlUZp4CtkbEGHBHRLwS2AP8I/A8YB9wLfAHQNTbRZ197is+x8TERE5OTjb2lzRoZmaGVu9zEFi35lm75li35lm75pW5dmtalZ2Z88AM8IbMfKSYrn4K+DPg4mKzY8C5NR87Bzjegr5KktT3GlmV/ZJipExEjAC/Cvxd9bxxRARwKfDd4iN3Ar9RrM6+BHg8Mx9pS+8lSeozjUxlbwRuLc4zbwD2Z+ZdEfFXEfESKlPX9wP/vdj+buBNwFHgSeBdre+2JEn9adVgzszvABfWaX/tMtsncPX6uyZJ0uDxzl+SJJWIwSxJUokYzJIklYjBLElSiRjMkiSViMEsSVKJGMySJJWIwSxJUokYzJIklYjBLElSiRjMkiSViMEsSVKJGMySJJWIwSxJUokYzJIklYjBLElSiZzR7Q5IklRG07Nz7D1whOPzC5w9NsKu7edz6YWb2v57DWZJkpaYnp1jz+2HWFg8BcDc/AJ7bj8E0PZwdipbkqQl9h448kwoVy0snmLvgSNt/92rBnNEvCAivhkRD0TE4Yj4QNH+soj424j4fkR8JiKeV7Q/v/j5aPH+5vb+CZIktdbx+YU1tbdSIyPmp4DXZuYFwFbgDRFxCfC/gJsy8zzgBHBVsf1VwInM/GXgpmI7SZJ6xtljI2tqb6VVgzkrThY/DhdfCbwW+FzRfitwafF6R/Ezxfuvi4hoWY8lSWqzXdvPZ2R46DltI8ND7Np+ftt/d0PnmCNiKCLuBx4Dvgr8PTCfmU8XmxwDqmfDNwE/Aijefxz4t63stCRJ7XTphZu4/rItbBobIYBNYyNcf9mWjqzKjsxsfOOIMeAO4PeBPyumq4mIc4G7M3NLRBwGtmfmseK9vwcuzsx/WbKvncBOgPHx8YumpqZa8fc84+TJk4yOjrZ0n4PAujXP2jXHujXP2jWvDLXbtm3bwcycWNq+psulMnM+ImaAS4CxiDijGBWfAxwvNjsGnAsci4gzgF8EflxnX/uAfQATExM5OTm5lq6samZmhlbvcxBYt+ZZu+ZYt+ZZu+aVuXaNrMp+STFSJiJGgF8FHgS+Bryl2OwdwBeK13cWP1O8/1e5lmG5JEkDrJER80bg1ogYohLk+zPzroj4HjAVER8CZoFbiu1vAf5vRBylMlK+og39liSpL60azJn5HeDCOu0/AC6u0/6vwFtb0jtJkgaMd/6SJKlEDGZJkkrEYJYkqUQMZkmSSsTHPkqSVJieneO6Ow8zv7AIwJm/MMz7/8srOnLHryqDWZIkKqG867MPsHj62VtvnHhykV2fewBo/3OYq5zKliQNvOnZOa7Z/9xQrlo8lR15DnOVwSxJGmjTs3Psuf0Qp1a4SWUnnsNcZTBLkgba3gNHWFg8teI2nXgOc5XBLEkaaKuNhoeHoiPPYa4ymCVJA22l0fCZvzDM3rdc0NFV2QazJGmg7dp+PiPDQ89pGxke4qOXb2X291/f0VAGL5eSJA2Y6dk5Hv3HJ3jn7i8RQHXJ14aA0wmbxkbYtf38jgdylSNmSdLAeN/0IX73M/fzs1OngWdDGSqhXD2f3K1QBoNZkjQgpmfnuO3eh1n+oqjOX7Ncj8EsSRoIew8cWTGUqzp5zXI9BrMkqe9Nz84x12DgdvKa5XoMZklSX6ve2asRnb5muR5XZUuS+toHvnh41Tt7QXeeJFWPwSxJ6lvTs3OceHJx2fc/evnWrgfxUqtOZUfEuRHxtYh4MCIOR8RvF+3XRcRcRNxffL2p5jN7IuJoRByJiO3t/AMkSVrOSiusN42NlC6UobER89PANZn57Yh4EXAwIr5avHdTZv5h7cYR8XLgCuAVwNnAX0bEr2Tm6vMIkiS1yGoLvrp9Lnk5qwZzZj4CPFK8fiIiHgRW+k+MHcBUZj4F/DAijgIXA3/Tgv5KkrSi6dk5rrvzMPMLy09hD22IUo6WASJXeP7kz20csRn4OvBK4PeAdwI/Ae6jMqo+EREfA+7NzD8vPnML8OXM/NySfe0EdgKMj49fNDU1td6/5TlOnjzJ6OhoS/c5CKxb86xdc6xb86zdzzs+v8C//PRnK26zIYJNo8HYv3lRh3pV37Zt2w5m5sTS9oYXf0XEKPB54Hcy8ycRcTPwQSp3NPsgcCPwm0DU+fjPpX9m7gP2AUxMTOTk5GSjXWnIzMwMrd7nILBuzbN2zbFuzbN2z3p2lHya1aLto5dvZezx75e2dg1dxxwRw1RC+bbMvB0gMx/NzFOZeRr4UyrT1QDHgHNrPn4OcLx1XZYk6VnV65RXmrquKuuCr1qNrMoO4Bbgwcz8SE37xprN3gx8t3h9J3BFRDw/Il4GnAd8s3VdliTpWXsPHGnoOuWR4aHSLviq1chU9muAXwcORcT9Rdt7gbdHxFYq09QPAe8GyMzDEbEf+B6VFd1XuyJbktQujdzbuiw3D2lEI6uyv0H988Z3r/CZDwMfXke/JElqyNljI8teFhXAlZe8lA9duqWznVoH7/wlSepJ07Nz7D1wZNlQ7qVRci2DWZLUc943fWjZZyv34ii5lk+XkiT1lOnZuWVDGSoLn772d//UyS61lMEsSeopew8cWTaUqxpZEFZWBrMkqac0Erpnj410oCft4TlmSVLprbbQq1avXK+8HINZklRq1Tt7NXITkU1jI+zafn7PrcSuZTBLkkrtA188vGIoD0Vw49su6OkwruU5ZklSaU3PznHiyZXvgX06s29CGRwxS5JKpPZcclDn0YR19PJCr3oMZklSKSw9l9xIKPf6Qq96DGZJUik0+pSoqn5Y6FWPwSxJKoVGLoWCyij5+su29F0gV7n4S5LUddOzc3UfY7jUUERfhzI4YpYkdcH07BzX3XmY+YWVV1zXGh4K9r6lfy6LWo7BLEnqqOnZOXZ99gEWTzeyvKuiVx/h2AyDWZLUUXsPHGk4lDeNjfDXu1/b5h6Vi8EsSeqItU5f9+OlUI0wmCVJbbfW6etBWOS1nFVXZUfEuRHxtYh4MCIOR8RvF+0vjoivRsT3i+9nFu0REX8UEUcj4jsR8ap2/xGSpPKanp3jmv2Nh/LI8FBf3ft6rRoZMT8NXJOZ346IFwEHI+KrwDuBezLzhojYDewGrgXeCJxXfL0auLn4LknqQ8vdRnNDQKPru6rb9utNQ9Zi1WDOzEeAR4rXT0TEg8AmYAcwWWx2KzBDJZh3AJ/MzATujYixiNhY7EeS1EdWuo1mo6E8iAu8VrKmG4xExGbgQuBvgfFq2Bbff6nYbBPwo5qPHSvaJEl9ZrVHMq5meCgGcoHXSqIysG1gw4hR4P8BH87M2yNiPjPHat4/kZlnRsSXgOsz8xtF+z3A/8zMg0v2txPYCTA+Pn7R1NRUa/6iwsmTJxkdHW3pPgeBdWuetWuOdWtet2o3v7DI8fkFTq3hOuR6ztgQbBwbYWxkuEU9a1wZjrtt27YdzMyJpe0NrcqOiGHg88BtmXl70fxodYo6IjYCjxXtx4Bzaz5+DnB86T4zcx+wD2BiYiInJycb/VsaMjMzQ6v3OQisW/OsXXOsW/M6XbvnXu401PR+Arjp8q1dPY9c5uNu1WCOiABuAR7MzI/UvHUn8A7ghuL7F2ra3xMRU1QWfT3u+WVJ6l3N3D5zOQFceclLB3px12oaGTG/Bvh14FBE3F+0vZdKIO+PiKuAh4G3Fu/dDbwJOAo8CbyrpT2WJHXM0sVda1FdaT0UwalMV1w3qJFV2d+AZR/68bo62ydw9Tr7JUnqkvWMkPv9kYyd4J2/JGkA1V57XB3R1l6D3IxBetBEOxnMktSnGh35niquzmk2lA3k1jKYJanPtHKx1nIM4/YxmCWpD3QijKvGRoaZ/f3Xt/33DCqDWZJ6WCcDGSqLu677r6/oyO8aVAazJPWo+YVF9tzT3KVMa+EDJjrLYJakHvXo4//KwuKaHnmwotpV2Z5D7h6DWZJ6wNIp6w0Bv/vK06zxWUTPcBRcXgazJJXMcs83rtXo8yMc+fYeg1mSSuR904e47d6Hnwljry0ePAazJJXE9Ozcc0K5WQ/d8J9b0h91h8EsSR3WyFR1szaNjbRwb+oGg1mSWmC5sK0uslougFsZyiPDQ+zafn4L96huMJglaRmtCNvqIq1WBnC1D+AjFfuRwSxp4DUytdyJsF3N0gVdMzMz/P2Vkx3uhdrNYJbU99ZyTrfTYbuSoQhufNsFjoIHjMEsqa/Ue85wrTIF70pGhoe4/rIthvIAMpgl9bzaMK61NJTLzrtxCQxmST1u6Q05yqbeQjEDWCsxmCX1pE4/7rCWYat2WjWYI+LjwK8Bj2XmK4u264D/BvxTsdl7M/Pu4r09wFXAKeB/ZOaBNvRb0gBZ7bxxuxi26oZGRsyfAD4GfHJJ+02Z+Ye1DRHxcuAK4BXA2cBfRsSvZGZ7HxYqqec1OgJuVyh7b2mVxarBnJlfj4jNDe5vBzCVmU8BP4yIo8DFwN803UNJfak6Cr7i3Cd41+4vtfUcsTfhUC+JbOC/PotgvmvJVPY7gZ8A9wHXZOaJiPgYcG9m/nmx3S3AlzPzc3X2uRPYCTA+Pn7R1NRUC/6cZ508eZLR0dGW7nMQWLfmWbvVzS8s8ujj/8rPTp1+pm18BB5dWOFDTXre0AbGf/EFjI0Mt37nJeEx17wy1G7btm0HM3NiaXuzi79uBj5IZd3DB4Ebgd+kshZiqbrJn5n7gH0AExMTOTk52WRX6puZmaHV+xwE1q15g167pVPRy9+2ckPxVXHNlqe58VBr1qEO2nT0oB9z61Hm2jX1ryEzH62+jog/Be4qfjwGnFuz6TnA8aZ7J6k01vpEpE7etjKAKy95KR+6dEsHfpvUXk0Fc0RszMxHih/fDHy3eH0n8KmI+AiVxV/nAd9cdy8ldcVyC7LKcM2w543Vrxq5XOrTwCRwVkQcA94PTEbEVir/Ph8C3g2QmYcjYj/wPeBp4GpXZEu9aXp2jl2ffYDF092PYS9b0iBpZFX22+s037LC9h8GPryeTknqrunZOa7Z/0DHb2lp8Ere+UtSYbn7Tbfb84Y28NHLtxrGUsFglgZQJ+6k1ehtK2dmZpg0lKVnGMxSH2tkJXUrQ3nQLleS2sFglvrU9Owce24/xMJiZf1lK+LX4JXaz2CW+sByN/dolQBu8jyw1BEGs9SjVnroQ6tD+cpLXmooSx1iMEs9pNPPIPbyJanzDGapBNZ6u8t28E5aUjkYzFKXLHfdcCdC2TtpSeVlMEtdsHTFdCeMDA9x/WVbDGGp5AxmqQM6PVVdHRE7PS31HoNZapNOT1V7jbHUHwxmqYU6fb9pw1jqPwaz1CLzC4vsuad9541dsCUNBoNZaoHp2TmO/XiBhcWhde3H0JVkMEtNqHejj2u2NHf2eHgo2PuWCwxjSYDBLDWkXauqPUcsaSmDWVpFK5/S5FS1pNUYzNIqPvDFw+ta0GUYS1oLg1mq0aopax+TKKlZqwZzRHwc+DXgscx8ZdH2YuAzwGbgIeBtmXkiIgL438CbgCeBd2bmt9vTdak1Wn0jEB+TKGk9GhkxfwL4GPDJmrbdwD2ZeUNE7C5+vhZ4I3Be8fVq4Obiu1QK7VrEtSEq3522lrReqwZzZn49IjYvad4BTBavbwVmqATzDuCTmZnAvRExFhEbM/ORVnVYatb7pg9x270PPxPG6w3lpSE8MzPDb105uc69Shp0UcnQVTaqBPNdNVPZ85k5VvP+icw8MyLuAm7IzG8U7fcA12bmfXX2uRPYCTA+Pn7R1NRUC/6cZ508eZLR0dGW7nMQ9Fvd5hcWOT6/wKnTrbtD9YYINp05wtjI8HPa+612nWLdmmftmleG2m3btu1gZk4sbW/14q+o01b3/xEzcx+wD2BiYiInJydb2pGZmRlavc9B0E91m56dY9dfPMDi6fXdjavWStcd91PtOsm6Nc/aNa/MtWs2mB+tTlFHxEbgsaL9GHBuzXbnAMfX00GpGdOzc1yz/wFONTAj1AhvBCKpU5oN5juBdwA3FN+/UNP+noiYorLo63HPL6vTqjcEaSaUfVCEpG5r5HKpT1NZ6HVWRBwD3k8lkPdHxFXAw8Bbi83vpnKp1FEql0u9qw19luqqd//qRjkillQWjazKfvsyb72uzrYJXL3eTkmrWe9lTwaxpLLyzl8qtaWj4OpUc621hLKBLKnsDGaV1vTsHLs++wCLNUnc7FVPI8NDXH/ZFgNZUukZzCqNdt2VayjCUJbUMwxmdVyrp6dXMjwU7H3LBYaypJ5hMKtlake8QxGcymxo5NvCm3I9h+eTJfUig1nrstyTmarXELcpc5dlGEvqdQazGtKu879rVZ32ro7IvRGIpH5jMOvnrBbC3QhlzxVLGhQGs56zGOuaLU9z41fuf+a9bo2Mazk9LWmQGMwDbD23sGwlp6cl6VkG8wDqZCAvdz7aUbAk1Wcw97FuLdhyxCtJzTOY+8xyly+1K5R9TKIktZbB3Ac6NTXt9LMktZ/B3AM6PSXtKFiSusdgLrF6I+F2hXJ1NDz2+Pf5wZWTbfotkqTVGMwl0o3LlwK48pKX8qFLtwAwM/P9jv1uSdLPM5i7rBth7FS1JJWXwdwlnQhkA1iSes+6gjkiHgKeAE4BT2fmRES8GPgMsBl4CHhbZp5YXzd7W6cXb7l6WpJ6VytGzNsy859rft4N3JOZN0TE7uLna1vwe0qvkQBu9+Itw1iSels7prJ3AJPF61uBGQYgmKdn59hz+yEWFk8BrQ/gpYu0JEn9KTKbj5CI+CFwgkoO/Z/M3BcR85k5VrPNicw8s85ndwI7AcbHxy+amppquh/1nDx5ktHR0ZbucznzC4sc+/EC2eI4ro66nze0gfFffAFjI8Mt3X89naxbv7F2zbFuzbN2zStD7bZt23YwMyeWtq93xPyazDweEb8EfDUi/q7RD2bmPmAfwMTERE5OTq6zK881MzNDq/dZVX/h1lBL9t3tKel21q3fWbvmWLfmWbvmlbl26wrmzDxefH8sIu4ALgYejYiNmflIRGwEHmtBP0uhXSupux3GkqTyaDqYI+KFwIbMfKJ4/XrgD4A7gXcANxTfv9CKjnZa7UKu6nOCW81AliQttZ4R8zhwR0RU9/OpzPxKRHwL2B8RVwEPA29dfzc7633Th7jt3oefOWO83lD2emJJUqOaDubM/AFwQZ32fwFet55OdUMrp6kDuOnyrQawJGnNBv7OX60+b1y9rMlQliQ1Y6CDeemUdbOcqpYktcpABnMrRsne8EOS1A4DE8ytCOPq6mxHxpKkdhmIYJ6enWPXZx9g8fTaJ629pEmS1EkDEcx7DxxZcygbyJKkbuj7YJ6enWNufqGhbT1vLEnqtr4M5unZOR79xyd45+4vNfwZR8iSpDLou2CuXgL1e1tOAxtW3d5AliSVSV8F8/TsXMPXJRvIkqQy6qtg3nvgSEOhvGlshL/e/dq290eSpLVafa63hxxvYJFXALu2n9/+zkiS1IS+Cuazx0ZWfN/7WEuSyq6vgnnX9vMZGR6q+96msRFuunyrl0JJkkqtr84xV0fCew8cAZ7w1pmSpJ7TV8EMlXC+9MJNzMzM8FtXTna7O5IkrUlfTWVLktTrDGZJkkrEYJYkqUQMZkmSSsRgliSpRCJzbc8pbksnIv4J+IcW7/Ys4J9bvM9BYN2aZ+2aY92aZ+2aV4ba/bvMfMnSxlIEcztExH2ZOdHtfvQa69Y8a9cc69Y8a9e8MtfOqWxJkkrEYJYkqUT6OZj3dbsDPcq6Nc/aNce6Nc/aNa+0tevbc8ySJPWifh4xS5LUcwxmSZJKpO+COSLeEBFHIuJoROzudn/KLiIeiohDEXF/RNxXtL04Ir4aEd8vvp/Z7X52W0R8PCIei4jv1rTVrVNU/FFxDH4nIl7VvZ533zK1uy4i5orj7v6IeFPNe3uK2h2JiO3d6XX3RcS5EfG1iHgwIg5HxG8X7R53q1ihdj1x3PVVMEfEEPDHwBuBlwNvj4iXd7dXPWFbZm6tuaZvN3BPZp4H3FP8POg+AbxhSdtydXojcF7xtRO4uUN9LKtP8PO1A7ipOO62ZubdAMW/1yuAVxSf+ZPi3/Ugehq4JjP/A3AJcHVRH4+71S1XO+iB466vghm4GDiamT/IzJ8BU8COLvepF+0Abi1e3wpc2sW+lEJmfh348ZLm5eq0A/hkVtwLjEXExs70tHyWqd1ydgBTmflUZv4QOErl3/XAycxHMvPbxesngAeBTXjcrWqF2i2nVMddvwXzJuBHNT8fY+X/MQQJ/EVEHIyInUXbeGY+ApUDHPilrvWu3Jark8dhY95TTLl+vOZ0ibWrIyI2AxcCf4vH3ZosqR30wHHXb8Ecddq8Hmxlr8nMV1GZBrs6Iv5TtzvUBzwOV3cz8O+BrcAjwI1Fu7VbIiJGgc8Dv5OZP1lp0zpt1u65teuJ467fgvkYcG7Nz+cAx7vUl56QmceL748Bd1CZvnm0OgVWfH+sez0steXq5HG4isx8NDNPZeZp4E95dtrQ2tWIiGEqwXJbZt5eNHvcNaBe7XrluOu3YP4WcF5EvCwinkflZP6dXe5TaUXECyPiRdXXwOuB71Kp2TuKzd4BfKE7PSy95ep0J/AbxSrZS4DHq1OPqlhy7vPNVI47qNTuioh4fkS8jMpCpm92un9lEBEB3AI8mJkfqXnL424Vy9WuV467M7r1i9shM5+OiPcAB4Ah4OOZebjL3SqzceCOyjHMGcCnMvMrEfEtYH9EXAU8DLy1i30shYj4NDAJnBURx4D3AzdQv053A2+isoDkSeBdHe9wiSxTu8mI2EpluvAh4N0AmXk4IvYD36OysvbqzDzVjX6XwGuAXwcORcT9Rdt78bhrxHK1e3svHHfeklOSpBLpt6lsSZJ6msEsSVKJGMySJJWIwSxJUokYzJIklYjBLElSiRjMkiSVyP8HIuGwqhtHhwsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_rul_values = train_df.groupby(['id']).count()['cycle'].values\n",
    "test_rul_values = test_df.groupby(['id']).count()['cycle'].values\n",
    "\n",
    "fig = plt.figure(figsize = (8, 4))\n",
    "plt.grid(True)\n",
    "plt.scatter(list(range(len(test_rul_values))), sorted(test_rul_values))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LGB模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_array = seq_array.reshape((seq_array.shape[0], -1))\n",
    "label_array = label_array[:, 0]\n",
    "train_n = int(seq_array.shape[0] * 0.8)\n",
    "\n",
    "X_train = seq_array[:train_n, :]\n",
    "y_train = label_array[:train_n]\n",
    "\n",
    "X_val = seq_array[train_n:, :]\n",
    "y_val = label_array[train_n:]\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_val = lgb.Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 没有使用\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',  # 设置提升类型\n",
    "    'objective': 'rmse', # 目标函数\n",
    "    'metric': {'rmse'},  # 评估函数\n",
    "    'num_leaves': 31,   # 叶子节点数\n",
    "    'learning_rate': 0.05,  # 学习速率\n",
    "    'feature_fraction': 0.9, # 建树的特征选择比例\n",
    "    'bagging_fraction': 0.8, # 建树的样本采样比例\n",
    "    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n",
    "    'verbose': 1 # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "}\n",
    "def evaluate_lgb(test_df, model, upper, plot_=True, title=None, noise=False, sigma = 0.1):\n",
    "    seq_array_test_last = [test_df[test_df['id'] == id][sequence_cols].values[-sequence_length:] for id in test_df['id'].unique()]\n",
    "    \n",
    "    for id_ in test_df['id'].unique() - 1:\n",
    "        length_ = seq_array_test_last[id_].shape[0]\n",
    "        if noise: \n",
    "            mean_ = seq_array_test_last[id_][:, :-1].mean()\n",
    "            index_ = np.random.randint(0, length_)\n",
    "            noise_ = np.random.normal(loc=mean_, scale=sigma, size=(seq_array_test_last[id_].shape[1] - 1))\n",
    "            seq_array_test_last[id_][index_, :-1] = noise_\n",
    "        \n",
    "        if length_ < sequence_length:\n",
    "            seq_array_test_last[id_] = np.pad(seq_array_test_last[id_], ((sequence_length - length_, 0), (0, 0)), 'constant')\n",
    "        \n",
    "    seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "    \n",
    "    label_array_test_last = test_df.groupby('id')['RUL'].nth(-1).values\n",
    "    seq_array_test_last = seq_array_test_last.reshape((seq_array_test_last.shape[0], -1))\n",
    "    y_pred_test = model.predict(seq_array_test_last)\n",
    "    y_true_test = label_array_test_last\n",
    "\n",
    "    sort_ids = y_true_test.argsort() \n",
    "    y_true_test = y_true_test[sort_ids] \n",
    "    y_true_test_clip = clip(copy.deepcopy(y_true_test), upper)\n",
    "    \n",
    "    y_pred_test = y_pred_test[sort_ids]\n",
    "    y_pred_test = clip(y_pred_test, upper)\n",
    "\n",
    "    print('Score: ', compute_score(y_true_test_clip, y_pred_test))\n",
    "    print('RMSE: ', compute_rmse(y_true_test_clip, y_pred_test))\n",
    "    print('MSE: ', compute_rmse(y_true_test_clip, y_pred_test) ** 2)\n",
    "    print('Penalized RMSE: ', compute_penalized_rmse(y_true_test_clip, y_pred_test))\n",
    "    \n",
    "    if plot_:\n",
    "        fig = plot_result(y_true_test, y_pred_test)\n",
    "        if title:\n",
    "            fig.savefig('./plot/' + title + '.eps', dpi=600, format='eps')\n",
    "    return compute_score(y_true_test_clip, y_pred_test), compute_rmse(y_true_test_clip, y_pred_test), compute_penalized_rmse(y_true_test_clip, y_pred_test)\n",
    "\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=lgb_val,\n",
    "    early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'FD004'\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "evaluate_lgb(test_df, gbm, upper=160, title='FD004')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape:  (21420, 100, 19)\n",
      "Label Shape:  (21420, 1)\n"
     ]
    }
   ],
   "source": [
    "filename = 'FD003'\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "\n",
    "sequence_length = 100\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "if filename == 'FD001':\n",
    "    upper = 130\n",
    "    sequence_cols = [x for x in sequence_cols if x not in ['s1', 's5', 's10', 's16', 's18', 's19', 'setting3']]\n",
    "elif filename == 'FD003':\n",
    "    upper = 130\n",
    "    sequence_cols = [x for x in sequence_cols if x not in ['s1', 's5', 's16', 's18', 's19', 'setting3']]\n",
    "else:\n",
    "    upper = 160\n",
    "    sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "seq_array, label_array = split(train_df, sequence_length, sequence_cols, upper)\n",
    "\n",
    "print('Data Shape: ', seq_array.shape)\n",
    "print('Label Shape: ', label_array.shape)\n",
    "\n",
    "# Save Train Data\n",
    "saved_date_filename = './data/' + filename \n",
    "np.save(saved_date_filename + '-train.npy', seq_array)\n",
    "np.save(saved_date_filename + '-label.npy', label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model定义与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nb_features, sequence_length_):\n",
    "    inp = Input(shape=(sequence_length_, nb_features))\n",
    "\n",
    "    x = Convolution1D(filters=8, kernel_size=3, strides=1, activation='relu', padding='same')(inp)\n",
    "    x = Convolution1D(filters=8, kernel_size=3, strides=1, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Reconstruction \n",
    "    reconstruction = UpSampling1D(size=2)(x)\n",
    "    reconstruction = Convolution1D(filters=8, kernel_size=3, strides=1, activation='relu', padding='same')(\n",
    "        reconstruction)\n",
    "    reconstruction = Convolution1D(filters=8, kernel_size=3, strides=1, activation='relu', padding='same')(\n",
    "        reconstruction)\n",
    "    reconstruction = Convolution1D(filters=nb_features, kernel_size=3, strides=1, activation='softmax', padding='same',\n",
    "                                   name='reconstruction')(reconstruction)\n",
    "\n",
    "    # Output\n",
    "    output = Convolution1D(filters=12, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n",
    "    output = Convolution1D(filters=12, kernel_size=3, strides=2, activation='relu', padding='same')(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = MaxPooling1D(pool_size=2, padding='same')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "\n",
    "    output = Bidirectional(LSTM(units=24, return_sequences=True, activation='relu'))(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = MaxPooling1D(pool_size=2, padding='same')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "\n",
    "    output = GlobalAveragePooling1D()(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = Dense(units=1, name='output')(output)\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[output, reconstruction])\n",
    "    model.compile(optimizer=Adam(epsilon=1e-8),\n",
    "                  loss={'output': score, 'reconstruction': rmse},\n",
    "                  loss_weights={'output': 1, 'reconstruction': 1.}\n",
    "                  )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(seq_array.shape[2], sequence_length)\n",
    "\n",
    "history = model.fit(\n",
    "    x=seq_array,\n",
    "    y={'output': label_array, 'reconstruction': seq_array},\n",
    "    epochs=200,\n",
    "    batch_size=128 * 8,\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min'),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=1e-8),\n",
    "        keras.callbacks.ModelCheckpoint(filepath='./ckpt_opt/' + filename + '.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        # TensorBoard(log_dir='/home/qihang/paper/tensorboard/phm/' + \"{0:%Y-%m-%d %H:%M:%S/}\".format(datetime.now()))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate(test_df, model, upper, plot_=True, title=None, noise=False, sigma = 0.1, debug=True):\n",
    "    seq_array_test_last = [test_df[test_df['id'] == id][sequence_cols].values[-sequence_length:] for id in test_df['id'].unique()]\n",
    "    \n",
    "    for id_ in test_df['id'].unique() - 1:\n",
    "        length_ = seq_array_test_last[id_].shape[0]\n",
    "        if noise and sigma != 0.0: \n",
    "            mean_ = seq_array_test_last[id_][:, :-1].mean()\n",
    "            index_ = np.random.randint(0, length_)\n",
    "            noise_ = np.random.normal(loc=mean_, scale=sigma, size=(seq_array_test_last[id_].shape[1] - 1))\n",
    "            seq_array_test_last[id_][index_, :-1] = noise_\n",
    "        \n",
    "        if length_ < sequence_length:\n",
    "            seq_array_test_last[id_] = np.pad(seq_array_test_last[id_], ((sequence_length - length_, 0), (0, 0)), 'constant')\n",
    "        \n",
    "    seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "    \n",
    "    label_array_test_last = test_df.groupby('id')['RUL'].nth(-1).values\n",
    "     \n",
    "    y_pred_test = model.predict(seq_array_test_last)[0][:, 0]\n",
    "    y_true_test = label_array_test_last\n",
    "\n",
    "    sort_ids = y_true_test.argsort() \n",
    "    y_true_test = y_true_test[sort_ids] \n",
    "    y_true_test_clip = clip(copy.deepcopy(y_true_test), upper)\n",
    "    \n",
    "    y_pred_test = y_pred_test[sort_ids]\n",
    "    y_pred_test = clip(y_pred_test, upper)\n",
    "    \n",
    "    if debug:\n",
    "        print('Score: ', compute_score(y_true_test_clip, y_pred_test))\n",
    "        print('RMSE: ', compute_rmse(y_true_test_clip, y_pred_test))\n",
    "        print('MSE: ', compute_rmse(y_true_test_clip, y_pred_test) ** 2)\n",
    "        print('Penalized RMSE: ', compute_penalized_rmse(y_true_test_clip, y_pred_test))\n",
    "    \n",
    "    if plot_:\n",
    "        fig = plot_result(y_true_test, y_pred_test)\n",
    "        if title:\n",
    "            fig.savefig('./plot/' + title + '.eps', dpi=600, format='eps')\n",
    "    return compute_score(y_true_test_clip, y_pred_test), compute_rmse(y_true_test_clip, y_pred_test), compute_penalized_rmse(y_true_test_clip, y_pred_test)\n",
    "\n",
    "\n",
    "def clip(y, upper):\n",
    "    y[np.where(y > upper)] = upper\n",
    "    return y\n",
    "\n",
    "def plot_result(y_true, y_pred):\n",
    "    fig = plt.figure(figsize = (24, 10))\n",
    "    plt.grid(True)\n",
    "    plt.plot(y_true, 'r', linewidth=8)\n",
    "    plt.scatter(range(len(y_pred)), y_pred, c='b', s=60)\n",
    "    plt.xlabel('# Test unit with increasing RUL', fontsize=54)\n",
    "    plt.xticks(fontsize=36)\n",
    "    plt.ylabel('RUL', fontsize=54)\n",
    "    plt.yticks(fontsize=36)\n",
    "    plt.legend(['Actual RUL', 'Predicted RUL'],  loc='best', fontsize=40)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def load_model_from_disk(filename):\n",
    "    return load_model('./ckpt/'+ filename + '.h5', custom_objects={'rmse': rmse, 'score': score})\n",
    "\n",
    "def load_model_from_disk_opt(filename):\n",
    "    return load_model('./ckpt_opt/'+ filename + '.h5', custom_objects={'rmse': rmse, 'score': score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  694.8213383791173\n",
      "RMSE:  18.554990606845493\n",
      "MSE:  344.28767642012446\n",
      "Penalized RMSE:  20.987260882469876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(694.8213383791173, 18.554990606845493, 20.987260882469876)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FD001\n",
    "sequence_length = 90 \n",
    "lambda_ = 0.8\n",
    "\n",
    "filename = 'FD001'\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols = [x for x in sequence_cols if x not in ['s1','s5','s10','s16','s18','s19','setting3']]\n",
    "\n",
    "model1 = load_model_from_disk_opt('BEST-FD001')\n",
    "evaluate(test_df, model1, upper=125, title='FD001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1814.553630727499\n",
      "RMSE:  19.519902204922023\n",
      "MSE:  381.0265820897196\n",
      "Penalized RMSE:  21.401491818957915\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1814.553630727499, 19.519902204922023, 21.401491818957915)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FD002\n",
    "sequence_length = 90 \n",
    "lambda_ = 0.8\n",
    "\n",
    "filename = 'FD002'\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "model2 = load_model_from_disk_opt('BEST-FD002')\n",
    "evaluate(test_df, model2, upper=160, title='FD002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  459.41961148963566\n",
      "RMSE:  13.496852586200944\n",
      "MSE:  182.16502973363913\n",
      "Penalized RMSE:  15.692520614920113\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(459.41961148963566, 13.496852586200944, 15.692520614920113)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FD003\n",
    "sequence_length = 90 \n",
    "lambda_ = 0.8\n",
    "\n",
    "filename = 'FD003'\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols = [x for x in sequence_cols if x not in ['s1','s5','s16','s18','s19', 'setting3']]\n",
    "\n",
    "model3 = load_model_from_disk_opt('BEST-FD003')\n",
    "evaluate(test_df, model3, upper=125, title='FD003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  2502.3574449473217\n",
      "RMSE:  20.674245614825537\n",
      "MSE:  427.4244317421329\n",
      "Penalized RMSE:  22.83963458619024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2502.3574449473217, 20.674245614825537, 22.83963458619024)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FD004\n",
    "sequence_length = 90 \n",
    "lambda_ = 0.8\n",
    "\n",
    "filename = 'FD004'\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "model4 = load_model_from_disk_opt('BEST-FD004')\n",
    "evaluate(test_df, model4, upper=160, title='FD004')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较三个Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    return np.sum(np.exp(d[d >= 0] / 10) - 1) + np.sum(np.exp(-1 * d[d < 0] / 13) - 1)\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y_true))) \n",
    "\n",
    "def compute_penalized_rmse(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    positive = np.maximum(diff, 0) # loss is bigger\n",
    "    negative = np.minimum(diff, 0) # loss is less\n",
    "    return np.sqrt(np.mean(np.square(positive) * (0.5 + 1) + np.square(negative)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAEJCAYAAACE8x4JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3xN9x/H8dfJJttMaiSoragEMZOYNWpE7JDYtNpS+ZXau5MqSlVbCaGoWSNqRBKqYtSovWesILJk3u/vj0sqJETce0/G9/l4nEfvveee831/c+t+7lnfowghkCRJkiQp7zFSO4AkSZIkSTkji7gkSZIk5VGyiEuSJElSHiWLuCRJkiTlUbKIS5IkSVIeZaJ2gNdRrFgx4ezsrNN1xsfHY2lpqdN1qiG/9ANkX3Kr/NKX/NIPkH3JrXTdl8OHD0cJIYpnNi9PFXFnZ2cOHTqk03WGhobi4eGh03WqIb/0A2Rfcqv80pf80g+QfcmtdN0XRVGuZjVP7k6XJEmSpDxKFnFJkiRJyqNkEZckSZKkPEoWcUmSJEnKo/LUiW1S3qHRaLhx4wbx8fGvvaytrS2nT5/WQyrDk33JffJLPyDnfbG0tKR06dIYGcntuLxOFnFJL6KiolAUhcqVK7/2F0VsbCzW1tZ6SmZYsi+5T37pB+SsLxqNhps3bxIVFUWJEiX0lEwyFPkzTNKL6OhoSpYsKX/pS1IuY2RkRMmSJXn06JHaUSQdkN+wkl6kpaVhamqqdgxJkjJhampKamqq2jHynccpj/kk+BNiUmIM1qYs4pLeKIqidgRJkjIh/23qXnxyPO1WtGPegXkce3TMYO3qtYgrijJcUZRDiqIkKYoS8Mzrboqi7FAU5YGiKPcURfldURRHfWaRJEmSJH2ITYqlzfI2hF0NY2nnpTQp1sRgbet7SzwSmA78+tzr9sBPgDPgBMQCS/ScRZJUExoaSunSpdOfV69endDQUJ224efnx/jx43W6TkPo2bMnGzZsACAgIIDGjRvrpR0PDw9+/vnnbL/f2dmZnTt36iVLTnh5ebFt2za1Y0jPeZT4iFZBrdh3fR8rvFbgU9PHoO3rtYgLIdYJITYA9597PVgI8bsQIkYIkQDMBxrpM4skPeXs7EyhQoWwsrKiZMmS9OvXj7i4OINmOHnypEHHiQ4ICMDY2BgrKytsbGyoVasWmzdvTp9/5coVFEWhTp06GZaLiorCzMyMZ288tHfvXho2bIitrS1FihShUaNGHDx48IV2np0iIyMzzXX8+HGOHTtGx44ddd/pPGjXrl1UqVKFwoUL4+npydWr/w2ZPWbMGMaNG6diOul5Dx4/oMWyFhyOPMzvXX+ne43uBs+QWy4xawqczGyGoiiDgcEAJUuW1PnWS1xcnM7XqYbc1g9bW1tiY2NztGxaWlqOl80OIQSrVq3C09OTyMhIOnfuzMSJE5kyZYrO23ral4SEBIQQeu1XSkoKSUlJmbaRmJhIvXr12L59OxqNhoCAAHr06MHp06exs7NL/xETGxtLREQE1apVA+DXX3/FycmJ5ORk0tLSuHnzJu3bt2f27Nl4eXmRnJzMvn37SE1NJTY2NkM7z8ss17x58/D29k5vPzExUW+ff1pa2mutXwhBQkKCXj+zZ92/fx8vLy/mzZtHmzZtmD59Ot7e3oSEhABQtWpVoqOjCQsLS/+x9SZ/q8TExFz1nZHbvsNe5VHKI/yP+3M1/ipTqk/B/o49oXdCAQP3RQih9wntLvWALObVBB4ATV61HhcXF6Fru3fv1vk61ZDb+nHq1KkcLxsTE6PDJC9ycnISO3bsSH/u7+8v2rVrJ4QQIjo6WvTv3184ODiIt956S4wbN06kpqYKIYRYsmSJaNSokRg1apSws7MTzs7OYuvWrenr+fXXX0WVKlWElZWVKFeunPjxxx/T+7J7925RqlSpTDPY2toKS0tLYWlpKQoXLiwAcfnyZSGEEJs2bRK1atUStra2okGDBuLYsWPp6/jnn3/Eu+++K6ysrES3bt1E9+7dxbhx4zLt89PsT8XHxwtAHDhwQAghxOXLlwUgpk2bJvz9/dPf5+LiIqZPny6cnJxETEyMOHjwoLC1tc3yb/t8O69Srlw5sWfPniyX//jjj0Xp0qWFtbW1qFOnjggPD0+fN2nSJOHt7S169+4trKysRI0aNcTZs2fFzJkzRfHixUXp0qXFn3/+mf5+d3d3MWbMGFGnTh1hY2MjOnToIO7fv58+f+nSpaJs2bKiSJEi6X1++hlFREQINzc3YWtrKxwcHMSHH34okpKSst3P7Fi0aJFo0KBB+vO4uDhhYWEhTp8+nf7awIEDxeTJk9Ofv8m/lTf5N6oPue077GVux94W1X+oLiymW4g/L/z5wnxd9wU4JLKoi6qena4oyttAMPCJEGKPmlkk/fPweHFasEA7LyHhv9fati2U/jggQDs/Kirz5Vet0s6/fj1nma5fv87WrVt59913AfD19cXExIQLFy5w5MgRtm/fnuE4akREBJUrVyYqKorPPvuMAQMGPP0xSokSJdi8eTMxMTEsWbKEkSNHcvTo0VdmiI6OJi4ujri4OD755BOaNGlCqVKl+Oeff+jfvz+LFi3i/v37DBkyhA4dOpCUlERycjKdOnWiT58+PHjwgK5du7J27dps9TktLY0lS5ZgamqKk5NThnk+Pj6sXLmStLQ0Tp8+TWxsLPXr10+fX6lSJYyNjfH19SU4OJiHDx9mq83MxMfHc/nyZSpXrpzle+rWrcvRo0d58OABvXr1omvXriQmJqbP37RpE3369OHhw4e8++67tG7dOn0wk4kTJzJkyJAM61u6dCkLFiwgMjISExMTPv74YwBOnTrFsGHDWLZsGZGRkdy/f58bN26kL2dsbMx3331HVFQUf//9N7t27WLB0/95M2FnZ5fl9OWXX2a6zMmTJ6lVq1b6c0tLSypUqMDJk//tpKxatSrHjhnuzGfpRZGxkXgEenA5+jJbem2hVYVWquZRrYgriuIE7ASmCSGWqZVDKpg6deqEnZ0djRs3xt3dnbFjx3Lnzh2Cg4OZM2cOlpaWlChRgpEjR7Jy5cr05ZycnBg0aFB6Ibt16xZ37twBoF27dlSoUAFFUXB3d6dVq1bs27cv25lWrVrFihUrWLt2LaampixevJghQ4ZQv3799PbMzc3Zv38/+/fvJyUlhREjRmBqaoq3tzd169Z96fr379+PnZ0dFhYW+Pv7ExQU9MKIXaVLl6Zy5crs3LmTwMBA+vbtm2G+jY0Ne/fuRVEUBg0aRPHixenQoUP63+DZdp5OFSpUyDRPdHQ0wEtHHPPx8aFo0aKYmJgwatQokpKSOHv2bPr8Jk2a0Lp1a0xMTOjatSv37t1jzJgxmJqa0qNHD65cuZLeDkCfPn2oVq0alpaWTJs2jdWrV5OWlsaaNWto3749TZs2xdzcnGnTpmUYqMjFxQU3NzdMTExwdnZmyJAhhIWFZZk7Ojo6y2nMmDGZLhMXF4etrW2G154/LGVtbZ2hP5JhXX90HfcAd27E3GBb7200K9dM7Uj6PSauKIrJkzaMAWNFUSyAVKAkEAL8IIT4UZ8ZpNzjZYeIChf+b35s7OMXvtiLFXv58mXKvF6WDRs20KJFiwyv/fvvv6SkpODo+N/VjhqNhjLPrNzBweGZzIUB0o/nBgcHM2XKFM6dO4dGoyEhIYFKlSplK8+RI0cYPnw427dvp3jx4gBcvXqVwMBA5s2bl/6+5ORkIiMjURSFUqVKZbje9/mt6ue5ubmxd+9e4uLiGDBgAHv27KFbt24vvK9v374EBASwb98+wsPDOX/+fIb5VatWJeDJLpIzZ87g4+PDiBEj+O233zK08yp2dnaA9li5hYVFpu+ZNWsWP//8c3qfY2JiiIqKSp9fsmTJ9MeFChWiWLFiGBsbpz8H7efztK1nP0snJydSUlKIiooiMjIywzxLS0uKFi2a/vzcuXN8+umnHDp0iISEBFJTU3FxcXllH1+HlZUVMTEZBwmJiYnJ8G8hNjY2vS+SYV2JvkKzwGbcf3yf7T7baVCmgdqRAP1viY8HHgNjAJ8nj8cDA4HywCRFUeKeTnrOIkkvVaZMGczNzYmKikrfaoqJicmwOzMrSUlJdOnSBX9/f+7cuUN0dDRt27ZN39X+Mvfu3aNz587Mnz8/fbf+0zzjxo3LsBWXkJBAz549cXR05ObNmxnWf+3atWz108rKigULFrBs2TKOHDnywvwuXbqwZcsWypcv/8ofBlWqVMHPz48TJ05kq+1nPd1dfO7cuUzn79mzh6+++orVq1fz8OFDoqOjsbW1zdbfNCvXnznucu3aNUxNTSlWrBiOjo4Z5iUkJHD//n8X1QwbNowqVapw/vx5YmJimDlz5ktzPH92/rPTzJkzM12mevXqGXaVx8fHc/HiRapXr57+2unTpzPscpcM48KDCzRd0pToxGh29d2Vawo46P8Ss8lCCOW5abIQYsqTx1bPTvrMIkmv4ujoSKtWrRg1ahQxMTFoNBouXrz40t2mTyUnJ5OUlETx4sUxMTEhODg40zO0n5eamkqXLl3o3bs33btnvDxl0KBB/Pjjj0RERCCEID4+ni1bthAbG0uDBg0wMTFh7ty5pKamsm7dOg4cOJDtvhYtWpSBAwcyderUF+ZZWloSEhKS6TXVZ86cYdasWenHi69fv85vv/2Gm5tbttt+Vtu2bbP8+8bGxmJiYkLx4sVJTU1l6tSpL2ypvq6goCDOnDlDQkICEydOxNvbG2NjY7y9vdm8eTN79+4lOTmZiRMnotFoMmSxsbHBysqKM2fOsHDhwpe28/Qch8ymsWPHZrpM586dOXHiBGvXriUxMZGpU6dSs2ZNqlSpkv6esLAw2rRp80Z/A+n1nIk6g3uAO49THxPiG4LrW65qR8pADrsqSc9YunQpycnJVKtWDXt7e7y9vbl169Yrl7O2tmbu3Ll069YNe3t7VqxYQYcOHV653I0bN9izZw9z5szJsLV27do1XF1dWbx4McOHD8fe3p633347fTe2mZkZ69atIyAgAHt7e1atWoWXl9dr9XXEiBFs3bqV48ePvzDP1dU102PZ1tbWREREUL9+fSwtLXFzc6NGjRrMmjUr/T1///33C1ufT68jf97gwYNZvnx5plu1rVu3pk2bNlSqVAknJycsLCwy7PLOiT59+jB06FAcHBxITExk7ty5gHYr+IcffqBXr144Ojpib2+fYXCeb7/9lhUrVmBtbc2gQYNe+MGlC8WLF2ft2rWMGzcOe3t7IiIiMpyPcfDgQSwtLalXr57O25Yyd+LuCTwCPEjVpLLbdze1HWqrHelFWZ22nhsneYlZ1nJbP3LzJWaGJPvyaj179hTr16/Xy7ozk1c/Ey8vL7Fly5YMr8lLzPTn6K2jotjXxYTjt47i9L3Tr17gGYa8xCy3DPYiSVIBtWLFCrUj5AnZvYRQenOHIw/TcllLLM0sCekbQsWiFdWOlCW5O12SJEmSnth/Yz/NlzbH1sKWcL/wXF3AQRZxSZIkSQJg77W9tFzWkmKFixHmF0Y5+3JqR3olWcQlSZKkAm/35d20DmpNKetShPmFUda2rNqRskUWcUmSJKlA235xO21XtMXZzplQv1BK2ZRSO1K2ySIuSZIkFVhbz2+lw28dqFy0MqG+oThYObx6oVxEFnFJkiSpQNpwZgOdVnaiRokahPiGUNyyuNqRXpss4pIkSVKB8/vJ3+n6e1fqONZhZ9+dFClURO1IOSKLuCQZQGhoaIYRwKpXr07oy+7okgN+fn6MHz9ep+s0hJ49e7JhwwYAAgICaNy4sV7a8fDwyHQo2aw4Ozuzc+dOvWTRt7lz52Z5tzQJlh9fTo+1PXAr7cb2Ptuxs8i7N5WRRVwqcJydnSlUqBBWVlaULFmSfv36pd+JzFBOnjyJh4eHwdoLCAjA2NgYKysrbGxsqFWrFps3b06ff+XKFRRFoU6dOhmWi4qKwszMDGdn5/TX9u7dS8OGDbG1taVIkSI0atQofVjVZ9t5doqMjMw01/Hjxzl27BgdO3bUfafzoF27dlGlShUKFy6Mp6cnV69ezfK9zs7OlChRIv1v3KrVf/e1Hjx4MEFBQdy9e9cQsfOUgKMB9FnfB3cnd4J7B2NjbqN2pDcii7hUIG3atIm4uDj++ecfDh48yPTp09WOpHcNGjQgLi6O6OhoPvjgA3r06PHCvanj4+Mz3JFsxYoVlCv337WyMTExtG/fno8++ogHDx5w8+ZNJk2ahLm5+QvtPDu99dZbmWZatGgRvXv3znBL1YIqKioKLy8vpk2bxoMHD3B1dX3lGO2rVq1K/xs/e8MdCwsL2rRpw9KlS/UdO0/56fBP9NvYjxblW7C512aszPL+fbdkEZcKtFKlStGmTZv0wvXo0SMGDBiAo6MjpUqVYvz48aSlpQH/7er19/fH3t6ecuXKERwcnL6uJUuWULVqVaytrSlfvjyLFi3Kst1nd9Xa2dmlb01ZWlqiKApXrlwBYPPmzdSuXRs7OzsaNmyY4WYlR44coU6dOlhbW9O9e3cSExOz1WcjIyP69OlDfHz8C/cK79OnD4GBgenPly5dSt++fdOfP71taM+ePTE2NqZQoUK0atWKmjVrZqvt5wUHB+Pu7p7l/E8++YQyZcpgY2ODi4sLe/bsSZ83efJkunbtio+PD9bW1rzzzjucO3eOL774ghIlSlCmTJkX7iR38eJFPDw8sLW1pWPHjjx48CB93rJly3BycqJo0aLMmDEjw3IHDhygQYMG2NnZ4ejoyPDhw0lOTs5Rn7Oybt06qlevTteuXbGwsGDy5MkcO3aMM2fO5Gh9Hh4ebNmyRacZ87L5B+YzZPMQ2lVsxx89/6CwaWG1I+mEHDtdMogR20Zw9PbRbL03LS0NY2Pj126jtkNt5rw357WWuX79Olu3bk2/A5ivry8lS5bkwoULxMfH0759e8qUKcOQIUMAiIiIwNfXl6ioKH766ScGDBjAzZs3URSFEiVKsHnzZsqXL094eDht2rShWrVqNGnS5KUZnt0aHjt2LHv37qVUqVL8888/9O/fn02bNuHq6kpQUBAdOnTg7NmzKIpCp06dGDFiBMOHD2fjxo307NmT0aNHv7LPaWlpLFmyBFNT0xfuF+7j40OTJk348ssvOXfuHLGxsdSvX5/FixcDUKlSJYyNjfH19aVHjx64ublhb2//Wn/zp+Lj47l8+TKVK1fO8j1169Zl4sSJ2Nra8v3339O1a1euXLmChYUFoN2jsnHjRgICAujfvz+tW7dm4MCB3Lx5k4CAAIYMGcLly5fT17d06VLWrVtHjRo16Nu3Lx9//DFBQUGcOnWKYcOGsXXrVurXr8/nn3+efrtVAGNjY7777jtcXV25ceMGbdq0YcGCBYwYMSLT3HZ2WR9jHTNmTKbHq0+ePJnhXuFP77d+8uTJDLcjfdbAgQMRQvDuu+/yzTffZFi+atWqGe5PXpDN2jcL/x3+dKrSiVXeqzAzNlM7ks7ILXGpQOrUqRN2dnY0btwYd3d3xo4dy507dwgODmbOnDlYWlpSokQJRo4cmeF2kE5OTgwaNCi9kN26dYs7d+4A0K5dOypUqICiKLi7u9OqVSv27duX7UyrVq1ixYoVrF27FlNTUxYvXsyQIUOoX79+envm5ubs37+f/fv3k5KSwogRIzA1NcXb25u6deu+dP379+/Hzs4OCwsL/P39CQoKokSJEhneU7p0aSpXrszOnTsJDAzMsBUOYGNjw969e1EUhUGDBlG8eHE6dOiQ/jd4tp2nU2a3NIX/frxYW1tnmdnHx4eiRYtiYmLCqFGjSEpK4uzZs+nzmzRpQuvWrTExMaFr167cu3ePMWPGYGpqSo8ePbhy5UqGH0l9+vShWrVqWFpaMm3aNFavXk1aWhpr1qyhffv2NG3aFHNzc6ZNm4aR0X9fjy4uLri5uWFiYoKzszNDhgx56X3mo6Ojs5yyOuEsLi4OW1vbDK/Z2toSGxub6fuXL1/OiRMnuHr1Kp6enrRu3TpDX62trXn06FGWGQuKmXtm4r/Dn67VurLae3W+KuAgt8QlA3mdLeTY2NiXfrHrwoYNG2jRokWG1/79919SUlJwdHRMf02j0WS4h7WDw38DQRQurN0d9/SkuODgYKZMmcK5c+fQaDQkJCRQqVKlbOU5cuQIw4cPZ/v27RQvrr1W9erVqwQGBjJv3rz09yUnJxMZGYmiKJQqVSrDseTnt6qf5+bmxt69e4mLi2PAgAHs2bOHbt26vfC+vn37EhAQwL59+wgPD39hl3vVqlXT72t+5swZfHx8GDFiBL/99luGdl7l6dZqbGxs+pb182bNmsXPP/+c3ueYmBiioqLS55csWTL9caFChShWrFj6XpxChQoB2s/naVvPfpZOTk6kpKQQFRVFZGRkhnmWlpYULVo0/fm5c+f49NNPOXToEAkJCaSmpuLi4vLKPr4OKysrYmJiMrwWExOT5b+FRo0aERsbS+HChfn8888JDAxkz549vP/++4D27/r8j4KCRAjBlLApTAmbQu93ehPQKQATo/xX8uSWuCQ9UaZMGczNzYmKikrfaoqJieHkyZOvXDYpKYkuXbrg7+/PnTt3iI6Opm3btmhvBfxy9+7do3PnzsyfP5933303Q55x48Zl2IpLSEigZ8+eODo6cvPmzQzrv3btWrb6aWVlxYIFC1i2bBlHjhx5YX6XLl3YsmUL5cuXf+UPgypVquDn55fhZLjserq7+Olx9uft2bOHr776itWrV/Pw4UOio6OxtbXN1t80K9evX09/fO3aNUxNTSlWrBiOjo4Z5iUkJHD//v3058OGDaNKlSqcP3+emJgYZs6c+dIcz5+d/+w0c+bMTJepXr16ht3f8fHxXLx4kerVq2erb4qiZMh0+vTpDLvXCxIhBONCxjElbAp+tf0I7BSYLws4yCIuSekcHR1p1aoVo0aNIiYmBo1Gw8WLF1+62/Sp5ORkkpKSKF68OCYmJgQHB79wUlVmUlNT6dKlC717937hTORBgwbx448/EhERgRCC+Ph4tmzZQmxsLA0aNMDExIS5c+eSmprKunXrOHDgQLb7WrRoUQYOHMjUqVNfmGdpaUlISEim11SfOXOGWbNmpR8vvn79Or/99htubm7ZbvtZbdu2zfLvGxsbi4mJCcWLFyc1NZWpU6e+sKX6uoKCgjhz5gwJCQlMnDgRb29vjI2N8fb2ZvPmzezdu5fk5GQmTpyIRqPJkMXGxgYrKyvOnDnDwoULX9rO82fnPzuNHTs202U6d+7MiRMnWLt2LYmJiUydOpWaNWtmejz82rVr/PXXXyQnJ5OYmMg333xDVFQUjRo1Sn9PWFgYbdq0yeFfKu8SQuC/3Z8v9n7BEJch/NLhF4yNXv8cm7xCFnFJesbSpUtJTk6mWrVq2Nvb4+3tza1bt165nLW1NXPnzqVbt27Y29uzYsUKOnTo8Mrlbty4wZ49e5gzZ06GrbVr167h6urK4sWLGT58OPb29rz99tvpu7HNzMxYt24dAQEB2Nvbs2rVqvST87JrxIgRbN26NcMZ70+5urpmeizb2tqaiIgI6tevj6WlJW5ubtSoUYNZs2alv+fvv/9+Yevz6XXkzxs8eDDLly/PdKu2devWtGnThkqVKuHk5ISFhUWGXd450adPH4YOHYqDgwOJiYnMnTsX0G4F//DDD/Tq1QtHR0fs7e0zDM7z7bffsmLFCqytrRk0aNArL/3KieLFi7N27VrGjRuHvb09ERERGc7HGDp0KEOHDgW0PyqGDRtG2bJlKVWqFNu2bSM4ODj9EEBiYiJbt27F19dX5zlzM43Q8HHwx8zeP5uP6n3EwnYLMVLyeZkTQuSZycXFReja7t27db5ONeS2fpw6dSrHy8bExOgwibpkX16tZ8+eYv369XpZd2YKwmcyd+5c8b///e+ly77Jv1F9eNPvsDRNmhj8x2DBZMSoP0cJjUajm2A5oOvvY+CQyKIu5s+DBJIk5RkrVqxQO0K+89FHH6kdwaDSNGkM3DSQgKMBjG08lunNpheYAYT0up9BUZThiqIcUhQlSVGUgOfmNVcU5YyiKAmKouxWFOXlZ9BIkiRJ0nNSNan03dCXgKMBTPGYUqAKOOj/mHgkMB349dkXFUUpBqwDJgBFgEPAKj1nkSRJkvKRlLQUeq3txYp/VzCz2Uwmuk8sUAUc9HyduBBiHYCiKK5A6WdmeQEnhRC/P5k/GYhSFKWKECJnYwzmwK5Lu0hOSTVUc5IkSZKOJKUm0X1Ndzae3cisVrP4tMGnakcC4JmLGgxCrWPi1YH0CyKFEPGKolx88rpBinhkbCRtgtpidL8KOx0r07iGsyGaLVCEEAXuV7Ek5QXiDa61zw0SUxPpsroLW89vZV6beQyvN1ztSCQlwUcfgY0NtG9vuHbVKuJWwL3nXnsEvDA0kaIog4HBoB2dSZf3YO5tOouAwuNpGuTKYPsZ9Kif9RjOuV1cXJzO70/9JqysrLhx4wa2travXcjT0tKyHGoyr5F9yX3ySz8gZ30RQvDo0SPi4+Nz1XdGdr/DEtMSmXByAocfHubTip9SI6FGrujHo0embNpUh+bN7xIba7jvY7WKeBzw/E1cbYAX/m8UQvwE/ATg6uoqdHkPZg8PD6r/WpLxJyazKOEDbv79NRtHf4qRUd7begwNDTXo/alfJSUlhRs3bnDz5s3XXjYxMTHLYTjzGtmX3Ce/9ANy3hcLCwtq1aqFqampHlLlTHa+w+KS43j/t/c5/PAwv3b8Fb/afgbJ9jKHDkHNmmBmBs2agbW1E6Ghlw32faxWET8JpI9CoCiKJVDhyesG5Vq+OFc6ReA6vR+bjfzxmHeI4GE/Y2lmaego+YqpqWmG+1C/jtDQ0AzDj+Zlsi+5T37pB+SvvrxKTFIM7Va0Y9/1fQR5BdHrnV6q5hECfvgBRo6ECRNg4kTQ8y0fMqXvS8xMFEWxAIwBY0VRLBRFMQHWAzUURenyZP5E4LghT2p7lkMRK659uxofhy/569FqGvzSgLP3LqoRRZIkSXpOdGI0rZa1Yv+N/azsslL1Av74MQ3/oK0AACAASURBVPTrpz0G/t578PHH6mXR9yVm44HHwBjA58nj8UKIe0AXYAbwEKgP9NBzlpcyMlJYNmQ0wb2DuRZ9g2rfuTJ95TY1I0mSJBV4Dx4/oMXSFvxz6x/WdF1D1+pdVc1z7Ro0aQKBgTBpEmzcCC+5fbze6bWICyEmCyGU56bJT+btFEJUEUIUEkJ4CCGu6DNLdrWq0IqN7Q5h9tiJCWfa0mraTDSavH0mpyRJUl50L/4enoGenLh7gg09NtCxSke1IxEdDZGR2uI9eTIYqTw0ez4fGT5n3GuW59qkfTjF9mSHZhxl/L2JvJ8/zmaVJEnKC27H3cYj0INz98+xqecm2lZsq1oWIWDHDu3jmjXh0iXIxv2NDEIW8SwUtyvMpW+C6Gg+m0jrjdScW5+zUWfVjiVJkpTv3Yy5iXuAO1eir7C111ZaVmipWpaEBOjdG1q1gj//1L6Wmy5ukEX8JYyMFDaMGclc1x0olveo93M91p7YpHYsSZKkfOvao2u4B7hzK/YWf/r8iWc5T9WyXL4MDRvCypUwc6a2kOc2sohnw0fve3J4yGEq2FXEe20HPCZPITXNwGPrSZIk5XOXH17GPcCdqIQodvTZQeOyjVXLsmMHuLrC1auwdSt8/jnkxgEoZRHPprK2ZdnZaw8VYn0JUyZT2r8T1+4+UjuWJElSvnAj4QZNA5oSkxTDrr67qF+6vqp5YmOhdGntYC7vvadqlJeSRfw1FLEpxLmvl9DVcj53rIN5+6t6/LH/lNqxJEmS8rTT904z4tgIElMTCekbgstbLqrkiIuDbU+uLPbygsOHoUIFVaJkmyzir8nISGG1/4fMqxtCmskjvLbUZ93pdWrHkiRJypNO3D2BR6AHGqEh1DeUWg61VMlx/jy4uUHnznD7tvY1E7XGNH0Nsojn0PD3mxAx4DA1HavTZXUX/rdtLMkpaWrHkiRJyjOO3j6KR4AHJkYmzKk9h+olqquSY8sWqFtXW7w3bQIHB1Vi5Igs4m/AtVIp/h4cxqA6g/k24gve+qwdFyMfqB1LkiQp1zsUeYhmgc0obFqYML8wyhYuq0qOGTPg/fehfHnt8e8WLVSJkWOyiL8hcxNzfnp/ET62P3HfajdVZtdlzZ7jaseSJEnKtf6+/jfNlzbH1sKW8H7hvF3kbdWypKaCjw/89Rc4O6sWI8dkEdeRZSMGsbhRGBqjRLpua8DHP61UO5IkSVKuE341nFZBrShhWYJwv3Cc7ZwNnuH0aW3RBu0dyAIDoVAhg8fQCVnEdWjge24cGXoYm/g6zLvVk082/49UTarasSRJknKFkMshtFnehtI2pQnzC6OMbRmDZ9iwAerVgyFDQKPRjn2eG6//zi5ZxHWsZnkHbn6xi94VP2Tu4W9pvew9zt+MUjuWJEmSqv688CftVrSjvH15Qn1Decv6LYO2n5am3eru3BmqVoXgYPVvXqIL+aALuY9VITOCes1nScclhF3eS9XvXFke8o/asSRJklSx+dxmOqzsQJViVdjtu5uSViUN2n5CgvaGJdOnQ//+EB4OZQy/E0AvZBHXI7/afvzaeC8oGnxCGjF0wTK1I0mSJBnU+tPr8VrlRc2SNdnVdxfFChczeIZChcDGBhYuhJ9/zl03MHlTsojrWd8WrhwffhjbODcW3etL7c8/ISExRe1YkiRJerfqxCq6/t4Vl7dc2NlnJ0UKFTFo+2vWaG9ioiiwYgUMHZq3j39nRhZxA6jmVJxbX+3g3cSRHLOYS9OfW3An7o7asSRJkvQm6HgQvdb1omGZhmz32Y6tha3B2k5NhdGjoWtX+OIL7Wv5rXg/JYu4gRQyN+GfL2bzXePlnHp0EJefXNh05IDasSRJknTu1yO/0nd9XzycPQjuHYy1ubXB2r5/H9q0ga+/hmHDYP58gzWtClnEDWxE817sG7CPlERTOqxrgt/cX9SOJEmSpDM/HvqRAX8MoFWFVmzuuRlLM0uDtX3+vPb2oeHh2mPfCxaAmZnBmleFLOIqqO1Qm719D1Ek1p3AhwOpPnoYcY+T1Y4lSZL0RuZGzGXYlmG0r9SeDT02UMjUsCOovPUWVKsGe/bAgAEGbVo1soirpGLpotz6Jpj6KaM5VfhH3vrckyMXbqkdS5IkKUe+3fctn2z7hM5VOrO221osTAxzCnhqKnz1lfY2opaW2puZ1KtnkKZzBVnEVWRmasz+6V8yotQqYi2P0WylC/uu71M7liRJ0muZET6D/+34H92rd2eV9yrMjA2zD/vuXWjZEsaMgbVrDdJkriOLeC7w3cBuhPTaT1GbwngEeDBly49oNELtWJIkSS8lhGDS7kmM3z2ePjX7EOQVhKmxqUHaPnRIe/x7/35YuhR8fQ3SbK6jahFXFMVZUZStiqI8VBTltqIo8xVFyQO3Ydc9z+o1ODjoIA1KtmTyoWFUGT2Q6LhEtWNJkiRlSgjB57s+Z2r4VPrX7s+SjkswMTLM1/cff0DjxtrLxv76C/r0MUizuZLaW+ILgLuAI1AbcAc+UDWRiuwL2bNrwCaaaCZw3upXSk9oSsTp62rHkiRJykAIwajto/jqr68Y6jKUxR0WY2xkbLD2a9WCjh3h8GGoU8dgzeZKahfxcsBqIUSiEOI2sA2ornImVZkYGxE+ZSpjnNcTX+gMDQJcmbsxXO1YkiRJAGiEho+CP+K7/d/xcb2PWdBuAUaK/kvJ7dswcaL2zmNOTrBqFRQz/AiuuY4ihHrHXhVFGQo0BIYC9sCfwAQhxPpn3jMYGAxQsmRJl5UrdXuf7ri4OKysrHS6Tl3Zf+EuE05OIM32Eh++PRSvUl4oWQw7lJv78bpkX3Kn/NKX/NIPMHxfNELD7HOz2XJ7C91Ld2dI+SFZfie9rpf15eRJGyZNqk5cnAkLF/5DuXLxOmlTX3T9uXh6eh4WQrhmOlMIodoEVAUOA6mAAAJ48sMis8nFxUXo2u7du3W+Tl26fveR6LCio2AyomNAHxEVnZDp+3J7P16H7EvulF/6kl/6IYRh+5Kalip81/sKJiPG7xovNBqNTtefVV8WLRLC1FSI8uWFOHZMp03qja4/F+CQyKIuqrY7XVEUI7Rb3usAS6AY2q3xr9TKlBuVLm7D+h7rmNh4KhsvB1FmciP2nriidixJkgqQVE0qfdb3IfBYIFM9pjKt2TSdbYG/zJgxMGQINGsGBw9CzZp6bzLPUfOYeBGgDDBfCJEkhLgPLAHaqpgpVzJSjJjSfAKTKm7iscUlmga58vWaXWrHkiSpAEhJS6HHmh78duI3vmz+JRPcJxis7bZtYdw47QAuRQx7A7Q8Q7UiLoSIAi4DwxRFMVEUxQ7wBY6plSm3m9y7Hdu9D2KW7MDof1vRfua38npySZL0Jik1Ce/fvVl7ei2zW81mdOPRem9zzx6YNUv7uGlTmD4djA134nueo/bZ6V7Ae8A94ALaY+MjVU2Uy7V0qcjlcfspFevFlpT/0XNtT+KTc/dJHpIk5T2PUx7TeVVn/jj7Bz+0/YGRDfT71SyE9o5jzZrB4sWQkKDX5vINVYu4EOKoEMJDCGEvhCgmhOgqhLirZqa8wLGoFde+Xc3UJl+y5vTv1F3UgH+uRKkdS5KkfCIhJYEOKzuw7cI2Fr+/mA/q6nf4jseP4csvq/DRR9rbiEZEQOHCem0y31B7S1zKISMjhQnNRhPcO5gLd28w6uwgpq/cpnYsSZLyuLjkONoub0vI5RCWdFzCwDoD9dqeRgPNm8P27Q5MngwbNoCtrV6bzFdkEc/jWlVoxXbvQ5gllGHCmba0mjZTHieXJClHYpJieC/oPfZe20tQ5yB8a+t/QHIjIxg0CGbM+JdJk7TPpeyTf658wKNWeVa2/Ban2J7s0IyjjL83kfdj1Y4lSVIe8vDxQ1oua0nEzQhWea+i5zs99daWEDB7NqxZo33erx80bHhfb+3lZ7KI5xP2VmZc+iaIDuazibTeSLMV9TkbdVbtWJIk5QH3E+7TfGlzjtw6wtpua+lSrYve2oqPh169YNQo2LRJb80UGLKI5yNGRgobx4xkW+8d3E+8R73F9Zj+u/xXIklS1u7G38Uz0JNT906xscdGOlTuoLe2Ll2Chg21457PnAkBAXprqsCQRTwfal3Jk8ODD2OVXJEJpzrgMWUyqWkatWNJkpTL3Iq9hUeABxceXGBzr820qdhGb21FRmrv/33tGmzdCp9/rr2VqPRmZBHPp8raluXfkXuoEOtLGFMo7d+Ra3cfqR1LkqRc4mbMTTwCPbj26BrBvYNpUb6FXtt76y3tMKqHDsF77+m1qQJFFvF8rIhNIc59vYSulvO5Y72Nt7+qx6b9p9SOJUmSyq5GX6VpQFNuxd7iT58/cXd210s7cXHQpw8cOaJ9/tlnUKGCXpoqsGQRz+eMjBRW+3/IvLohaEwf0WNXfdadXqd2LEmSVHLp4SWaBjTlfsJ9dvbdSaOyjfTSzvnz4OYGK1bA4cN6aUJCFvECY/j7Tbg0+jDvlKxOl9Vd6DR/LMkpaWrHkiTJgM7fP0/TJU2JS44jxDeEeqXq6aWdLVugbl24fRu2b4eB+h0vpkCTRbwAKWtfijC/MNqVHMzG+1/w1mftuBj5QO1YkiQZwOl7p2ka0JTktGR2++6mjmMdvbSzfTu8/z6UL6/dAm/eXC/NSE/IIl7AmJuYs3noInrbLuK+dQhVZtdlzZ7jaseSJEmP/r3zL+4B2uPeoX6h1Cypvxtze3pqLx/76y9wctJbM9ITsogXUEEjBrOoQRgao0S6bmvAyF9Wqh1JkiQ9OHLrCJ6BnpgZmxHmF0a14tV03saZM9CqFdy9C6am2rPQCxXSeTNSJl5ZxBVFMVYU5RtDhJEMa3CbBhwZehibhDrMudET/+3+pGpS1Y4lSZKOHLh5gGZLm2FpZkmYXxiVilbSeRsbNkC9enDsmPYacMmwXlnEhRBpgIuiyMvy86Oa5R24+80uPqz7IbP+nsW7s1pz5pq8rakk5XX7ru+jxdIW2FvYE+4XToUiur22Ky0Nxo+Hzp2hShXt8W9XV502IWVDdnenHwE2KorSR1EUr6eTPoNJhmNuYsb8tvP53nMJJ2L+osZcV5aH/KN2LEmScij8ajitlrXCwcqB8H7hONnp/uD0tGkwYwb07w/h4VC6tM6bkLIhu0W8CHAfaAa8/2Rqr69Qkjo+bupHoPteUDT4hDRi6IJlakeSJOk17bq0i/eC3qOsbVnC/MIobaPb6iqe3On4o49gyRL4+WewsNBpE9JryFYRF0L0y2Tqr+9wkuH1beHK8eGHsItzY9G9vrw79hNS0lLUjiVJUjZsu7CN9r+15+0ibxPqF4qjtaNO1796NbRpA8nJULQo+PnJ8c/Vlq0irihKaUVR1iuKcldRlDuKoqxVFEXuPMmnqjmV4NbXO6iTNJKj5nNpsawFd+LuqB1LkqSX2HR2Ex1XdqRKsSqE+IZQwrKEztadmgqjR0P37hAbq52k3CG7u9OXAH8AbwGlgE1PXpPyKQszEw7PnM1yr+UcvHmQGvNcWLL9gNqxJEnKxLrT6/Ba7UWtkrUI6RtCscLFdLbu+/e1W99ffw3DhsHu3dqtcCl3yG4RLy6EWCKESH0yBQDF9ZhLyiV6vdOLvf32EfPQlP57muA39xe1I0mS9IyVJ1bS7fdu1CtVjx19dmBfyF6n6+/RQ3vi2s8/w4IFYGam09VLbyi7RTxKURSfJ9eMGyuK4oP2RDepAKjzVm1OfHyIIrHuBD4cSPXRQ4lNSFI7liQVeH/e/pPe63rTqGwjtvXehq2Frc7WrdFo//vdd7BnDwwYoLNVSzqU3SLeH+gG3AZuAd5PXntjiqL0UBTltKIo8YqiXFQUpYku1ivpVsXSRbn1TTD1U0ZzqvAiSo3z5PztSLVjSVKB9cs/v/DV2a/wdPZka6+tWJtb62S9KSkwciQMHap9XqOGdjAXKXfK1ohtQBchRAchRHEhRAkhRCchxNU3bVxRlJbAV0A/wBpoClx60/VK+mFmasz+6V8yotQqHtscp+lyF/669pfasSSpwFl4cCEDNw3E1d6VTT03YWlmqZP13r2rHT51zhwoXPi/rXEp98ruiG0d9dT+FGCqEGK/EEIjhLgphLipp7YkHfluYDeOfLgfS1NLPAM96fXdQjQaoXYsSSoQvt//PR9s/YD3K73P9BrTKWSqm0HKDx4EFxfYvx+WLdMWciN5d41cTxHi1V++iqLMAGyBVUD809eFEDke1uvJFv5jYCIwELAANgD/E0I8fuZ9g4HBACVLlnRZuVK3N+qIi4vDyspKp+tUgxr9iE2JZdC2OdyxCaH03R4sfL8/Vhamb7ze/PKZgOxLbpSX+/Hbtd/46fJPNC3WlPFVx5OUkKSTvjx+bESvXm6Ym2uYOvUElSrF6SDt68nLn8vzdN0XT0/Pw0KIzAe1FUK8cgJ2ZzKFZGfZl6zzLUAAhwBHoBjwFzAjq2VcXFyEru3evVvn61SDWv1ISU0TTSZOEExGWI6oK/afuvbG68wvn4kQsi+5UV7tx9TQqYLJiB5reoiUtBQhxJv3JSVFCI1G+zg0VIh7994w5BvIq59LZnTdF+CQyKIuZueYuBGwUAjh+dzU7A1/XDzd2p4nhLglhIgCZgNt33C9kgGZGBsRPmUqY5zXE1/oDA0CXAgIDVM7liTlG0IIJoRMYGLoRPrU7ENQ5yBMjEzeeL23b4OHB/zwg/a5uzsU093l5ZKBZOeYuAYYruuGhRAPgRtot8alPO4L305s7hSBtUkRBoY35/v93z/d4yJJUg4JIRi9czTT90xnwLsDWNJxCcZGxm+83r//hjp14MgRKC5H/MjTsnvawg5FUfwVRSmjKEqRp5MO2l8CfKQoSglFUeyBEcBmHaxXUkG7elW5PvEA7Su1Z8SfI6gxsS9RjxLUjiVJeZIQgpF/juSbfd8wzHUYP73/k04K+E8/abe6CxXSFvPu3XUQVlLN61wn/iEQDhx+Mh3SQfvTgIPAOeA02lueztDBeiWV2JjbsK77OroVm8op4+WUndyYvSeuqB1LkvIUjdDw4dYP+T7ie0bUH8EPbX/ASHnzU8WPH9de/928ufZs9Jo1dRBWUlV272JWLpOp/Js2LoRIEUJ8IISwE0I4CCE+FkIkvul6JXUZKUas+nACkypu4rHFJZoGufL1mp1qx5KkPCFNk8bgTYNZeGghoxuNZnbr2ShveKuwx0/OQKpZE3btgs2boYgu9qVKqntpEVcU5bNnHnd9bt5MfYWS8ofJvdux3fsgZskOjP63Nb1++EYeJ5ekl0jVpNJvYz9+OfILE5tO5IvmX7xxAQ8Ph7ffhpAQ7XNPTzB+873yUi7xqi3xHs88/vy5ee/pOIuUD7V0qciV8fspn+zFb1Gf0WNtD+KT41+9oCQVMClpKfis82HZ8WVM85zGFM8pb1TAhYD587W7zq2swMFBh2GlXONVRVzJ4nFmzyUpUw5FrLgwczVfNv+SNafW4DTNjV1HL6gdS5JyjeS0ZLqv6c6qk6v4usXXjG86/o3W9/gx+PnBRx9pbyN64ABUq6abrFLu8qoiLrJ4nNlzScqSoiiMbjyan5oG8yD1Ji1X1WXaymC1Y0mS6pJSk+iyugvrz6xnTus5/K/R/954nb/9BkuXwpQpsGED2Oru5mZSLvOqIl5LUZQYRVFigZpPHj99/o4B8kn5zACPVoT0OIR5ohMTz7Sj5bQZpMm7LEgF1OOUx3Rc2ZHN5zazsN1CPnH75I3WFxOj/W+/fhARARMnyvHP87uXfrxCCGMhhI0QwloIYfLk8dPnbz5ItlQgedQqz/XJ+3CK7clOzXgqjvMmNilW7ViSZFDxyfG0/6092y9u55cOvzDUdWiO1yUEzJ6tPYHt0iVQFHn70IJC/kaTVFHMtjCXvgmio8VsrhX6g/o/1+ds1Fm1Y0mSQcQmxdJmeRtCr4SytPNS+r/bP8frio+HXr1g1Cho0kSOwFbQyCIuqcbISGHD6JHs6LODewn3qDW/HuOW/aF2LEnSq0eJj2gd1Jp91/exwmsFPjV9cryuyEgLGjaEVatg5kxYswasrXUYVsr1ZBGXVOdZzpO/+h7G+FFFZl7qyCcb1pOaJo+TS/nPg8cPaLGsBYciD7G662q613izMU9XrizL9euwdSt8/rl2N7pUsMgiLuUKlUqW5fqUPVSI9eW4/VxK+Xfk6p1otWNJks5EJUTRfGlzjt85zrru6/Cq6pWj9QgBUVHaxx98cIHDh+E9OWpHgSWLuJRrFLEpxLmvl+AeP5G71tuo9G09jt8+qXYsSXpjd+Lu4BnoyZmoM/zR4w/aV2qfo/XExkLXrtobmCQkgIWFhnLldBxWylNkEZdyFSMjhcltPZlXN4RCtjE0/LU+a0+tVTuWJOVYZGwkHoEeXHp4ic09N9P67dY5Ws/58+DmBuvXQ//+2ruQSZIs4lKuNPz9Jpz8+DA1StTA+3dvGk4YS3JKmtqxJOm1XH90HfcAd27E3GBb7200L988R+vZvBnq1oU7d2D7du2Z6PL4twSyiEu5WCmbUoT5hVE9cTB/m3zBW5+142LkA7VjSVK2XIm+gnuAO3fj77LdZztNnJrkaD0aDUydCuXLw+HD2rHQJekpWcSlXM3cxJwTXyyit+0i7luHUGW2K7/vOaZ2LEl6qYsPLuIe4M7DxIfs7LOTBmUavPY6Hj3SjsBmZAR//AF//QVOTnoIK+VpsohLeULQiMEsahCGxiiJbn82YEH4SrUjSVKmzkadpWlAU+KT49ntu5u6peq+9jpOn9aOuNb/yRgwDg7yGLiUOVnEpTxjcJsGHBl6mLcLu/Dh7p74b/cnVZOqdixJSnfy7kncA9xJ1aQS6hdKbYfar72O9eu1BTw6Gj7+WA8hpXxFFnEpT6lZ3oGTo3fxYd0PmfX3LBz8W3P2epTasSSJY7eP4RHogZFiRKhvKDVK1Hit5dPSYPx48PLS3jb08GFo2lRPYaV8QxZxKc8xMzZjftv5DHNcwn3Lv6g+14XlIf+oHUsqwP659Q/NljbDwsSCML8wqhav+trriIqCn3+GgQMhPBxKl9ZDUCnfkUVcyrMWDPYj0H0vIPAJacTgH5aqHUkqgCJuRNAssBnWZtaE+4VTsWjF11r+wgXtVnjJknD0KCxeDObmegor5TuyiEt5Wt8Wrhwffgi7ODcWR/ny/oKPSUlLUTuWVED8de0vWi5rSdHCRQnzC6Oc/esNn7Z6NdSqBV99pX3u4KCHkFK+Jou4lOdVcyrBra930NxyJJvvzaPFshbcjr2jdiwpnwu9EkrroNY4WjsS7heOk132r/9KTYXRo6F7d6hdG/r102NQKV/LFUVcUZSKiqIkKooSpHYWKW+yMDNhp/9slnst58CNg5Sd4cKS7QfUjiXlUzsv7aTt8rY42TkR5hdGKZtS2V72/n1o0wa+/hqGDYPdu8HRUY9hpXwtVxRx4AfgoNohpLyv1zu9CPLch0g1pf+eJvjN/UXtSFI+s/X8VtqvaE/FohUJ9Q3Fwer19oFfvAgHDsAvv8CCBWBmpqegUoGgehFXFKUHEA3sUjuLlD90aVSbUyMOUSTWncCHA6n22VBi4pPUjiXlAxvPbKTTyk5UL1GdkL4hFLcsnu1ljxzR/rdePbhy5b+BXCTpTShCCPUaVxQb4BDQHBgAvC2E8HnuPYOBwQAlS5Z0WblStyN1xcXFYWVlpdN1qiG/9AN015fk1DRG/LGW00UXUuxxHRZ6fk4x82I6SJh98nPJfXLaj7B7YUw7PY2KVhX5puY3WJlkbx2pqQqLFpVnzZoyfP31MerWffjabWclv3wmIPvyMp6enoeFEK6ZzhRCqDYB3wOjnzyeDAS97P0uLi5C13bv3q3zdaohv/RDCN335X8Bq4TlDEvh8K2DCLu8V6frfhX5ueQ+OenH8uPLhfEUY9Hol0biUeKjbC93544Q7u5CgBCffCJEcvJrN/1S+eUzEUL25WWAQyKLuqja7nRFUWoDLYDv1MogFQxf+3Zj/8D9FDaxxGOJJz1nLUSjUW8PlJS3BB4NxGedD43LNmabzzZszG2ytdzBg+DiAhERsGwZzJkDpqZ6DisVOGoeE/cAnIFriqLcBvyBLoqiyKG3JJ2rUaIGIT0PUiymJSvjPqDK6IFExyWqHUvK5RYfXky/jf1oXr45W3tvxcos+7tIT50CY2PYtw98fF79fknKCTWL+E9ABaD2k+lHYAvQWsVMUj7mVMKeyG830UQzgfNWv1J6QlMiTl9XO5aUS/1w4AcGbx7Me2+/x6aemyhsWviVyyQnw/792se+vnDyJLz7rp6DSgWaakVcCJEghLj9dALigEQhxD21Mkn5n4mxEeFTpjLGeT3xhc7QJMiFsCthaseScpnv/v6O4cHD6VC5A+u7r8fCxOKVy9y6Bc2aaadbt7SvWVrqOahU4Kl+idlTQojJ4rkz0yVJX77w7cTmThGULlaE5kubM+uv7+VxcgmAL/d+yafbP6VL1S783vV3zE1ePZD5339rj38fOQJLlsjBWyTDyTVFXJIMrV29qhz94ADtK7XHf+cIKnzWh6hHCWrHklQ0NWwqn+/6nJ41erLSeyVmxq8eiWXRInB3h0KFtLvSu3c3QFBJekIWcalAszG3YW23dTRTpnLFagVlJzdi74kraseSDEwIwfiQ8UwKnYRvLV+WdV6GiZFJtpY9exZatIBDh+Cdd/QcVJKeI4u4VOAZGxmxa+IEJlXcxGOLyzQNcuXrNTvVjiUZiBCCz3Z8xow9MxhUZxC/dvwVYyPjly5z44b2tqGgHQN90yawtzdAWEl6jizikvTE5N7t2O59ELNkB0b/25rpu795OiiRlE8JIRixbQTf/v0tH9b9kB/b/4iR8vKvxfBw7fHvXr1AowETE+2lZJKkBlnEJekZLV0qcmX8fpqX8mJC4FmD6gAAG0FJREFU+Gf0WNODqEfxaseS9EAjNAzbMoy5B+Yy0m0k89rMe2kBFwLmzoXmzbVb3WvXgpH8BpVUJv8XlKTnOBSxYseg1XzV4it+P7WG0lPc2HX0gtqxJB1K06Qx8I+BLDq8iDGNxjCr1SwURcny/YmJ2uu+P/kE2rbVjsJWtaoBA0tSFmQRl6RMKIrCZ40+Y0bVYJLNb9JyVV2mrQxWO5akA2kiDd8Nviw5uoRJ7pOY2XzmSws4aIdLvXsXpkyB9evB1tZAYSXpFWQRl6SX+LxbK0J6HMI80YmJZ9rRctoM0jQatWNJOZSSlsL009NZ/u9yZjSbwWSPyS8t4KGhEBmpPea9ZQtMnCh3oUu5i/zfUZJewaNWea5P3odTbE92asbTcnEXYpJi1I4lvabktGS6r+lO6L1Qvm35LWObjM3yvULA7Nna499jn7xNnrwm5UayiEtSNhSzLcylb4L4tNpswu9swu1nNw5ePqt2LCmbElMT8Vrlxfoz6/no7Y8Y1XBUlu+Nj9eeeT5qFHTuDPPmGTCoJL0mWcQlKZuMjBRmdR3Jjj47uBl9j3qL6zFu2R9qx5JeISElgY4rO7Ll/BZ+bPcjXqW8snzvtWvQsCGsWgUzZ8Lvv4O1tQHDStJrkkVckl6TZzlPtnU+TOHHFZl5qSPukyeRmiaPk+dG8cnxtF/Rnh0Xd/Brh18Z4jrkpe+3sQELC9i6FT7/HF5xvpskqU4WcUnKgQbVynJ9yh4qxPoRrkyllH9Hrt6JVjuW9IzYpFjeW/4eYVfDWNp5Kf3e7Zfp+8T/27vz8KjK64Hj3xN2CAFxiUUqLigoKEgoIsoSdrGIimDZsSIqxUoRxI0atS0Khv4qWgSFssomSxGLyhLCImtYBAVEQYogGkQwCTuc3x/3xg5xJpmsd2ZyPs9znyT3vnPveedk5sxd5r4KEyY4XyOrXNm5/3m7dkUcrDF5ZEXcmDyqElOOL0ZMoEuFN/m+4ofc+k5DPk/93OuwDHD05FHaTG3Dmv1rmN5pOj1u9j9AYloadOkCDz0Ekyc782zv24QTK+LG5ENUlDBzcH+mtk6CMj9x6zu3Mn7NHK/DKtaOnDhCq8mtSDmYwntd3qNL7S5+2+3eDY0awdy5kJgIDz9cxIEaUwCsiBtTALo3uYOUfilcW7E2fT++n8bDnuX0mXNeh1XspGak0mJSC7Z/v515D8zjnlr3+G23dCn85jfw3Xfw8ccwaJDtgZvwZEXcmAJyRcwVrHgwmVoZ/VhTcjhVn7qLA0dsfPKicij9EPGT4tn1wy4WdF3AXdffFbBttWpwyy2QkuJ8F9yYcGVF3JgCFFOhDDtGjKVnpXH8EJ1E79V/YNaKrV6HFfEOph2k+cTm7D26lw+6fUCba9v8os2xY84AJqpQsyYkJUH16h4Ea0wBsiJuTCGYPPBh3r49GY06TfdltzFj+wyvQ4pY/z32X5r+qykH0g7wYfcPaXF1i1+02bEDGjZ0buDy1VcVPIjSmMJhRdyYQtK3XSPGNRhDw1/H0XVOVx6ZM5iTp896HVZE2fvjXppNbMbh44dZ3HMxTao3+UWbuXOdAn70qHMuvEYNG1rWRA4r4sYUomtjY0jqs5RH6w9g3PZEqj7Vll37D3sdVkT48siXNJvYjGMnj7G011IaVWv0izbDh0OnTnDjjc7576ZNPQjUmEJkRdyYQla6RGnGdBjNQxdP5MeKq6n9ehzTlm3yOqywtvPwTpr+qyknzp4gqXcScVXj/LarVw/69oXkZOdiNmMijWdFXETKiMh4EdknImkisllE7vQqHmMK2zsDejOp2SpA6bHsdh7552SvQwpL27/fTvOJzTmv50nqnUTdy+tesHzbNucObAB33glvv+3cStWYSOTlnnhJYD/QDKgEDANmichVHsZkTKHq1aoB2x5PoXJ6I8al9ubx//yRM+fOeB1W2Nh6aCvxk+KJkiiW91lOncvqXLB81iznBi4JCc5oZMZEOs+KuKpmqGqCqn6tqudVdSGwF/B/XMyYCHHDlZfy7YjF9L9lEG9sGE2zCa3Yvvc7r8MKeRsPbiR+UjzlSpZjxYMrqHVJrZ+XnT0LQ4fCAw84h9DXrYMKdhG6KQZC5py4iMQC1wOfeR2LMYWtbOmSvHl3ItPum8b6bzZQ959xjP9onddhhay136yl5eSWVCpbieQ+ydSoUuPnZefPQ4cOMGIE9O/vfP/7V7/yMFhjipCoqtcxICKlgEXAV6r6SJZl/YB+ALGxsXEzZhTs923T09OJjo4u0HV6IVL6AcWvL0k7DvDXL5/nXPmDtD07jKfb3lFE0eWOV3nZdmwbQ7cN5aJSFzGq7ihiy8b+os3s2dWoUOEs7dsfynF9xe3/K1xYXwKLj49PUdUGfheqqqcTztGAGcB/gFLZtY2Li9OClpSUVODr9EKk9EO1ePZl9zeH9eKBrZUE9IYhj+ix9JOFG1geeJGXZXuWafm/lteao2vqgZ8OXLBsyhTVRYtyv87i+P8VDqwvgQEbNUBd9PRwuogIMB6IBTqpql3hY4qlGldczMERi2h0dig7Koyl5ZR4DqYd9DosT3381ce0f7c9V1e+muQ+yVStWBWAM2dg4EDo2RPGjvU4SGM85vU58THADUAHVT3hcSzGeKp0qRKsefkVJrafxY4jnxI3No5/LVntdVie+OCLD+gwvQM1L65JUu8kYqOdQ+jffw+tW8M//uEU8lmzPA7UGI95+T3x6sAjQD3gkIiku1N3r2IyJhT0/k1n1vZdy+mMaH6/Ip7fJY7h/Hnvr10pKvN3zufemfdy02U3saz3Mi6tcCkAhw5BXJxz5fmUKfD3v0OpUh4Ha4zHvPyK2T5VFVUtq6rRPtM0r2IyJlTUuawOmx7dwKVprZmZ3p9aQ/tyNP2k12EVutmfzabz7M7EVY1jSa8lVClX5edlsbHQtSusXg09engYpDEhxOvD6caYAKrHVubga+/TRIexO3oC1YY1Zd2O/V6HVWimfTqN3835HY2qNeKjHh9RuWxlTp92Rh7btQtEnK+R1a/vdaTGhA4r4saEsJIloliR8BJPXzWP4+V3cuf8OJK/TvY6rAI3cctEes7rSbPqzVjUfRExZWL49lto0QJGjYJFi7yO0JjQZEXcmDAwvPc9bHpsPZdVrELLyS0ZMOX1iDlPPi5lHA/++0FaXdOKhd0WEl06mjVrnPPfmzfDjBnORWzGmF+yIm5MmKhXrRbrH17PrRd14M09T3DtkF4cPnbc67Dy5Y31b/DIwkdof117FnRdQPlS5Vm+HJo1g3LlYO1a51aqxhj/rIgbE0ZiysSQ3H8OLeVlvq44jSsT7mDV9q+9DitPRq0ZxeOLHqdjzY7M7TKXsiWdocYaNYLHH4cNG+CmmzwO0pgQZ0XcmDBTskQUS/78PAnXL+RE2T00ndqAEe8t8TqsXHll1Ss8+fGTdL6xM7M7zyb1UBm6dYOjR51hQxMToUqVnNdjTHFnRdyYMPVCt/Ys7ryB0qcv5+nP2vLaJ69l3so4ZKkqLy5/kWeWPkO3m7rxbqd3WbO6FHFx8P77sH271xEaE16siBsTxlrVv45vX15LpxvvY8jiIcS/2ZXvfgzNgbRVleeWPUdCcgJ96vVhUsfJjHmzJC1bwkUXwfr1cEdojv1iTMiyIm5MmLuoQjSz7p/F87e+SnLqbK56+TaWbfnK67AuoKoMWTyE4auG069+P8bfPZ7XRpbgj3+E9u2du7DdcIPXURoTfqyIGxMBRISX2z3F8Dofcqr0AVrNbMBL00Pjy9WqyhMfPkHimkQG/GYAb/32LaIkil69YORImDcPKlXyOkpjwpMVcWMiyNOdW7O820bKnKzOC7vuos3Lf/P0PPl5Pc+jCx9l9PrRPHnbk9xT5nV69BDOnYOqVWHwYIiydyFj8sxePsZEmKY3X83+hE+ontaVxeefo9OsTqSdSivyOM6dP8dDCx5i3KZxPHPHs1z+6UjatBG2bIHDh4s8HGMikhVxYyLQJZXKs2fkVBJbj2LBrgXcPPpWFm3YVWTbP3v+LL3m92Lilok81/hF9rzzF4YMEe6917mBS2xskYViTESzIm5MhIqKEgY1/hMf9VjM/iOptJ/XkOemLCj07Z45d4Zuc7rx7rZ3+VuLv7H5//7MrJnC8OEwezZUrFjoIRhTbFgRNybCtbwmnpU9Uih/4jr+tqcjzRJe4Oy584WyrVNnT9HlvS7M/nw2iW0SeabJM7zwgjOAydNPOyORGWMKjhVxY4qB2268kv0vrqRGWh9WyEtcMbgjB48cLdBtnDx7kvtm3cf8nfP5bdRovps/CICGDaFt2wLdlDHGZUXcmGKiSkw5do2YwAPRb5Ja6UOaT2vI56mfF8i6j585zt3T72bR7kXc8s1YFv55APv3w7lzBbJ6Y0wAVsSNKUaiooQZT/Yn+cEkfjr1Ew3H3cpTE+fka53pp9O56927WLJnCb9aP4GtE/qRmAjTpkGJEgUUuDHGLyvixhRDTarfQUq/FCocr83IfffTeNiznD6T+93mn079RLup7Vi5byUVl0zl9Po+LF4MgwbZ+W9jioIVcWOKqStirmD3s8nUyujHmpLDqfrUXXx18EjQjz968ihtprRh3YF1TO80nalPdWPjRmjRohCDNsZcwIq4McVYTIUy7Bgxlp6VxvFDdBK1RjXg/Q1bc3zckRNHiP9XK9bv30T/S96jc+3OdOgA1asXQdDGmJ9ZETfGMHngw7x9ezIlypzigY9vY/q26QHbpmak0nhsPFu/3Y7MnE8tOhZhpMYYX1bEjTEA9G3XiK+fSSGuahzd5naj6V+f5OTpsxe0OZR+iLjRzdl1eDeVPnifpHHteewxjwI2xnhbxEWkiojME5EMEdknIt28jMeY4u7y6MtZ2msprWIGsPLsKKo+1ZYd/00FIPVUKreNbc7+tH3USvkP2/7dmqZNPQ7YmGLO6z3xN4HTQCzQHRgjIrW9DcmY4q10idIs/tNo+l4ykR8rruam0Q1IXLiAgVsG8sPpg7xc8yO2zG9OtWpeR2qM8ayIi0gFoBMwTFXTVXUVsADo6VVMxpj/efsPvZncfBWgDE7pyI+nfmJxz8U83+t2ypTxOjpjDIB4NdawiNwCfKKq5XzmDQaaqWoHn3n9gH4AsbGxcTNmzCjQONLT04mOji7QdXohUvoB1pdQs+9wOi8vn8/dVzbm7obXeB1OvkVCTjJZX0JTQfclPj4+RVUb+F2oqp5MQBPgUJZ5DwPLAz0mLi5OC1pSUlKBr9MLkdIPVetLqIqUvkRKP1StL6GqoPsCbNQAddHLc+LpQEyWeTFAmgexGGOMMWHHyyL+BVBSRK7zmVcX+MyjeIwxxpiw4lkRV9UMYC7wkohUEJHbgY7AFK9iMsYYY8KJ118x6w+UA74HpgOPqartiRtjjDFBKOnlxlX1CHCPlzEYY4wx4crrPXFjjDHG5JEVcWOMMSZMWRE3xhhjwpRnd2zLCxFJBfYV8GovAQ4X8Dq9ECn9AOtLqIqUvkRKP8D6EqoKui/VVfVSfwvCqogXBhHZqIFuZxdGIqUfYH0JVZHSl0jpB1hfQlVR9sUOpxtjjDFhyoq4McYYE6asiMM4rwMoIJHSD7C+hKpI6Uuk9AOsL6GqyPpS7M+JG2OMMeHK9sSNMcaYMGVF3BhjjAlTVsSNMcaYMBXxRVxEBojIRhE5JSIT/SxvKSI7ReS4iCSJSPVs1nWV2+a4+5hWhRp8NkQkPct0TkRGB2jbx13u2755EYcckIgsF5GTPrHtyqatiMirIvKDO40QESnKeAMRkTIiMl5E9olImohsFpE7s2kfUnkRkSoiMk9EMtw+dAvQLmRzALnLQ6jlwJ9gXx+hnJdwf7/Kro54XUMivogDB4G/ABOyLhCRS3DGNB8GVAE2AjOzWdd0YDNwMfAc8J6I+L2LTmFT1ejMCYgFTgCzs3nIGt/HqOryIgk0eAN8YquZTbt+OCPf1QVuBn4LPFIUAQahJLAfaAZUwvm/miUiV2XzmFDKy5vAaZz/p+7AGBGp7addKOcAcp+HUMpBIMG8PkI2LxHwfuW3joRCDYn4Iq6qc1V1PvCDn8X3AZ+p6mxVPQkkAHVFpFbWhiJyPVAfeEFVT6jqHGAb0Knwog/a/Thjsq/0OpAi0BtIVNVvVPUAkAj08TYkh6pmqGqCqn6tqudVdSGwF4jzOraciEgFnP/lYaqarqqrgAVATz/NQzYHEN55yKeQzouPsHu/yqaOeF5DIr6I56A2sDXzD1XNAL5y5/tru0dV03zmbQ3Qtqj1BiZr9t8XvEVEDovIFyIyTEQ8HUvej+FufKtzOHR2Qc4InRz8gojEAtcDn2XTLFTycj1wTlW/8JkX6LkNmxxAUHkIlRxkJ5jXR7jkJRLerzJ5XkOKexGPBo5lmXcMqJjPtkVGRK7EOWw4KZtmK4A6wGU4n/q6AkMKP7qgDQWuAa7AuUnC+yJybYC2WfNwDIgOlXN/mUSkFDANmKSqOwM0C6W85Oe1EJI5gKDyEEo5CCTY10fI5yVC3q98eV5DwrqIuxd8aIBpVRCrSAdissyLAdLy2TZfctmvXsAqVd0baH2qukdV97qHFrcBL+Ec0ip0wfRFVdepapqqnlLVScBqoH2AVWbNQwyQnsOn+gIRbF5EJAqYgnN+eUCg9XmZFz/y81ooshzkRjB5CLEc+JWL10c45CWk36/ywPMaEtZFXFWbq6oEmO4IYhWf4VwEAvx8XvBa/B92+wy4RkR8PzXVDdA2X3LZr15k/6nW7yaAIvl0nsccZRffBTmjkHLgTzB9cfd6xuNcvNNJVc/kZhMUUV78+AIoKSLX+cwL9Nx6loNg5SMPXuYgWIFiDPm8EOLvV3ngfQ1R1YiecK5ULQsMx/lUXhYo6S67FOdwRid3/qvA2mzWtRZ4zW17L3AUuNTDvjUGMoCKObS7E4h1f68FbMe5uCIU8lMZaJuZF5yrojOAmgHaPwrswDm0WNV9ATzqdT984nvL/T+JDqJtSOUFmIFz9WwF4Hb3tVE73HKQmzyEWg78xBf06yPU8xLO71eB6kgo1BDPE1sET34Czic53ynBZ3krYCfOVx6WA1f5LHsLeMvn76vcNieAXUArj/s2FpjiZ/6VOIdurnT/fg34zn0B7cE5PFXK69y4sV0KbMA5pHTU/Sdv7bO8Cc4hwcy/BRgBHHGnEbhjAHg9AdXd/6+T7vOfOXUPh7zgfEVmvhvPf4Fu4ZaDnPIQ6jnw05eAr48wzEvYvl+RTR3B4xpiA6AYY4wxYSqsz4kbY4wxxZkVcWOMMSZMWRE3xhhjwpQVcWOMMSZMWRE3xhhjwpQVcWOMMSZMWRE3Ecsdk3iLiGwXkdkiUj4f62ouIgvd3+8WkaezaVtZRPrnYRsJIjI4wPwDbl8+F5GuPsuWi0gDn7+vEpHtWWPOCxGJFpExIvKVOGNyp4jIw3ldXz7iuKCP7rx57vPxpYgcc3/fIiKNc7He+iLSzufvv4jIwCAe942IbBORT8UZG/rX7vwaIrIlS9uf1ykiU0XknmDjMyYYVsRNJDuhqvVUtQ7O/bMf9V0ojly/BlR1gaq+kk2TykCui3gO/q6q9YCOwFh3YI/C9g7wI3Cdqt4CtMO5IcwFRKREEcRyAVW9130++gIr3TzXU9VPssSW3ehX9XH6lBdNVPVm4BPg2Tyuw5h8syJuiouVQA13T3WHiPwT2AT8WkTaiMgaEdnk7rFHA4hIOxHZ6Q5ucl/mikSkj4i84f4e6+4VbnWnxsArwLXunuFIt90QEdng7r296LOu50Rkl4gsAWrm1AlV3Q0cBy4qsGfGD3FGyWoIPK+q591tp6rqq+7y5u5e6Ls4YyIjIoPcox7bffY+fz4y4P49WEQS3N+Xi8irIrJenCEnm7jzy4nIDPe5mgmUy2Xs34gzfOVq4F4RWSUi9dxll7t77+WAPwPd3TxlDrBxk4gki8geEflDEJtbg3ObU2M8EapjtBpTYNy9sTuBD91ZNYEHVbW/iFwCPI9z+8MMERkKDBKREcDbQAvgS2BmgNW/DiSr6r3uHmk08DRQx91TRETaANfhFEUBFohIU5zbSv4OuAXntbgJSMmhL/WB3ar6fR6eityoDWzNLOABNMTp514RiQMeBG7F6eM6EUnG2ZPPTklVbSgi7YEXcG5h+RhwXFVvFpGbcZ6X3MpQ1dsBROSJrAtV9YSIvOTGn/mBox7OuOMtcY6m7BCRt1T1XDbbaYtzq1pjPGF74iaSlXPPUW7EuRf4eHf+PlVd6/7eCLgRWO227Y1z7+1awF5V3a3OvYmnBthGC2AMgKqeU9Ws4wUDtHGnzTgFqRZOUW8CzFPV46r6E7Agm778SUR2Aetw7uOcyd99kwv8XsruEYMtInLQZ/Z6/d+Qknfg9CVDVdOBuTj9y8lc92cKzn2lAZriPt+q+inwaR5CDvShKycLVfW0+yHpCM69y/1ZKSLf48Saua1Az7vd29oUGiviJpJlnhOvp6qPq+ppd36GTxsBFvu0u1FVH3KXFdSbrwDDfbZRQ1UzP1AEu42/q2pN4AFgsoiUdef/wIWH1qsAh4MKSuTXPheEPZpl8edA3cxrBlT1r+6RBd/xkLM+j/6c5cL3mbJZlp9yf57jwiOD+X3ufWPzjSHr9rM65fN71ph8NcH50LEb5wgC/DIXkIt8GJMXVsRNcbcWuF1EagCISHkRuR5nVKKr3XPDAF0DPH4pzuFfRKSEiMTgjDjlO2bwR8Dvfc61XyEilwErcM7ZlhNnjOEOOQWrqnNxjiz0dmctB3qISGYR7Q0k5dxtUNX9Ph8s3sqy7Et3O3/JvHDN/eAQqFivAO5xn78KOMMsrsQZjeoyEblYRMoAvw0itBU4I44hInWAm4PpTza+BuLc3+/3mZ81T7miqseBgTi5rayqR4EfRaQZgIhcjHMEZnVet2FMTqyIm2JNVVOBPsB0EfkUp6jXUtWTQD/gA/fCtn0BVvEEEC8i23AOCddW1R9wDs9vF5GRqvox8C6wxm33Hs6YyptwDsVuAebgFL1gvIRz3j4KGIdTjLaKyFacc/Kv+bRt6V7olTndFuQ2wLny+2LgSxFJAZYAQ/01dPsyEViPc8j/HVXdrKpn3HjXAQtxPhzlZAwQ7ebjKXed+TESeEJEPuHCPeVlOEcbNvtc2JYrqvoNMBv3gxzQA3jJPTWzFOfCwK99HvKOTy6CzbcxAdlQpMYYY0yYsj1xY4wxJkxZETfGGGPClBVxY4wxJkxZETfGGGPClBVxY4wxJkxZETfGGGPClBVxY4wxJkz9PyjffVC9YqnnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    }
   ],
   "source": [
    "predict_ = list(range(90, 111))\n",
    "real_ = 100\n",
    "\n",
    "# 计算三个Loss的具体数值\n",
    "score_value = []\n",
    "rmse_value = []\n",
    "custom_rmse_value = []\n",
    "for i in range(len(predict_)):\n",
    "    score_value.append(compute_score(np.array(real_), np.array(predict_[i])))\n",
    "    rmse_value.append(compute_rmse(real_, predict_[i]))\n",
    "    custom_rmse_value.append(compute_penalized_rmse(real_, predict_[i]))\n",
    "    \n",
    "# 画图\n",
    "fig = plt.figure(figsize = (8, 4))\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.plot(list(range(-10, 11)), rmse_value, 'b--') \n",
    "plt.plot(list(range(-10, 11)), custom_rmse_value, 'green')\n",
    "\n",
    "\n",
    "plt.xlabel('Predicted RUL - Ground Truth RUL')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.ylabel('Error')\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(['Penalized RMSE (lambda = 0)', 'Penalized RMSE (lambda = 0.5)'],  loc='best', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# 保存\n",
    "fig.savefig('./plot/loss-comparison.eps', dpi=600, format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看 lambda 参数对结果的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_list = ['FD004']\n",
    "sequence_length = 100\n",
    "lambda_set = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "result = []\n",
    "for lambda_ in lambda_set:\n",
    "    def score(y_true, y_pred):\n",
    "        diff = y_pred - y_true\n",
    "        positive = K.maximum(diff, 0)\n",
    "        negative = K.minimum(diff, 0)\n",
    "        return K.sqrt(K.mean(K.square(positive) * (1 + lambda_) + K.square(negative)))\n",
    "    \n",
    "    def compute_penalized_rmse(y_true, y_pred):\n",
    "        diff = y_pred - y_true\n",
    "        positive = np.maximum(diff, 0)\n",
    "        negative = np.minimum(diff, 0)\n",
    "        return np.sqrt(np.mean(np.square(positive) * (1 + lambda_) + np.square(negative)))\n",
    "    \n",
    "    for index, file in enumerate(file_list):\n",
    "        sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "        upper = 160\n",
    "        \n",
    "        train_df, test_df, truth_df = read_data(file)\n",
    "        train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "        seq_array, label_array = split(train_df, sequence_length, sequence_cols, upper)\n",
    "\n",
    "        model = build_model(seq_array.shape[2], sequence_length)\n",
    "        history = model.fit(\n",
    "            x=seq_array, \n",
    "            y={'output': label_array, 'reconstruction': seq_array},\n",
    "            epochs=200, \n",
    "            batch_size=128 * 8,\n",
    "            verbose=0, \n",
    "            validation_split=0.2,\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min'),\n",
    "                keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-8),\n",
    "            ])\n",
    "        \n",
    "        print(lambda_, file)\n",
    "        a, b, c = evaluate(test_df, model, upper=upper, plot_=False)\n",
    "        result.append(str(lambda_) + ', ' + str(file) + ': ' + str(a) + ',' + str(b) + ',' + str(c) + ',' + str(np.sqrt(a) + b) +'\\n')\n",
    "    print('---------------')\n",
    "\n",
    "np.save('./ckpt_opt/effect_on_lambda', np.array(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 查看 Sequence Length 参数对结果的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['FD004']\n",
    "seq_set = [60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110]\n",
    "result = []\n",
    "lambda_ = 0.5\n",
    "for sequence_length in seq_set:\n",
    "    for index, file in enumerate(file_list):\n",
    "        sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "        upper = 160\n",
    "        \n",
    "        train_df, test_df, truth_df = read_data(file)\n",
    "        train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "        seq_array, label_array = split(train_df, sequence_length, sequence_cols, upper)\n",
    "\n",
    "        model = build_model(seq_array.shape[2], sequence_length)\n",
    "        \n",
    "        history = model.fit(\n",
    "            x=seq_array,\n",
    "            y={'output': label_array, 'reconstruction': seq_array},\n",
    "            epochs=200,\n",
    "            batch_size=128 * 8,\n",
    "            verbose=0,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min'),\n",
    "                keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-8),\n",
    "            ])\n",
    "        print(sequence_length, file)\n",
    "        a, b, c = evaluate(test_df, model,  upper=upper, plot_=False)\n",
    "        \n",
    "        result.append(str(sequence_length) + ', ' + str(file) + ': ' + str(a) + ',' + str(b) + ',' + str(c) + ',' + str(np.sqrt(a) + b) + '\\n')\n",
    "    print('---------------')\n",
    "\n",
    "np.save('./ckpt_opt/effect_on_sequence', np.array(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2个影响因素都查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/adc/miniconda3/envs/tf1.0/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 94.83389, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 94.83389\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 94.83389\n",
      "\n",
      "Epoch 00004: val_loss improved from 94.83389 to 94.21875, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.21875 to 92.20425, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 92.20425\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 92.20425\n",
      "\n",
      "Epoch 00008: val_loss improved from 92.20425 to 90.40760, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 90.40760 to 84.70968, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 84.70968 to 79.73160, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 79.73160 to 67.72195, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 67.72195\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 67.72195\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 67.72195\n",
      "\n",
      "Epoch 00015: val_loss improved from 67.72195 to 61.60960, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 61.60960 to 59.36672, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 59.36672 to 49.65699, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 49.65699 to 37.43631, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 37.43631\n",
      "\n",
      "Epoch 00020: val_loss improved from 37.43631 to 31.67473, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 31.67473\n",
      "\n",
      "Epoch 00022: val_loss improved from 31.67473 to 25.43414, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 25.43414 to 24.10498, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.10498 to 23.83022, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.83022\n",
      "\n",
      "Epoch 00026: val_loss improved from 23.83022 to 23.39884, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.39884\n",
      "\n",
      "Epoch 00028: val_loss improved from 23.39884 to 22.20954, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.20954\n",
      "\n",
      "Epoch 00030: val_loss improved from 22.20954 to 22.12254, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.12254\n",
      "\n",
      "Epoch 00044: val_loss improved from 22.12254 to 21.99662, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.99662\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.99662\n",
      "\n",
      "Epoch 00047: val_loss improved from 21.99662 to 21.87335, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.87335\n",
      "\n",
      "Epoch 00049: val_loss improved from 21.87335 to 21.84545, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.84545\n",
      "\n",
      "Epoch 00051: val_loss improved from 21.84545 to 21.76728, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.76728\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.76728\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.76728\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.76728\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.76728\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.76728\n",
      "\n",
      "Epoch 00058: val_loss improved from 21.76728 to 21.73844, saving model to ./ckpt_opt/0 70.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 21.73844\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 21.73844\n",
      "0 70 FD004\n",
      "Score:  4893.575301817399\n",
      "RMSE:  24.220345346451616\n",
      "MSE:  586.6251287013804\n",
      "Penalized RMSE:  24.220345346451616\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.82756, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.82756 to 103.63672, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 103.63672\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 103.63672\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 103.63672\n",
      "\n",
      "Epoch 00006: val_loss improved from 103.63672 to 101.56567, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 101.56567 to 90.70813, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 90.70813 to 77.24635, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 77.24635\n",
      "\n",
      "Epoch 00010: val_loss improved from 77.24635 to 72.51449, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 72.51449\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 72.51449\n",
      "\n",
      "Epoch 00013: val_loss improved from 72.51449 to 71.85863, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 71.85863\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 71.85863\n",
      "\n",
      "Epoch 00016: val_loss improved from 71.85863 to 70.71427, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 70.71427 to 42.65814, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 42.65814\n",
      "\n",
      "Epoch 00019: val_loss improved from 42.65814 to 31.71016, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 31.71016\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.71016 to 31.66853, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 31.66853\n",
      "\n",
      "Epoch 00023: val_loss improved from 31.66853 to 27.56438, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.56438\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.56438\n",
      "\n",
      "Epoch 00026: val_loss improved from 27.56438 to 26.96228, saving model to ./ckpt_opt/0 80.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.96228\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.96228\n",
      "0 80 FD004\n",
      "Score:  9563.09340836566\n",
      "RMSE:  26.601231566172746\n",
      "MSE:  707.6255208371454\n",
      "Penalized RMSE:  26.601231566172746\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.83126, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 110.83126\n",
      "\n",
      "Epoch 00003: val_loss improved from 110.83126 to 105.70374, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 105.70374 to 103.21601, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 103.21601 to 93.08509, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 93.08509\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 93.08509\n",
      "\n",
      "Epoch 00008: val_loss improved from 93.08509 to 90.85869, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 90.85869\n",
      "\n",
      "Epoch 00010: val_loss improved from 90.85869 to 66.42436, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 66.42436\n",
      "\n",
      "Epoch 00012: val_loss improved from 66.42436 to 65.40109, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 65.40109 to 62.79931, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.79931 to 58.37053, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 58.37053 to 47.77584, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 47.77584 to 42.76749, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 42.76749\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 42.76749\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 42.76749\n",
      "\n",
      "Epoch 00020: val_loss improved from 42.76749 to 33.07543, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 33.07543 to 31.22847, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 31.22847\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 31.22847\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 31.22847\n",
      "\n",
      "Epoch 00025: val_loss improved from 31.22847 to 25.73526, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.73526\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.73526\n",
      "\n",
      "Epoch 00028: val_loss improved from 25.73526 to 25.22399, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.22399\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.22399\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.22399\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.22399\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.22399\n",
      "\n",
      "Epoch 00034: val_loss improved from 25.22399 to 24.21522, saving model to ./ckpt_opt/0 90.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.21522\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 110.14445\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 110.14445\n",
      "\n",
      "Epoch 00004: val_loss improved from 110.14445 to 98.74270, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 98.74270\n",
      "\n",
      "Epoch 00006: val_loss improved from 98.74270 to 93.55718, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 93.55718 to 89.02682, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 89.02682 to 80.34330, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 80.34330 to 70.79950, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 70.79950\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 70.79950\n",
      "\n",
      "Epoch 00012: val_loss improved from 70.79950 to 63.27722, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 63.27722 to 57.32993, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 57.32993\n",
      "\n",
      "Epoch 00015: val_loss improved from 57.32993 to 49.94950, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 49.94950 to 44.86590, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 44.86590\n",
      "\n",
      "Epoch 00018: val_loss improved from 44.86590 to 33.71847, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 33.71847 to 31.17675, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 31.17675\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.17675 to 26.80332, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.80332 to 26.03997, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.03997 to 21.63982, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 21.63982 to 20.23793, saving model to ./ckpt_opt/0 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 20.23793\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 20.23793\n",
      "0 100 FD004\n",
      "Score:  3638.3206083619048\n",
      "RMSE:  22.57275625885059\n",
      "MSE:  509.52932512147845\n",
      "Penalized RMSE:  22.57275625885059\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 102.38881, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 102.38881 to 95.82873, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 95.82873 to 95.02752, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.02752 to 92.16521, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 92.16521\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.16521 to 90.30950, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 90.30950 to 90.22725, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 90.22725 to 88.99579, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 88.99579 to 82.78207, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 82.78207\n",
      "\n",
      "Epoch 00011: val_loss improved from 82.78207 to 78.24055, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 78.24055\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 78.24055\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 78.24055\n",
      "\n",
      "Epoch 00015: val_loss improved from 78.24055 to 67.74818, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 67.74818 to 63.07129, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 63.07129 to 46.53639, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.53639 to 44.21575, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 44.21575\n",
      "\n",
      "Epoch 00020: val_loss improved from 44.21575 to 30.77420, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 30.77420 to 25.61906, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 25.61906 to 22.22367, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 22.22367\n",
      "\n",
      "Epoch 00024: val_loss improved from 22.22367 to 20.05825, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 20.05825\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 20.05825\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 20.05825\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 20.05825\n",
      "\n",
      "Epoch 00029: val_loss improved from 20.05825 to 20.02656, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 20.02656\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 20.02656\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 20.02656\n",
      "\n",
      "Epoch 00033: val_loss improved from 20.02656 to 19.60963, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 19.60963\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 19.60963\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 19.60963\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 19.60963\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 19.60963\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 19.60963\n",
      "\n",
      "Epoch 00040: val_loss improved from 19.60963 to 19.49311, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 19.49311 to 19.35393, saving model to ./ckpt_opt/0 110.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 19.35393\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 19.35393\n",
      "0 110 FD004\n",
      "Score:  3815.9335531830525\n",
      "RMSE:  22.45795794981618\n",
      "MSE:  504.3598752757118\n",
      "Penalized RMSE:  22.45795794981618\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.41294, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.41294 to 105.79332, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.79332 to 95.76685, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.76685 to 95.14610, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 95.14610 to 91.30041, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 91.30041 to 79.73784, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 79.73784\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 79.73784\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 79.73784\n",
      "\n",
      "Epoch 00010: val_loss improved from 79.73784 to 72.88786, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 72.88786 to 65.79802, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 65.79802\n",
      "\n",
      "Epoch 00013: val_loss improved from 65.79802 to 61.02766, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 61.02766\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 61.02766\n",
      "\n",
      "Epoch 00016: val_loss improved from 61.02766 to 56.62049, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 56.62049 to 44.91105, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 44.91105\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.91105 to 40.90728, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 40.90728 to 29.78278, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 29.78278 to 24.83056, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 24.83056 to 23.55600, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 23.55600 to 21.40635, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 21.40635\n",
      "\n",
      "Epoch 00025: val_loss improved from 21.40635 to 20.67253, saving model to ./ckpt_opt/0 120.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 20.67253\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 20.67253\n",
      "0 120 FD004\n",
      "Score:  4747.827078910243\n",
      "RMSE:  24.71377367727564\n",
      "MSE:  610.7706093716023\n",
      "Penalized RMSE:  24.71377367727564\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 115.99654, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 115.99654 to 112.85220, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 112.85220\n",
      "\n",
      "Epoch 00004: val_loss improved from 112.85220 to 103.60972, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 103.60972 to 97.03878, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 97.03878 to 87.24410, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 87.24410\n",
      "\n",
      "Epoch 00008: val_loss improved from 87.24410 to 85.71993, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 85.71993 to 78.72310, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 78.72310 to 71.77587, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 71.77587\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 71.77587\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 71.77587\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 71.77587\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 71.77587\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 71.77587\n",
      "\n",
      "Epoch 00017: val_loss improved from 71.77587 to 68.69390, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 68.69390 to 55.77661, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 55.77661 to 53.34994, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 53.34994 to 42.09843, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 42.09843 to 37.01975, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 37.01975\n",
      "\n",
      "Epoch 00023: val_loss improved from 37.01975 to 28.58564, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 28.58564 to 27.51024, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.51024\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.51024\n",
      "\n",
      "Epoch 00027: val_loss improved from 27.51024 to 27.06701, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 27.06701 to 26.57648, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.57648\n",
      "\n",
      "Epoch 00030: val_loss improved from 26.57648 to 26.55856, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 26.55856 to 26.52309, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.52309\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.52309\n",
      "\n",
      "Epoch 00034: val_loss improved from 26.52309 to 26.47616, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.47616\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.47616\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.47616\n",
      "\n",
      "Epoch 00038: val_loss improved from 26.47616 to 25.99136, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.99136\n",
      "\n",
      "Epoch 00058: val_loss improved from 25.99136 to 25.93709, saving model to ./ckpt_opt/0 130.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 25.93709\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 25.93709\n",
      "0 130 FD004\n",
      "Score:  8321.657999621886\n",
      "RMSE:  26.005529493276228\n",
      "MSE:  676.2875642256597\n",
      "Penalized RMSE:  26.005529493276228\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 119.30275, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 119.30275 to 105.12014, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.12014 to 98.78678, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.78678 to 94.06584, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.06584 to 88.06709, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 88.06709 to 82.15325, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 82.15325 to 76.95900, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.95900 to 70.23952, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 70.23952\n",
      "\n",
      "Epoch 00010: val_loss improved from 70.23952 to 61.05829, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 61.05829\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 61.05829\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 61.05829\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 61.05829\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 61.05829\n",
      "\n",
      "Epoch 00016: val_loss improved from 61.05829 to 55.09707, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 55.09707 to 45.77561, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.77561 to 37.63364, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 37.63364 to 35.64956, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 35.64956 to 27.31964, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 27.31964\n",
      "\n",
      "Epoch 00022: val_loss improved from 27.31964 to 26.71654, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.71654 to 26.55136, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.55136 to 26.31736, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.31736 to 24.54497, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.54497 to 24.33037, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.33037\n",
      "\n",
      "Epoch 00034: val_loss improved from 24.33037 to 23.77220, saving model to ./ckpt_opt/0.1 70.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.77220\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.77220\n",
      "0.1 70 FD004\n",
      "Score:  8022.641847200835\n",
      "RMSE:  25.626640145733322\n",
      "MSE:  656.7246851589108\n",
      "Penalized RMSE:  26.456053019895506\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.24095, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.24095 to 95.46654, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 95.46654\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.46654 to 93.68184, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 93.68184 to 92.52119, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.52119 to 83.39124, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 83.39124\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.39124 to 74.14858, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 74.14858 to 67.31287, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 67.31287 to 62.13230, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 62.13230\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 62.13230\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 62.13230\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.13230 to 60.07457, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 60.07457\n",
      "\n",
      "Epoch 00016: val_loss improved from 60.07457 to 59.24059, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 59.24059 to 43.08351, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 43.08351 to 41.13389, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 41.13389 to 31.41724, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 31.41724 to 24.74615, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 24.74615\n",
      "\n",
      "Epoch 00022: val_loss improved from 24.74615 to 22.67265, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 22.67265 to 22.45954, saving model to ./ckpt_opt/0.1 80.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.45954\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.45954\n",
      "0.1 80 FD004\n",
      "Score:  7406.941167296629\n",
      "RMSE:  25.342659288906514\n",
      "MSE:  642.2503798335996\n",
      "Penalized RMSE:  26.397364897707938\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 121.94805, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 121.94805\n",
      "\n",
      "Epoch 00003: val_loss improved from 121.94805 to 98.10049, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 98.10049\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.10049 to 92.86108, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.86108 to 90.71579, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 90.71579 to 88.07219, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 88.07219 to 80.12402, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 80.12402\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 80.12402\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 80.12402\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 80.12402\n",
      "\n",
      "Epoch 00013: val_loss improved from 80.12402 to 76.72597, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 76.72597\n",
      "\n",
      "Epoch 00015: val_loss improved from 76.72597 to 57.06443, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 57.06443 to 53.59861, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 53.59861 to 44.14312, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 44.14312\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.14312 to 36.35021, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 36.35021 to 31.89844, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.89844 to 26.86656, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 26.86656\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.86656 to 24.93601, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.93601 to 23.74458, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.74458\n",
      "\n",
      "Epoch 00026: val_loss improved from 23.74458 to 22.91627, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.91627\n",
      "\n",
      "Epoch 00036: val_loss improved from 22.91627 to 21.76129, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 21.76129 to 21.15253, saving model to ./ckpt_opt/0.1 90.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 21.15253\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 21.15253\n",
      "0.1 90 FD004\n",
      "Score:  3525.10505311057\n",
      "RMSE:  22.94364180766466\n",
      "MSE:  526.4106993984177\n",
      "Penalized RMSE:  23.811462531998036\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.00827, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 110.00827\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 110.00827\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 110.00827\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 110.00827\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 110.00827\n",
      "\n",
      "Epoch 00007: val_loss improved from 110.00827 to 87.58136, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 87.58136 to 78.92800, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 78.92800\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 78.92800\n",
      "\n",
      "Epoch 00011: val_loss improved from 78.92800 to 77.78879, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 77.78879 to 61.12389, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 61.12389\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 61.12389\n",
      "\n",
      "Epoch 00015: val_loss improved from 61.12389 to 53.75162, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 53.75162 to 53.65263, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 53.65263 to 51.57878, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 51.57878 to 46.49642, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 46.49642 to 38.14979, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 38.14979\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.14979 to 37.10112, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 37.10112 to 24.40607, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24.40607\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.40607 to 23.16103, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 23.16103 to 23.04284, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.04284\n",
      "\n",
      "Epoch 00027: val_loss improved from 23.04284 to 22.46633, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 22.46633 to 21.63229, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 21.63229\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 21.63229\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 21.63229\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.63229\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.63229\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.63229\n",
      "\n",
      "Epoch 00035: val_loss improved from 21.63229 to 21.21901, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 21.21901 to 20.64116, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 20.64116\n",
      "\n",
      "Epoch 00045: val_loss improved from 20.64116 to 20.61909, saving model to ./ckpt_opt/0.1 100.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 20.61909\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 20.61909\n",
      "0.1 100 FD004\n",
      "Score:  3127.241744644385\n",
      "RMSE:  22.06898927841287\n",
      "MSE:  487.04028777070226\n",
      "Penalized RMSE:  22.872212304798147\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.68449, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 114.68449 to 102.90138, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 102.90138 to 96.54428, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.54428 to 92.28922, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.28922 to 86.97659, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.97659 to 82.99691, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 82.99691 to 76.68042, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.68042 to 71.65461, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 71.65461 to 68.48340, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 68.48340\n",
      "\n",
      "Epoch 00011: val_loss improved from 68.48340 to 67.24430, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 67.24430\n",
      "\n",
      "Epoch 00013: val_loss improved from 67.24430 to 57.05405, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 57.05405\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 57.05405\n",
      "\n",
      "Epoch 00016: val_loss improved from 57.05405 to 50.40371, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 50.40371 to 42.12121, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 42.12121\n",
      "\n",
      "Epoch 00019: val_loss improved from 42.12121 to 36.87682, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 36.87682 to 34.91216, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.91216 to 25.98937, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 25.98937 to 22.76123, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 22.76123 to 20.61531, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 20.61531\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 20.61531\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 20.61531\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 20.61531\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 20.61531\n",
      "\n",
      "Epoch 00029: val_loss improved from 20.61531 to 19.98427, saving model to ./ckpt_opt/0.1 110.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 19.98427\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 19.98427\n",
      "0.1 110 FD004\n",
      "Score:  4931.713500919781\n",
      "RMSE:  23.792921835108555\n",
      "MSE:  566.1031294515855\n",
      "Penalized RMSE:  24.787574386634283\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.38674, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 114.38674 to 106.11435, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 106.11435 to 94.79320, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 94.79320 to 91.68907, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.68907 to 87.56343, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 87.56343 to 81.12648, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 81.12648 to 75.00068, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 75.00068 to 72.58885, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 72.58885 to 66.00818, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 66.00818\n",
      "\n",
      "Epoch 00017: val_loss improved from 66.00818 to 50.89600, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 50.89600\n",
      "\n",
      "Epoch 00019: val_loss improved from 50.89600 to 50.37296, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 50.37296\n",
      "\n",
      "Epoch 00021: val_loss improved from 50.37296 to 44.86234, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 44.86234 to 28.16730, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.16730 to 23.66677, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23.66677\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.66677\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.66677\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.66677\n",
      "\n",
      "Epoch 00028: val_loss improved from 23.66677 to 23.54218, saving model to ./ckpt_opt/0.1 120.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.54218\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.54218\n",
      "0.1 120 FD004\n",
      "Score:  5279.564451928365\n",
      "RMSE:  25.309334330816927\n",
      "MSE:  640.5624042690683\n",
      "Penalized RMSE:  26.36473013664614\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 113.59539, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 113.59539 to 106.84911, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 106.84911 to 103.17345, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 103.17345 to 97.00976, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 97.00976\n",
      "\n",
      "Epoch 00006: val_loss improved from 97.00976 to 85.00638, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.00638 to 83.15223, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.15223 to 79.16614, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 79.16614 to 76.69222, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 76.69222 to 71.99876, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 71.99876\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 71.99876\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 71.99876\n",
      "\n",
      "Epoch 00014: val_loss improved from 71.99876 to 60.46360, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 60.46360 to 55.34877, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 55.34877 to 49.35042, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.35042 to 45.35257, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.35257 to 41.25383, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 41.25383 to 40.72083, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 40.72083 to 29.01501, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 29.01501 to 26.14392, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.14392 to 23.88593, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 23.88593 to 23.48877, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 23.48877 to 21.56614, saving model to ./ckpt_opt/0.1 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.56614\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.56614\n",
      "0.1 130 FD004\n",
      "Score:  6101.094183202994\n",
      "RMSE:  25.57087172116414\n",
      "MSE:  653.8694805802319\n",
      "Penalized RMSE:  26.620584318934327\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.60302, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 107.60302\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.60302 to 97.05035, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.05035 to 92.33122, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.33122 to 85.89760, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 85.89760 to 83.75695, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 83.75695\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.75695 to 75.97307, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 75.97307 to 74.44139, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 74.44139 to 64.53908, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 64.53908\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 64.53908\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 64.53908\n",
      "\n",
      "Epoch 00014: val_loss improved from 64.53908 to 56.41274, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 56.41274\n",
      "\n",
      "Epoch 00016: val_loss improved from 56.41274 to 48.27655, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 48.27655 to 46.02618, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.02618 to 36.70623, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 36.70623 to 29.37758, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 29.37758 to 25.40167, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 25.40167 to 23.38882, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 23.38882\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 23.38882\n",
      "\n",
      "Epoch 00024: val_loss improved from 23.38882 to 22.42232, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 22.42232 to 22.31625, saving model to ./ckpt_opt/0.2 70.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.31625\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.31625\n",
      "0.2 70 FD004\n",
      "Score:  5119.815912065412\n",
      "RMSE:  24.017060167851824\n",
      "MSE:  576.8191791062147\n",
      "Penalized RMSE:  25.692127412801586\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 106.63194, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 106.63194 to 98.45257, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 98.45257 to 92.41629, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 92.41629\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 92.41629\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.41629 to 88.88768, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.88768 to 80.46796, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 80.46796\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 80.46796\n",
      "\n",
      "Epoch 00010: val_loss improved from 80.46796 to 76.98518, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 76.98518 to 66.38235, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 66.38235\n",
      "\n",
      "Epoch 00013: val_loss improved from 66.38235 to 64.38879, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 64.38879\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 64.38879\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 64.38879\n",
      "\n",
      "Epoch 00017: val_loss improved from 64.38879 to 49.55114, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 49.55114 to 42.87353, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 42.87353 to 37.40589, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 37.40589\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.40589 to 27.68071, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 27.68071 to 26.75761, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.75761\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.75761\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.75761 to 25.84271, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.84271\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.84271\n",
      "\n",
      "Epoch 00028: val_loss improved from 25.84271 to 25.35935, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.35935\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.35935\n",
      "\n",
      "Epoch 00031: val_loss improved from 25.35935 to 23.97361, saving model to ./ckpt_opt/0.2 80.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.97361\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.97361\n",
      "0.2 80 FD004\n",
      "Score:  3217.8383720461497\n",
      "RMSE:  22.887591185161213\n",
      "MSE:  523.8418302590693\n",
      "Penalized RMSE:  24.41350880807115\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.46937, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.46937 to 103.68204, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.68204 to 96.33153, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.33153 to 94.21023, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.21023 to 91.87456, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 91.87456 to 90.34788, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 90.34788 to 82.56755, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 82.56755 to 78.20427, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 78.20427\n",
      "\n",
      "Epoch 00010: val_loss improved from 78.20427 to 66.35035, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 66.35035 to 59.34581, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 59.34581\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 59.34581\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 59.34581\n",
      "\n",
      "Epoch 00015: val_loss improved from 59.34581 to 58.24690, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 58.24690 to 47.37762, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 47.37762\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 47.37762\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.37762 to 32.15847, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 32.15847\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.15847 to 27.82367, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 27.82367\n",
      "\n",
      "Epoch 00023: val_loss improved from 27.82367 to 27.31689, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.31689\n",
      "\n",
      "Epoch 00025: val_loss improved from 27.31689 to 24.83197, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.83197 to 24.82445, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.82445\n",
      "\n",
      "Epoch 00028: val_loss improved from 24.82445 to 23.90634, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.90634\n",
      "\n",
      "Epoch 00030: val_loss improved from 23.90634 to 23.42684, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.42684\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.42684\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.42684\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.42684\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.42684\n",
      "\n",
      "Epoch 00036: val_loss improved from 23.42684 to 23.28997, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.28997\n",
      "\n",
      "Epoch 00053: val_loss improved from 23.28997 to 22.81253, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.81253\n",
      "\n",
      "Epoch 00055: val_loss improved from 22.81253 to 22.65493, saving model to ./ckpt_opt/0.2 90.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 22.65493\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 22.65493\n",
      "0.2 90 FD004\n",
      "Score:  2857.1603998116148\n",
      "RMSE:  21.757249094305166\n",
      "MSE:  473.377888151643\n",
      "Penalized RMSE:  23.290027592031347\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 117.39811, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 117.39811\n",
      "\n",
      "Epoch 00003: val_loss improved from 117.39811 to 103.54778, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 103.54778 to 98.84336, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.84336 to 94.83693, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 94.83693 to 93.09526, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 93.09526\n",
      "\n",
      "Epoch 00008: val_loss improved from 93.09526 to 80.33719, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 80.33719\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 80.33719\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 80.33719\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 80.33719\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 80.33719\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 80.33719\n",
      "\n",
      "Epoch 00015: val_loss improved from 80.33719 to 79.04878, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 79.04878 to 65.54664, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 65.54664\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 65.54664\n",
      "\n",
      "Epoch 00019: val_loss improved from 65.54664 to 51.14973, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 51.14973\n",
      "\n",
      "Epoch 00021: val_loss improved from 51.14973 to 46.54108, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 46.54108 to 30.89771, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.89771 to 28.94076, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 28.94076\n",
      "\n",
      "Epoch 00025: val_loss improved from 28.94076 to 23.91610, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.91610\n",
      "\n",
      "Epoch 00036: val_loss improved from 23.91610 to 23.79484, saving model to ./ckpt_opt/0.2 100.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.79484\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23.79484\n",
      "0.2 100 FD004\n",
      "Score:  7136.339635866233\n",
      "RMSE:  24.719996620980886\n",
      "MSE:  611.0782329413064\n",
      "Penalized RMSE:  26.27146031723295\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 117.47348, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 117.47348\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 117.47348\n",
      "\n",
      "Epoch 00004: val_loss improved from 117.47348 to 110.96639, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 110.96639 to 102.65228, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 102.65228 to 88.99206, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.99206 to 76.88944, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.88944 to 76.88290, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 76.88290 to 72.06929, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 72.06929\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 72.06929\n",
      "\n",
      "Epoch 00012: val_loss improved from 72.06929 to 64.74098, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 64.74098 to 64.64454, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 64.64454 to 55.71230, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 55.71230 to 50.82310, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 50.82310 to 49.99621, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.99621 to 39.14614, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 39.14614\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.14614 to 34.39726, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 34.39726\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.39726 to 27.51679, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 27.51679 to 24.88899, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 24.88899 to 23.57242, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23.57242\n",
      "\n",
      "Epoch 00025: val_loss improved from 23.57242 to 22.56221, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 22.56221 to 21.69285, saving model to ./ckpt_opt/0.2 110.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.69285\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.69285\n",
      "0.2 110 FD004\n",
      "Score:  4134.092427965053\n",
      "RMSE:  22.009762636366002\n",
      "MSE:  484.4296513091729\n",
      "Penalized RMSE:  23.408666963970393\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.20064, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.20064 to 110.41715, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 110.41715 to 95.91782, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.91782 to 94.55588, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 94.55588\n",
      "\n",
      "Epoch 00006: val_loss improved from 94.55588 to 84.46198, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 84.46198 to 75.99150, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 75.99150 to 70.17230, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 70.17230\n",
      "\n",
      "Epoch 00010: val_loss improved from 70.17230 to 69.65718, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 69.65718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 69.65718\n",
      "\n",
      "Epoch 00013: val_loss improved from 69.65718 to 66.49210, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 66.49210\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.49210 to 51.86683, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 51.86683\n",
      "\n",
      "Epoch 00017: val_loss improved from 51.86683 to 51.03980, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 51.03980 to 35.53519, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 35.53519 to 34.87406, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 34.87406 to 30.93399, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 30.93399\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 30.93399\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 30.93399\n",
      "\n",
      "Epoch 00024: val_loss improved from 30.93399 to 29.70229, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 29.70229 to 24.54393, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.54393 to 24.50150, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.50150\n",
      "\n",
      "Epoch 00028: val_loss improved from 24.50150 to 23.85104, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.85104\n",
      "\n",
      "Epoch 00048: val_loss improved from 23.85104 to 23.79543, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.79543\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.79543\n",
      "\n",
      "Epoch 00051: val_loss improved from 23.79543 to 23.20204, saving model to ./ckpt_opt/0.2 120.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 23.20204\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 23.20204\n",
      "0.2 120 FD004\n",
      "Score:  3510.991231528282\n",
      "RMSE:  22.42717515878641\n",
      "MSE:  502.9781856028863\n",
      "Penalized RMSE:  24.028263329178614\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 100.97407, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 100.97407 to 98.87578, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 98.87578 to 93.13054, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.13054 to 88.45244, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 88.45244 to 82.53661, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 82.53661 to 78.89162, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 78.89162 to 71.33924, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 71.33924 to 67.27927, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 67.27927 to 63.15223, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 63.15223\n",
      "\n",
      "Epoch 00011: val_loss improved from 63.15223 to 61.78877, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 61.78877 to 57.59471, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 57.59471 to 55.67504, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 55.67504 to 49.29771, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 49.29771\n",
      "\n",
      "Epoch 00016: val_loss improved from 49.29771 to 45.36537, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 45.36537 to 39.82490, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 39.82490 to 39.32705, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.32705 to 32.04525, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 32.04525 to 28.48480, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 28.48480 to 27.27658, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 27.27658 to 24.55286, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24.55286\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24.55286\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.55286\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.55286\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.55286\n",
      "\n",
      "Epoch 00028: val_loss improved from 24.55286 to 23.63830, saving model to ./ckpt_opt/0.2 130.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.63830\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.63830\n",
      "0.2 130 FD004\n",
      "Score:  5950.997866243049\n",
      "RMSE:  24.351879995844747\n",
      "MSE:  593.0140593320235\n",
      "Penalized RMSE:  26.000361166421712\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.12346, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 114.12346 to 110.55994, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 110.55994 to 99.82818, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 99.82818 to 97.45662, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 97.45662\n",
      "\n",
      "Epoch 00006: val_loss improved from 97.45662 to 96.37909, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 96.37909\n",
      "\n",
      "Epoch 00008: val_loss improved from 96.37909 to 95.92415, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 95.92415 to 79.63847, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 79.63847\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 79.63847\n",
      "\n",
      "Epoch 00012: val_loss improved from 79.63847 to 76.70917, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 76.70917\n",
      "\n",
      "Epoch 00014: val_loss improved from 76.70917 to 72.43795, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 72.43795 to 57.34808, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 57.34808\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 57.34808\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 57.34808\n",
      "\n",
      "Epoch 00019: val_loss improved from 57.34808 to 37.35615, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 37.35615\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 37.35615\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 37.35615\n",
      "\n",
      "Epoch 00023: val_loss improved from 37.35615 to 26.54976, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.54976\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.54976\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.54976\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.54976\n",
      "\n",
      "Epoch 00028: val_loss improved from 26.54976 to 26.38328, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.38328\n",
      "\n",
      "Epoch 00030: val_loss improved from 26.38328 to 25.45797, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.45797\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.45797\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.45797\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.45797\n",
      "\n",
      "Epoch 00035: val_loss improved from 25.45797 to 25.22527, saving model to ./ckpt_opt/0.3 70.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.22527\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.22527\n",
      "0.3 70 FD004\n",
      "Score:  10258.187055567296\n",
      "RMSE:  26.478628626943138\n",
      "MSE:  701.1177739635726\n",
      "Penalized RMSE:  28.556600169706137\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 116.80452, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 116.80452 to 107.08644, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.08644 to 98.91208, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.91208 to 95.43367, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 95.43367 to 89.86275, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.86275 to 85.36618, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.36618 to 82.55476, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 82.55476\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 82.55476\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 82.55476\n",
      "\n",
      "Epoch 00011: val_loss improved from 82.55476 to 65.67899, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 65.67899\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 65.67899\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 65.67899\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 65.67899\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 65.67899\n",
      "\n",
      "Epoch 00017: val_loss improved from 65.67899 to 55.96891, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 55.96891 to 50.60237, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 50.60237 to 40.29102, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 40.29102 to 38.66633, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.66633 to 27.20920, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 27.20920 to 24.21076, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24.21076\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.21076 to 22.96124, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.96124\n",
      "\n",
      "Epoch 00034: val_loss improved from 22.96124 to 22.39721, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.39721\n",
      "\n",
      "Epoch 00042: val_loss improved from 22.39721 to 21.91709, saving model to ./ckpt_opt/0.3 80.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 21.91709\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 21.91709\n",
      "0.3 80 FD004\n",
      "Score:  3186.4366049527807\n",
      "RMSE:  22.057558254233076\n",
      "MSE:  486.53587613888567\n",
      "Penalized RMSE:  24.39130415279883\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.59035, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.59035 to 108.76479, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 108.76479 to 103.35885, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 103.35885\n",
      "\n",
      "Epoch 00005: val_loss improved from 103.35885 to 90.63170, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 90.63170\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 90.63170\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 90.63170\n",
      "\n",
      "Epoch 00009: val_loss improved from 90.63170 to 79.93148, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 79.93148\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 79.93148\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 79.93148\n",
      "\n",
      "Epoch 00013: val_loss improved from 79.93148 to 78.57440, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 78.57440\n",
      "\n",
      "Epoch 00015: val_loss improved from 78.57440 to 65.85406, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 65.85406\n",
      "\n",
      "Epoch 00017: val_loss improved from 65.85406 to 54.05974, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 54.05974 to 50.54848, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 50.54848 to 39.63398, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 39.63398 to 29.52289, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 29.52289\n",
      "\n",
      "Epoch 00022: val_loss improved from 29.52289 to 28.56352, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.56352 to 26.84024, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.84024\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.84024 to 25.06704, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.06704\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.06704 to 24.72385, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.72385\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.72385\n",
      "\n",
      "Epoch 00030: val_loss improved from 24.72385 to 24.55260, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 24.55260 to 24.32130, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.32130\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.32130\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.32130\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.32130\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.32130\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.32130\n",
      "\n",
      "Epoch 00038: val_loss improved from 24.32130 to 23.96621, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.96621\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.96621\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.96621\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.96621\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.96621\n",
      "\n",
      "Epoch 00044: val_loss improved from 23.96621 to 23.94963, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.94963\n",
      "\n",
      "Epoch 00046: val_loss improved from 23.94963 to 23.35612, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.35612\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.35612\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.35612\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.35612\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.35612\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.35612\n",
      "\n",
      "Epoch 00053: val_loss improved from 23.35612 to 23.35058, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.35058\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.35058\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.35058\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.35058\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.35058\n",
      "\n",
      "Epoch 00059: val_loss improved from 23.35058 to 22.85437, saving model to ./ckpt_opt/0.3 90.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 22.85437\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 22.85437\n",
      "0.3 90 FD004\n",
      "Score:  4250.104312084414\n",
      "RMSE:  23.60483042716591\n",
      "MSE:  557.1880194952577\n",
      "Penalized RMSE:  26.03763388821508\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 118.48935, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 118.48935 to 109.70012, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 109.70012 to 101.12312, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 101.12312 to 94.80042, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.80042 to 92.00393, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.00393 to 84.23731, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 84.23731\n",
      "\n",
      "Epoch 00008: val_loss improved from 84.23731 to 82.95800, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 82.95800 to 75.16049, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 75.16049\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 75.16049\n",
      "\n",
      "Epoch 00012: val_loss improved from 75.16049 to 74.46277, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 74.46277 to 72.32071, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 72.32071 to 56.49763, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 56.49763\n",
      "\n",
      "Epoch 00016: val_loss improved from 56.49763 to 53.45248, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 53.45248 to 43.40926, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 43.40926 to 34.03709, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 34.03709\n",
      "\n",
      "Epoch 00020: val_loss improved from 34.03709 to 32.28183, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.28183 to 28.31241, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 28.31241\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.31241 to 26.55206, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.55206 to 23.70448, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 23.70448 to 23.44912, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 23.44912 to 21.87282, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 21.87282 to 21.00641, saving model to ./ckpt_opt/0.3 100.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.00641\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.00641\n",
      "0.3 100 FD004\n",
      "Score:  2591.4236090906566\n",
      "RMSE:  21.647743717783253\n",
      "MSE:  468.6248080708243\n",
      "Penalized RMSE:  23.604000048839765\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 105.31160, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 105.31160 to 100.18628, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 100.18628 to 98.85134, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.85134 to 95.51436, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 95.51436 to 90.32336, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 90.32336\n",
      "\n",
      "Epoch 00007: val_loss improved from 90.32336 to 83.99261, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.99261 to 82.21850, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 82.21850 to 74.38770, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 74.38770 to 70.18914, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 70.18914\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 70.18914\n",
      "\n",
      "Epoch 00013: val_loss improved from 70.18914 to 65.47163, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 65.47163 to 55.31476, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 55.31476\n",
      "\n",
      "Epoch 00016: val_loss improved from 55.31476 to 48.69640, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 48.69640\n",
      "\n",
      "Epoch 00018: val_loss improved from 48.69640 to 45.28298, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 45.28298\n",
      "\n",
      "Epoch 00020: val_loss improved from 45.28298 to 33.20443, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 33.20443 to 30.33633, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 30.33633 to 25.36798, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 25.36798\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.36798 to 24.09384, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.09384\n",
      "\n",
      "Epoch 00034: val_loss improved from 24.09384 to 23.67734, saving model to ./ckpt_opt/0.3 110.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.67734\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.67734\n",
      "0.3 110 FD004\n",
      "Score:  5643.76036592402\n",
      "RMSE:  23.67128376157739\n",
      "MSE:  560.3296749211173\n",
      "Penalized RMSE:  26.20881308171959\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.02549, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.02549 to 97.24718, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 97.24718\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.24718 to 93.96962, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 93.96962 to 86.79296, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.79296 to 82.21492, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 82.21492 to 76.10249, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 76.10249\n",
      "\n",
      "Epoch 00009: val_loss improved from 76.10249 to 69.36662, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 69.36662\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 69.36662\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 69.36662\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 69.36662\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 69.36662\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 69.36662\n",
      "\n",
      "Epoch 00016: val_loss improved from 69.36662 to 51.15402, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 51.15402\n",
      "\n",
      "Epoch 00018: val_loss improved from 51.15402 to 50.32092, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 50.32092\n",
      "\n",
      "Epoch 00020: val_loss improved from 50.32092 to 43.60665, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 43.60665 to 40.47812, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 40.47812 to 31.24606, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 31.24606\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 31.24606\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 31.24606\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 31.24606\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 31.24606\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 31.24606\n",
      "\n",
      "Epoch 00029: val_loss improved from 31.24606 to 24.66189, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.66189\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.66189\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.66189\n",
      "\n",
      "Epoch 00033: val_loss improved from 24.66189 to 24.28195, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.28195\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.28195\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.28195\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.28195\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.28195\n",
      "\n",
      "Epoch 00039: val_loss improved from 24.28195 to 24.00415, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.00415\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.00415\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.00415\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.00415\n",
      "\n",
      "Epoch 00044: val_loss improved from 24.00415 to 23.86646, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.86646\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.86646\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.86646\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.86646\n",
      "\n",
      "Epoch 00049: val_loss improved from 23.86646 to 22.35126, saving model to ./ckpt_opt/0.3 120.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 22.35126\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 22.35126\n",
      "0.3 120 FD004\n",
      "Score:  4100.298320372536\n",
      "RMSE:  23.507458886879196\n",
      "MSE:  552.6006233183157\n",
      "Penalized RMSE:  25.376155884276127\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 115.93533, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 115.93533 to 106.42517, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 106.42517 to 105.40912, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 105.40912 to 100.62319, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 100.62319 to 92.26656, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.26656 to 82.84569, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 82.84569 to 81.56987, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 81.56987 to 68.79105, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 68.79105 to 68.00300, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 68.00300\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 68.00300\n",
      "\n",
      "Epoch 00012: val_loss improved from 68.00300 to 56.97872, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 56.97872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 56.97872\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 56.97872\n",
      "\n",
      "Epoch 00016: val_loss improved from 56.97872 to 51.95481, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 51.95481 to 47.51532, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 47.51532\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 47.51532\n",
      "\n",
      "Epoch 00020: val_loss improved from 47.51532 to 35.84947, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 35.84947\n",
      "\n",
      "Epoch 00022: val_loss improved from 35.84947 to 34.07479, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 34.07479 to 28.25421, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 28.25421 to 25.71536, saving model to ./ckpt_opt/0.3 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.71536\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.71536\n",
      "0.3 130 FD004\n",
      "Score:  5923.940327479948\n",
      "RMSE:  24.077440036766212\n",
      "MSE:  579.7231187240726\n",
      "Penalized RMSE:  26.444679385371376\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.87054, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.87054 to 102.83510, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 102.83510 to 97.12147, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.12147 to 90.98334, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 90.98334 to 85.57505, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 85.57505 to 83.40178, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.40178 to 81.77962, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 81.77962 to 73.81848, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 73.81848 to 73.18528, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 73.18528\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 73.18528\n",
      "\n",
      "Epoch 00012: val_loss improved from 73.18528 to 68.90870, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 68.90870 to 60.95519, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 60.95519 to 56.29776, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 56.29776 to 51.84743, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 51.84743 to 46.50367, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 46.50367\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.50367 to 34.38961, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 34.38961 to 33.54536, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 33.54536 to 27.98319, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 27.98319 to 26.49254, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.49254 to 25.53820, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 25.53820 to 25.42606, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.42606 to 24.59866, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 24.59866 to 24.22272, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.22272\n",
      "\n",
      "Epoch 00035: val_loss improved from 24.22272 to 24.07571, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 24.07571 to 23.67015, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.67015\n",
      "\n",
      "Epoch 00038: val_loss improved from 23.67015 to 23.65963, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.65963\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.65963\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.65963\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.65963\n",
      "\n",
      "Epoch 00043: val_loss improved from 23.65963 to 22.91268, saving model to ./ckpt_opt/0.4 70.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.91268\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.91268\n",
      "0.4 70 FD004\n",
      "Score:  3559.6481972546408\n",
      "RMSE:  21.726601088014284\n",
      "MSE:  472.0451948377035\n",
      "Penalized RMSE:  24.416308142692294\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.25797, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 111.25797\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 111.25797\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 111.25797\n",
      "\n",
      "Epoch 00005: val_loss improved from 111.25797 to 95.60861, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 95.60861 to 85.93274, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 85.93274\n",
      "\n",
      "Epoch 00008: val_loss improved from 85.93274 to 84.44105, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 84.44105 to 80.52794, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 80.52794 to 73.08679, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 73.08679 to 67.63013, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 67.63013\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 67.63013\n",
      "\n",
      "Epoch 00014: val_loss improved from 67.63013 to 53.97148, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 53.97148 to 48.97621, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 48.97621\n",
      "\n",
      "Epoch 00017: val_loss improved from 48.97621 to 39.01832, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 39.01832\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.01832 to 30.02710, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 30.02710 to 29.09735, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 29.09735 to 23.87420, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 23.87420 to 23.09389, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 23.09389\n",
      "\n",
      "Epoch 00024: val_loss improved from 23.09389 to 22.01748, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.01748\n",
      "\n",
      "Epoch 00026: val_loss improved from 22.01748 to 21.91253, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 21.91253 to 21.78723, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 21.78723\n",
      "\n",
      "Epoch 00029: val_loss improved from 21.78723 to 21.58103, saving model to ./ckpt_opt/0.4 80.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.58103\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.58103\n",
      "0.4 80 FD004\n",
      "Score:  2339.627111910886\n",
      "RMSE:  20.57187238574384\n",
      "MSE:  423.20193345533\n",
      "Penalized RMSE:  22.72573723435312\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 123.16195, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 123.16195 to 107.00864, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 107.00864\n",
      "\n",
      "Epoch 00004: val_loss improved from 107.00864 to 103.99845, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 103.99845\n",
      "\n",
      "Epoch 00006: val_loss improved from 103.99845 to 84.13436, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 84.13436\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 84.13436\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 84.13436\n",
      "\n",
      "Epoch 00010: val_loss improved from 84.13436 to 82.10360, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 82.10360 to 76.49770, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 76.49770\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 76.49770\n",
      "\n",
      "Epoch 00014: val_loss improved from 76.49770 to 57.07362, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 57.07362\n",
      "\n",
      "Epoch 00016: val_loss improved from 57.07362 to 54.32496, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 54.32496 to 47.20907, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 47.20907 to 38.83736, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 38.83736 to 36.41850, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 36.41850\n",
      "\n",
      "Epoch 00021: val_loss improved from 36.41850 to 28.64477, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.64477 to 26.71528, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.71528\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.71528\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.71528 to 26.33098, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 26.33098 to 25.39692, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.39692 to 23.36543, saving model to ./ckpt_opt/0.4 90.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.36543\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.36543\n",
      "0.4 90 FD004\n",
      "Score:  2848.6237945872817\n",
      "RMSE:  22.87034915948898\n",
      "MSE:  523.0528706769384\n",
      "Penalized RMSE:  25.601694792931823\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 128.25566, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 128.25566 to 117.12202, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 117.12202 to 101.79811, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 101.79811 to 95.45210, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 95.45210 to 94.39159, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 94.39159 to 84.20066, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 84.20066\n",
      "\n",
      "Epoch 00008: val_loss improved from 84.20066 to 79.49497, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 79.49497 to 78.08427, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 78.08427\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 78.08427\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 78.08427\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 78.08427\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 78.08427\n",
      "\n",
      "Epoch 00015: val_loss improved from 78.08427 to 66.85043, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 66.85043 to 57.17070, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.17070 to 50.32297, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 50.32297 to 44.21353, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.21353 to 37.41094, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 37.41094 to 31.95731, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.95731 to 28.10130, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.10130 to 27.12110, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.12110\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.12110 to 27.02535, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.02535\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.02535\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 27.02535\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 27.02535\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.02535\n",
      "\n",
      "Epoch 00030: val_loss improved from 27.02535 to 26.08367, saving model to ./ckpt_opt/0.4 100.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 26.08367\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 26.08367\n",
      "0.4 100 FD004\n",
      "Score:  6212.826827769533\n",
      "RMSE:  25.306457659568494\n",
      "MSE:  640.4167992755329\n",
      "Penalized RMSE:  28.707573979054303\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 113.51015, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 113.51015 to 113.04685, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 113.04685 to 98.84687, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.84687 to 95.32763, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 95.32763 to 91.30588, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 91.30588 to 89.43337, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 89.43337 to 85.01190, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 85.01190 to 77.71660, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 77.71660 to 71.50523, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 71.50523\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 71.50523\n",
      "\n",
      "Epoch 00012: val_loss improved from 71.50523 to 68.07320, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 68.07320\n",
      "\n",
      "Epoch 00014: val_loss improved from 68.07320 to 66.95542, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.95542 to 62.08079, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 62.08079 to 51.13520, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 51.13520 to 44.31403, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 44.31403\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.31403 to 38.05377, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.05377 to 26.42979, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 26.42979\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.42979 to 23.79152, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 23.79152\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23.79152\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.79152\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.79152\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.79152\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.79152\n",
      "\n",
      "Epoch 00029: val_loss improved from 23.79152 to 21.23745, saving model to ./ckpt_opt/0.4 110.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.23745\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.23745\n",
      "0.4 110 FD004\n",
      "Score:  4400.702325406384\n",
      "RMSE:  22.660274493010387\n",
      "MSE:  513.4880400985771\n",
      "Penalized RMSE:  25.775886701395613\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.38076, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 109.38076\n",
      "\n",
      "Epoch 00003: val_loss improved from 109.38076 to 101.99908, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 101.99908 to 98.55924, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 98.55924\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 98.55924\n",
      "\n",
      "Epoch 00007: val_loss improved from 98.55924 to 84.84143, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 84.84143\n",
      "\n",
      "Epoch 00009: val_loss improved from 84.84143 to 64.17495, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 64.17495\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 64.17495\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 64.17495\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 64.17495\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 64.17495\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 64.17495\n",
      "\n",
      "Epoch 00016: val_loss improved from 64.17495 to 57.06947, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.06947 to 50.58807, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 50.58807\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 50.58807\n",
      "\n",
      "Epoch 00020: val_loss improved from 50.58807 to 49.07858, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 49.07858 to 44.25939, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 44.25939\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 44.25939\n",
      "\n",
      "Epoch 00024: val_loss improved from 44.25939 to 42.02807, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 42.02807\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 42.02807\n",
      "\n",
      "Epoch 00027: val_loss improved from 42.02807 to 41.32334, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 41.32334 to 41.06923, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 41.06923\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 41.06923\n",
      "\n",
      "Epoch 00031: val_loss improved from 41.06923 to 40.82206, saving model to ./ckpt_opt/0.4 120.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 40.82206\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 40.82206\n",
      "0.4 120 FD004\n",
      "Score:  61702.435010485264\n",
      "RMSE:  36.29381929642309\n",
      "MSE:  1317.241319121413\n",
      "Penalized RMSE:  40.41373740156005\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 100.90204, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 100.90204 to 99.58691, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 99.58691 to 98.05321, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 98.05321\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.05321 to 95.08704, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 95.08704 to 93.14431, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 93.14431 to 89.25807, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 89.25807 to 88.02366, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 88.02366 to 85.22507, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 85.22507 to 76.81239, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 76.81239 to 76.57443, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 76.57443 to 73.39331, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.39331 to 68.65291, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 68.65291 to 61.87642, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 61.87642 to 50.16880, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 50.16880 to 45.10916, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 45.10916\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.10916 to 40.53285, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 40.53285 to 36.22760, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 36.22760 to 29.89909, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 29.89909\n",
      "\n",
      "Epoch 00022: val_loss improved from 29.89909 to 27.38957, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.38957\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.38957 to 27.33486, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.33486\n",
      "\n",
      "Epoch 00026: val_loss improved from 27.33486 to 26.94424, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 26.94424 to 24.78688, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.78688\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.78688\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.78688\n",
      "\n",
      "Epoch 00031: val_loss improved from 24.78688 to 24.61479, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.61479\n",
      "\n",
      "Epoch 00046: val_loss improved from 24.61479 to 24.44562, saving model to ./ckpt_opt/0.4 130.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 24.44562\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 24.44562\n",
      "0.4 130 FD004\n",
      "Score:  6928.8290520595065\n",
      "RMSE:  24.07370073095018\n",
      "MSE:  579.5430668833512\n",
      "Penalized RMSE:  27.29202612382833\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.30068, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.30068 to 103.16159, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.16159 to 100.47364, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 100.47364 to 96.29251, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 96.29251 to 91.36711, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 91.36711\n",
      "\n",
      "Epoch 00007: val_loss improved from 91.36711 to 88.13435, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 88.13435 to 81.71704, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 81.71704\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 81.71704\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 81.71704\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 81.71704\n",
      "\n",
      "Epoch 00013: val_loss improved from 81.71704 to 79.26152, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 79.26152 to 71.93146, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 71.93146 to 71.10145, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 71.10145\n",
      "\n",
      "Epoch 00017: val_loss improved from 71.10145 to 56.19617, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 56.19617 to 45.90569, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 45.90569 to 44.79308, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 44.79308 to 31.47901, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 31.47901\n",
      "\n",
      "Epoch 00022: val_loss improved from 31.47901 to 28.33501, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.33501 to 26.32220, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.32220 to 24.48431, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.48431\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.48431 to 24.32320, saving model to ./ckpt_opt/0.5 70.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.32320\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.32320\n",
      "0.5 70 FD004\n",
      "Score:  3187.51455888659\n",
      "RMSE:  23.24896927333665\n",
      "MSE:  540.5145722725517\n",
      "Penalized RMSE:  26.27656569985296\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.08739, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 114.08739 to 105.91404, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.91404 to 99.42155, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 99.42155 to 97.16748, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 97.16748 to 90.48295, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 90.48295 to 88.06969, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.06969 to 79.28606, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 79.28606 to 78.76637, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 78.76637 to 74.81635, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 74.81635 to 72.68062, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 72.68062 to 68.96988, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 68.96988 to 58.00382, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 58.00382\n",
      "\n",
      "Epoch 00014: val_loss improved from 58.00382 to 49.88217, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 49.88217\n",
      "\n",
      "Epoch 00016: val_loss improved from 49.88217 to 43.69824, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 43.69824 to 36.67516, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 36.67516 to 31.46577, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 31.46577\n",
      "\n",
      "Epoch 00020: val_loss improved from 31.46577 to 29.50827, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 29.50827\n",
      "\n",
      "Epoch 00022: val_loss improved from 29.50827 to 25.36297, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 25.36297 to 25.21259, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.21259 to 24.95024, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.95024\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.95024 to 24.28380, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 24.28380 to 23.74597, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.74597\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.74597\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.74597\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.74597\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.74597\n",
      "\n",
      "Epoch 00033: val_loss improved from 23.74597 to 23.58087, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.58087\n",
      "\n",
      "Epoch 00043: val_loss improved from 23.58087 to 23.05310, saving model to ./ckpt_opt/0.5 80.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 23.05310\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 23.05310\n",
      "0.5 80 FD004\n",
      "Score:  3519.849821595079\n",
      "RMSE:  22.30978234225866\n",
      "MSE:  497.72638815895635\n",
      "Penalized RMSE:  25.66605392336557\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 115.43236, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 115.43236 to 94.26841, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 94.26841\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 94.26841\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 94.26841\n",
      "\n",
      "Epoch 00006: val_loss improved from 94.26841 to 88.38416, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 88.38416\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 88.38416\n",
      "\n",
      "Epoch 00009: val_loss improved from 88.38416 to 82.58651, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 82.58651\n",
      "\n",
      "Epoch 00011: val_loss improved from 82.58651 to 79.27531, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 79.27531 to 77.13959, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 77.13959 to 60.97708, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 60.97708\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 60.97708\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 60.97708\n",
      "\n",
      "Epoch 00017: val_loss improved from 60.97708 to 58.75703, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 58.75703 to 52.73429, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 52.73429 to 35.36727, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 35.36727\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 35.36727\n",
      "\n",
      "Epoch 00022: val_loss improved from 35.36727 to 28.23042, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 28.23042\n",
      "\n",
      "Epoch 00024: val_loss improved from 28.23042 to 26.95210, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.95210\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.95210\n",
      "\n",
      "Epoch 00027: val_loss improved from 26.95210 to 24.74277, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 24.74277 to 24.26389, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 24.26389 to 23.36410, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.36410\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.36410\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.36410\n",
      "\n",
      "Epoch 00033: val_loss improved from 23.36410 to 23.33618, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.33618\n",
      "\n",
      "Epoch 00052: val_loss improved from 23.33618 to 22.99853, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.99853\n",
      "\n",
      "Epoch 00054: val_loss improved from 22.99853 to 22.83969, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 22.83969 to 22.81248, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.81248\n",
      "\n",
      "Epoch 00064: val_loss improved from 22.81248 to 22.35546, saving model to ./ckpt_opt/0.5 90.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 22.35546\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 22.35546\n",
      "0.5 90 FD004\n",
      "Score:  2281.9568994709834\n",
      "RMSE:  21.040202985902926\n",
      "MSE:  442.6901416879984\n",
      "Penalized RMSE:  24.263130561489202\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.88481, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.88481 to 106.93718, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 106.93718\n",
      "\n",
      "Epoch 00004: val_loss improved from 106.93718 to 104.78329, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 104.78329 to 87.55164, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 87.55164 to 80.62028, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 80.62028 to 80.47517, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 80.47517 to 71.90301, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 71.90301\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 71.90301\n",
      "\n",
      "Epoch 00011: val_loss improved from 71.90301 to 70.39677, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 70.39677\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 70.39677\n",
      "\n",
      "Epoch 00014: val_loss improved from 70.39677 to 66.34861, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.34861 to 57.80850, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 57.80850\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.80850 to 53.13459, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 53.13459 to 43.14540, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 43.14540 to 38.68380, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.68380 to 35.39221, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 35.39221\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 35.39221\n",
      "\n",
      "Epoch 00023: val_loss improved from 35.39221 to 26.14458, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.14458 to 23.44438, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.44438\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.44438\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.44438\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.44438\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.44438\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.44438\n",
      "\n",
      "Epoch 00031: val_loss improved from 23.44438 to 22.81328, saving model to ./ckpt_opt/0.5 100.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.81328\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.81328\n",
      "0.5 100 FD004\n",
      "Score:  4183.960303802108\n",
      "RMSE:  22.866125198240933\n",
      "MSE:  522.8596815816289\n",
      "Penalized RMSE:  26.61683638162501\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.32348, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.32348 to 95.12349, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 95.12349\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.12349 to 93.57108, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 93.57108 to 91.09745, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 91.09745\n",
      "\n",
      "Epoch 00007: val_loss improved from 91.09745 to 84.21470, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 84.21470 to 81.84234, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 81.84234 to 74.64311, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 74.64311 to 63.93919, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 63.93919 to 61.86061, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 61.86061 to 61.39875, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 61.39875\n",
      "\n",
      "Epoch 00014: val_loss improved from 61.39875 to 51.22423, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 51.22423\n",
      "\n",
      "Epoch 00016: val_loss improved from 51.22423 to 49.74733, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.74733 to 47.51600, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 47.51600\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.51600 to 37.91230, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 37.91230\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.91230 to 35.25766, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 35.25766 to 32.83498, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 32.83498\n",
      "\n",
      "Epoch 00024: val_loss improved from 32.83498 to 32.55088, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 32.55088\n",
      "\n",
      "Epoch 00026: val_loss improved from 32.55088 to 27.17606, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 27.17606 to 23.03082, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.03082\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.03082\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.03082\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.03082\n",
      "\n",
      "Epoch 00032: val_loss improved from 23.03082 to 22.92030, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.92030\n",
      "\n",
      "Epoch 00034: val_loss improved from 22.92030 to 22.56296, saving model to ./ckpt_opt/0.5 110.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.56296\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.56296\n",
      "0.5 110 FD004\n",
      "Score:  3194.30881976004\n",
      "RMSE:  21.19039172133856\n",
      "MSE:  449.0327013037738\n",
      "Penalized RMSE:  24.59820003883878\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 101.94593, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 101.94593 to 96.25209, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 96.25209 to 93.57465, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 93.57465\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 93.57465\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 93.57465\n",
      "\n",
      "Epoch 00007: val_loss improved from 93.57465 to 86.31626, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 86.31626 to 84.81072, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 84.81072 to 77.26887, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 77.26887\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 77.26887\n",
      "\n",
      "Epoch 00012: val_loss improved from 77.26887 to 71.51403, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 71.51403 to 57.85811, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 57.85811\n",
      "\n",
      "Epoch 00015: val_loss improved from 57.85811 to 53.18869, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 53.18869 to 51.45278, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 51.45278\n",
      "\n",
      "Epoch 00018: val_loss improved from 51.45278 to 45.03506, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 45.03506\n",
      "\n",
      "Epoch 00020: val_loss improved from 45.03506 to 36.02237, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 36.02237 to 34.03351, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 34.03351 to 30.47952, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.47952 to 26.86947, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.86947 to 23.27298, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.27298\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.27298\n",
      "\n",
      "Epoch 00027: val_loss improved from 23.27298 to 23.07375, saving model to ./ckpt_opt/0.5 120.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.07375\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.07375\n",
      "0.5 120 FD004\n",
      "Score:  2749.9519780603696\n",
      "RMSE:  21.581733378228957\n",
      "MSE:  465.7712156089619\n",
      "Penalized RMSE:  24.930969504243766\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.58149, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 110.58149 to 107.95556, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.95556 to 97.10247, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.10247 to 93.54297, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 93.54297 to 89.62001, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.62001 to 85.02378, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.02378 to 79.69229, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 79.69229 to 75.96383, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 75.96383 to 72.35994, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 72.35994 to 69.96541, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 69.96541 to 68.81907, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 68.81907\n",
      "\n",
      "Epoch 00013: val_loss improved from 68.81907 to 67.54505, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 67.54505 to 66.98073, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.98073 to 65.77399, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 65.77399 to 56.75710, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 56.75710 to 55.70797, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 55.70797 to 47.97024, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.97024 to 37.42172, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 37.42172 to 33.21928, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 33.21928 to 28.06839, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.06839 to 26.97047, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.97047 to 25.51250, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.51250 to 22.82205, saving model to ./ckpt_opt/0.5 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.82205\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.82205\n",
      "0.5 130 FD004\n",
      "Score:  4525.6270072055395\n",
      "RMSE:  23.93322375340353\n",
      "MSE:  572.7991992304791\n",
      "Penalized RMSE:  27.314691244751064\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 121.92592, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 121.92592 to 110.65057, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 110.65057 to 98.27340, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.27340 to 94.27870, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.27870 to 89.34390, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.34390 to 88.81765, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.81765 to 82.09285, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 82.09285 to 73.87284, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 73.87284 to 69.14158, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 69.14158 to 62.45346, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 62.45346\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 62.45346\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 62.45346\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.45346 to 60.45604, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 60.45604\n",
      "\n",
      "Epoch 00016: val_loss improved from 60.45604 to 49.65083, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 49.65083\n",
      "\n",
      "Epoch 00018: val_loss improved from 49.65083 to 38.88958, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 38.88958\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.88958 to 28.26526, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 28.26526 to 26.99441, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.99441 to 26.26215, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.26215 to 26.22490, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.22490 to 25.09777, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 25.09777 to 24.74048, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.74048 to 23.85609, saving model to ./ckpt_opt/0.6 70.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.85609\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.85609\n",
      "0.6 70 FD004\n",
      "Score:  4038.0074023006946\n",
      "RMSE:  23.335462149973274\n",
      "MSE:  544.5437937528353\n",
      "Penalized RMSE:  27.3800197200981\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 134.22522, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 134.22522 to 120.22717, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 120.22717\n",
      "\n",
      "Epoch 00004: val_loss improved from 120.22717 to 111.89285, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 111.89285 to 97.01401, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 97.01401 to 89.77379, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 89.77379 to 78.87200, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 78.87200 to 73.42338, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 73.42338 to 68.46557, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 68.46557\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 68.46557\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 68.46557\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 68.46557\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 68.46557\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 68.46557\n",
      "\n",
      "Epoch 00016: val_loss improved from 68.46557 to 57.95170, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.95170 to 53.57747, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 53.57747 to 45.30595, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 45.30595 to 45.01550, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 45.01550 to 31.02655, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.02655 to 27.66703, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 27.66703 to 24.98878, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24.98878\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.98878 to 23.75593, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.75593\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.75593\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.75593\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.75593\n",
      "\n",
      "Epoch 00029: val_loss improved from 23.75593 to 23.64724, saving model to ./ckpt_opt/0.6 80.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.64724\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.64724\n",
      "0.6 80 FD004\n",
      "Score:  3500.315509574628\n",
      "RMSE:  22.162491667131917\n",
      "MSE:  491.17603689569165\n",
      "Penalized RMSE:  26.42873122367775\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 118.59288, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 118.59288 to 111.87986, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 111.87986 to 99.64107, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 99.64107 to 92.98762, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.98762 to 92.69704, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.69704 to 80.41193, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 80.41193\n",
      "\n",
      "Epoch 00008: val_loss improved from 80.41193 to 79.80739, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 79.80739 to 65.88026, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 65.88026\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 65.88026\n",
      "\n",
      "Epoch 00012: val_loss improved from 65.88026 to 63.16607, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 63.16607 to 62.44490, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 62.44490\n",
      "\n",
      "Epoch 00015: val_loss improved from 62.44490 to 50.93166, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 50.93166 to 49.54015, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.54015 to 44.15373, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 44.15373 to 41.10284, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 41.10284 to 35.29816, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 35.29816 to 30.07880, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 30.07880\n",
      "\n",
      "Epoch 00022: val_loss improved from 30.07880 to 26.76383, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.76383\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.76383\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.76383\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.76383\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.76383\n",
      "\n",
      "Epoch 00028: val_loss improved from 26.76383 to 26.19335, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.19335\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.19335\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.19335\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.19335\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.19335\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.19335\n",
      "\n",
      "Epoch 00035: val_loss improved from 26.19335 to 23.96444, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 23.96444 to 23.64987, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 23.64987 to 22.83273, saving model to ./ckpt_opt/0.6 90.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.83273\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.83273\n",
      "0.6 90 FD004\n",
      "Score:  2085.8728585206854\n",
      "RMSE:  21.139999583628228\n",
      "MSE:  446.89958239580164\n",
      "Penalized RMSE:  24.586410396692756\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.15643, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.15643 to 107.01608, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.01608 to 100.47442, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 100.47442 to 92.75763, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 92.75763\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.75763 to 83.57424, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.57424 to 79.75408, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 79.75408 to 72.02141, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 72.02141\n",
      "\n",
      "Epoch 00010: val_loss improved from 72.02141 to 66.83084, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 66.83084\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 66.83084\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 66.83084\n",
      "\n",
      "Epoch 00014: val_loss improved from 66.83084 to 62.28882, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 62.28882 to 61.03264, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 61.03264 to 52.22601, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 52.22601\n",
      "\n",
      "Epoch 00018: val_loss improved from 52.22601 to 47.67567, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.67567 to 40.37348, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 40.37348 to 37.93552, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.93552 to 33.63999, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 33.63999 to 31.51961, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 31.51961 to 30.88234, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 30.88234 to 26.14044, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.14044 to 22.42125, saving model to ./ckpt_opt/0.6 100.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.42125\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.42125\n",
      "0.6 100 FD004\n",
      "Score:  2984.908072247332\n",
      "RMSE:  21.199364316297896\n",
      "MSE:  449.41304741512454\n",
      "Penalized RMSE:  24.24543902982106\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 102.88675, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 102.88675 to 98.32597, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 98.32597 to 95.71052, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.71052 to 92.07079, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.07079 to 88.05455, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 88.05455 to 82.64060, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 82.64060 to 80.15634, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 80.15634\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 80.15634\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 80.15634\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 80.15634\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 80.15634\n",
      "\n",
      "Epoch 00013: val_loss improved from 80.15634 to 76.80198, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 76.80198 to 64.00639, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 64.00639 to 59.01147, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 59.01147\n",
      "\n",
      "Epoch 00017: val_loss improved from 59.01147 to 52.53309, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 52.53309 to 47.51706, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 47.51706\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 47.51706\n",
      "\n",
      "Epoch 00021: val_loss improved from 47.51706 to 31.93341, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 31.93341\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 31.93341\n",
      "\n",
      "Epoch 00024: val_loss improved from 31.93341 to 31.63126, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 31.63126\n",
      "\n",
      "Epoch 00026: val_loss improved from 31.63126 to 31.42472, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 31.42472\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 31.42472\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 31.42472\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 31.42472\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 31.42472\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 31.42472\n",
      "\n",
      "Epoch 00033: val_loss improved from 31.42472 to 31.33329, saving model to ./ckpt_opt/0.6 110.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 31.33329\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 31.33329\n",
      "0.6 110 FD004\n",
      "Score:  11205.751491366282\n",
      "RMSE:  29.53777728435464\n",
      "MSE:  872.4802869001369\n",
      "Penalized RMSE:  33.15447765467337\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 104.52768, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 104.52768 to 97.38819, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 97.38819 to 97.15656, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.15656 to 90.79860, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 90.79860\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 90.79860\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 90.79860\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 90.79860\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 90.79860\n",
      "\n",
      "Epoch 00010: val_loss improved from 90.79860 to 78.30443, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 78.30443\n",
      "\n",
      "Epoch 00012: val_loss improved from 78.30443 to 71.09537, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 71.09537 to 65.51612, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 65.51612 to 53.48209, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 53.48209 to 49.86043, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 49.86043\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 49.86043\n",
      "\n",
      "Epoch 00018: val_loss improved from 49.86043 to 35.57944, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 35.57944\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 35.57944\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.57944 to 32.63413, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 32.63413\n",
      "\n",
      "Epoch 00023: val_loss improved from 32.63413 to 26.79366, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.79366 to 25.59663, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.59663\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.59663\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.59663 to 24.92789, saving model to ./ckpt_opt/0.6 120.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.92789\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.92789\n",
      "0.6 120 FD004\n",
      "Score:  5631.391932816781\n",
      "RMSE:  24.863136904095235\n",
      "MSE:  618.1755767117824\n",
      "Penalized RMSE:  29.783341202148094\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 120.49507, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 120.49507 to 102.96094, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 102.96094 to 96.00315, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.00315 to 91.78539, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.78539 to 86.95743, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.95743 to 85.63908, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.63908 to 82.16759, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 82.16759 to 72.88500, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 72.88500\n",
      "\n",
      "Epoch 00010: val_loss improved from 72.88500 to 70.47192, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 70.47192\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 70.47192\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 70.47192\n",
      "\n",
      "Epoch 00014: val_loss improved from 70.47192 to 69.70460, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 69.70460 to 55.67732, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 55.67732 to 44.75211, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 44.75211 to 41.96933, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 41.96933 to 39.25287, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.25287 to 35.78796, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 35.78796\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.78796 to 27.22657, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 27.22657\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.22657\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.22657 to 23.55493, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.55493\n",
      "\n",
      "Epoch 00026: val_loss improved from 23.55493 to 22.76922, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.76922\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.76922\n",
      "\n",
      "Epoch 00029: val_loss improved from 22.76922 to 22.36342, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.36342\n",
      "\n",
      "Epoch 00031: val_loss improved from 22.36342 to 21.76941, saving model to ./ckpt_opt/0.6 130.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 21.76941\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 21.76941\n",
      "0.6 130 FD004\n",
      "Score:  3166.030812222558\n",
      "RMSE:  22.306211658910843\n",
      "MSE:  497.56707857213\n",
      "Penalized RMSE:  26.30853763199902\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 115.67295, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 115.67295\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 115.67295\n",
      "\n",
      "Epoch 00004: val_loss improved from 115.67295 to 101.46142, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 101.46142 to 92.17449, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.17449 to 85.00666, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 85.00666\n",
      "\n",
      "Epoch 00008: val_loss improved from 85.00666 to 78.54203, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 78.54203 to 77.03663, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 77.03663\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 77.03663\n",
      "\n",
      "Epoch 00012: val_loss improved from 77.03663 to 75.46720, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 75.46720 to 59.20557, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 59.20557 to 54.23953, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 54.23953\n",
      "\n",
      "Epoch 00016: val_loss improved from 54.23953 to 45.56876, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 45.56876\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.56876 to 37.18392, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 37.18392\n",
      "\n",
      "Epoch 00020: val_loss improved from 37.18392 to 28.50986, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 28.50986\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 28.50986\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.50986 to 26.96287, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.96287 to 24.20440, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.20440\n",
      "\n",
      "Epoch 00039: val_loss improved from 24.20440 to 24.09371, saving model to ./ckpt_opt/0.7 70.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.09371\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.09371\n",
      "0.7 70 FD004\n",
      "Score:  2974.0201466787075\n",
      "RMSE:  22.250508139324346\n",
      "MSE:  495.08511245813895\n",
      "Penalized RMSE:  25.867573825207153\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 95.23052, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 95.23052\n",
      "\n",
      "Epoch 00003: val_loss improved from 95.23052 to 94.88452, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 94.88452\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 94.88452\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 94.88452\n",
      "\n",
      "Epoch 00007: val_loss improved from 94.88452 to 85.65484, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 85.65484\n",
      "\n",
      "Epoch 00009: val_loss improved from 85.65484 to 64.74665, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 64.74665\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 64.74665\n",
      "\n",
      "Epoch 00012: val_loss improved from 64.74665 to 63.13486, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 63.13486 to 55.52460, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 55.52460\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 55.52460\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 55.52460\n",
      "\n",
      "Epoch 00017: val_loss improved from 55.52460 to 47.81543, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 47.81543 to 40.48036, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 40.48036 to 39.43173, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 39.43173 to 29.15043, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 29.15043\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 29.15043\n",
      "\n",
      "Epoch 00023: val_loss improved from 29.15043 to 26.56923, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.56923 to 23.14364, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.14364\n",
      "\n",
      "Epoch 00054: val_loss improved from 23.14364 to 23.03683, saving model to ./ckpt_opt/0.7 80.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 23.03683\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 23.03683\n",
      "0.7 80 FD004\n",
      "Score:  2381.949413367797\n",
      "RMSE:  21.005456826698495\n",
      "MSE:  441.2292164982944\n",
      "Penalized RMSE:  24.819934226345286\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.84458, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 111.84458\n",
      "\n",
      "Epoch 00003: val_loss improved from 111.84458 to 108.84808, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 108.84808 to 106.65652, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 106.65652 to 95.53139, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 95.53139 to 86.32577, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 86.32577\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 86.32577\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 86.32577\n",
      "\n",
      "Epoch 00010: val_loss improved from 86.32577 to 71.92170, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 71.92170\n",
      "\n",
      "Epoch 00012: val_loss improved from 71.92170 to 70.61956, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 70.61956\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 70.61956\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 70.61956\n",
      "\n",
      "Epoch 00016: val_loss improved from 70.61956 to 70.45578, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 70.45578 to 55.86258, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 55.86258 to 45.76059, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 45.76059\n",
      "\n",
      "Epoch 00020: val_loss improved from 45.76059 to 38.28952, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.28952 to 38.19691, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 38.19691 to 27.40975, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.40975\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.40975 to 24.72331, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.72331\n",
      "\n",
      "Epoch 00037: val_loss improved from 24.72331 to 24.63236, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.63236\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.63236\n",
      "\n",
      "Epoch 00040: val_loss improved from 24.63236 to 24.50277, saving model to ./ckpt_opt/0.7 90.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.50277\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.50277\n",
      "0.7 90 FD004\n",
      "Score:  2620.670556329551\n",
      "RMSE:  22.192517523287222\n",
      "MSE:  492.5078340214104\n",
      "Penalized RMSE:  26.888072236965805\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.33606, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 111.33606\n",
      "\n",
      "Epoch 00003: val_loss improved from 111.33606 to 103.22416, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 103.22416 to 93.43609, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 93.43609 to 90.88999, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 90.88999 to 87.44230, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 87.44230 to 83.06613, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.06613 to 70.60154, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 70.60154 to 68.37688, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 68.37688 to 55.69501, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 55.69501 to 52.26827, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 52.26827\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 52.26827\n",
      "\n",
      "Epoch 00014: val_loss improved from 52.26827 to 48.52932, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 48.52932 to 45.95666, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 45.95666 to 41.33459, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 41.33459\n",
      "\n",
      "Epoch 00018: val_loss improved from 41.33459 to 33.16252, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 33.16252\n",
      "\n",
      "Epoch 00020: val_loss improved from 33.16252 to 32.45739, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.45739 to 31.09171, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 31.09171 to 27.18133, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.18133\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.18133 to 26.06952, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.06952 to 25.77343, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.77343\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.77343 to 25.18859, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.18859\n",
      "\n",
      "Epoch 00036: val_loss improved from 25.18859 to 25.12977, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.12977\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.12977\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.12977\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.12977\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.12977\n",
      "\n",
      "Epoch 00042: val_loss improved from 25.12977 to 24.43320, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.43320\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.43320\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.43320\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.43320\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.43320\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.43320\n",
      "\n",
      "Epoch 00049: val_loss improved from 24.43320 to 24.07372, saving model to ./ckpt_opt/0.7 100.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 24.07372\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 24.07372\n",
      "0.7 100 FD004\n",
      "Score:  3045.1416406463013\n",
      "RMSE:  21.98277154800836\n",
      "MSE:  483.24224493192577\n",
      "Penalized RMSE:  26.18710496648829\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.37393, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.37393 to 106.52758, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 106.52758\n",
      "\n",
      "Epoch 00004: val_loss improved from 106.52758 to 98.43622, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.43622 to 90.33042, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 90.33042 to 87.99057, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 87.99057 to 83.38503, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.38503 to 82.56630, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 82.56630 to 78.27307, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 78.27307 to 69.73428, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 69.73428 to 67.06647, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 67.06647\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 67.06647\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 67.06647\n",
      "\n",
      "Epoch 00015: val_loss improved from 67.06647 to 54.11763, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 54.11763 to 46.36103, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 46.36103\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.36103 to 39.40131, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.40131 to 34.32584, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 34.32584\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.32584 to 26.97180, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.97180 to 25.61380, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 25.61380 to 22.94760, saving model to ./ckpt_opt/0.7 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.94760\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.94760\n",
      "0.7 110 FD004\n",
      "Score:  6904.071248143001\n",
      "RMSE:  22.688761199947287\n",
      "MSE:  514.7798847882334\n",
      "Penalized RMSE:  28.181243438066176\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.79510, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 107.79510 to 104.91731, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 104.91731 to 101.54049, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 101.54049 to 92.00284, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 92.00284\n",
      "\n",
      "Epoch 00006: val_loss improved from 92.00284 to 81.45519, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 81.45519\n",
      "\n",
      "Epoch 00008: val_loss improved from 81.45519 to 76.11658, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 76.11658\n",
      "\n",
      "Epoch 00010: val_loss improved from 76.11658 to 72.07399, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 72.07399 to 70.45255, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 70.45255\n",
      "\n",
      "Epoch 00013: val_loss improved from 70.45255 to 66.87676, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 66.87676 to 56.97120, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 56.97120\n",
      "\n",
      "Epoch 00016: val_loss improved from 56.97120 to 53.28703, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 53.28703\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 53.28703\n",
      "\n",
      "Epoch 00019: val_loss improved from 53.28703 to 52.22287, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 52.22287 to 37.89074, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.89074 to 33.02215, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 33.02215\n",
      "\n",
      "Epoch 00023: val_loss improved from 33.02215 to 31.00116, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 31.00116\n",
      "\n",
      "Epoch 00025: val_loss improved from 31.00116 to 27.49383, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 27.49383 to 24.91326, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.91326\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.82625\n",
      "\n",
      "Epoch 00043: val_loss improved from 24.82625 to 24.26989, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.26989\n",
      "\n",
      "Epoch 00051: val_loss improved from 24.26989 to 24.17208, saving model to ./ckpt_opt/0.7 120.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 24.17208\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 24.17208\n",
      "0.7 120 FD004\n",
      "Score:  2999.559018614382\n",
      "RMSE:  21.512227017676317\n",
      "MSE:  462.77591126004285\n",
      "Penalized RMSE:  24.37072541170815\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.08114, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.08114 to 97.83856, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 97.83856\n",
      "\n",
      "Epoch 00004: val_loss improved from 97.83856 to 95.58889, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 95.58889\n",
      "\n",
      "Epoch 00006: val_loss improved from 95.58889 to 86.37795, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 86.37795\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 86.37795\n",
      "\n",
      "Epoch 00009: val_loss improved from 86.37795 to 85.67428, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 85.67428 to 83.39000, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 83.39000 to 82.58007, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 82.58007\n",
      "\n",
      "Epoch 00013: val_loss improved from 82.58007 to 70.43812, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 70.43812 to 62.17188, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 62.17188\n",
      "\n",
      "Epoch 00016: val_loss improved from 62.17188 to 56.49921, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 56.49921 to 45.30234, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 45.30234\n",
      "\n",
      "Epoch 00019: val_loss improved from 45.30234 to 40.27297, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 40.27297\n",
      "\n",
      "Epoch 00021: val_loss improved from 40.27297 to 34.89998, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 34.89998 to 30.05961, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.05961 to 26.26524, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.26524\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.26524\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.26524\n",
      "\n",
      "Epoch 00027: val_loss improved from 26.26524 to 24.12645, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.12645\n",
      "\n",
      "Epoch 00038: val_loss improved from 24.12645 to 24.04135, saving model to ./ckpt_opt/0.7 130.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.04135\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.04135\n",
      "0.7 130 FD004\n",
      "Score:  4208.645161124887\n",
      "RMSE:  22.345166109376606\n",
      "MSE:  499.30644845563285\n",
      "Penalized RMSE:  27.208802975687068\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.53815, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.53815 to 105.06612, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.06612 to 98.93328, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.93328 to 92.79534, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.79534 to 91.28459, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 91.28459 to 85.61701, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.61701 to 84.23923, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 84.23923\n",
      "\n",
      "Epoch 00009: val_loss improved from 84.23923 to 81.54975, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 81.54975 to 73.02069, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 73.02069\n",
      "\n",
      "Epoch 00012: val_loss improved from 73.02069 to 70.46370, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 70.46370 to 64.76775, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 64.76775 to 57.24347, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 57.24347\n",
      "\n",
      "Epoch 00016: val_loss improved from 57.24347 to 45.63709, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 45.63709\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.63709 to 41.48381, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 41.48381 to 35.12508, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 35.12508 to 32.72993, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.72993 to 30.23661, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 30.23661\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.23661 to 29.22305, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 29.22305 to 28.41349, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 28.41349\n",
      "\n",
      "Epoch 00026: val_loss improved from 28.41349 to 26.77628, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.77628\n",
      "\n",
      "Epoch 00037: val_loss improved from 26.77628 to 26.74456, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.74456\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.74456\n",
      "\n",
      "Epoch 00040: val_loss improved from 26.74456 to 26.56824, saving model to ./ckpt_opt/0.8 70.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 26.56824\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 26.56824\n",
      "0.8 70 FD004\n",
      "Score:  4616.8051061591195\n",
      "RMSE:  24.52560352293316\n",
      "MSE:  601.5052281641115\n",
      "Penalized RMSE:  29.07508139979696\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.99234, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.99234 to 104.73744, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 104.73744\n",
      "\n",
      "Epoch 00004: val_loss improved from 104.73744 to 101.84443, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 101.84443 to 94.89954, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 94.89954 to 92.74362, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 92.74362 to 90.56564, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 90.56564 to 84.75611, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 84.75611\n",
      "\n",
      "Epoch 00010: val_loss improved from 84.75611 to 80.84575, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 80.84575 to 79.57031, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 79.57031 to 71.86095, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 71.86095 to 69.78880, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 69.78880 to 67.37823, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 67.37823 to 62.06784, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 62.06784 to 57.09964, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.09964 to 50.72099, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 50.72099 to 44.13347, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.13347 to 31.11804, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 31.11804 to 28.93273, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 28.93273\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.93273 to 28.26474, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 28.26474\n",
      "\n",
      "Epoch 00024: val_loss improved from 28.26474 to 26.36521, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.36521\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.36521\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.36521\n",
      "\n",
      "Epoch 00028: val_loss improved from 26.36521 to 24.29637, saving model to ./ckpt_opt/0.8 80.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.29637\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.29637\n",
      "0.8 80 FD004\n",
      "Score:  2985.1379606494365\n",
      "RMSE:  22.35833829749845\n",
      "MSE:  499.89529142538584\n",
      "Penalized RMSE:  27.791071226954006\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.27133, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.27133 to 110.07620, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 110.07620\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 110.07620\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 110.07620\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 110.07620\n",
      "\n",
      "Epoch 00007: val_loss improved from 110.07620 to 99.08637, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 99.08637\n",
      "\n",
      "Epoch 00009: val_loss improved from 99.08637 to 86.99780, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 86.99780 to 80.95285, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 80.95285\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 80.95285\n",
      "\n",
      "Epoch 00013: val_loss improved from 80.95285 to 73.54162, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 73.54162\n",
      "\n",
      "Epoch 00015: val_loss improved from 73.54162 to 65.53545, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 65.53545 to 52.37099, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 52.37099 to 46.51969, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 46.51969\n",
      "\n",
      "Epoch 00019: val_loss improved from 46.51969 to 45.44921, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 45.44921 to 29.93740, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 29.93740 to 29.07625, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 29.07625 to 26.38897, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.38897\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.38897 to 25.21202, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.21202\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.21202\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.21202\n",
      "\n",
      "Epoch 00028: val_loss improved from 25.21202 to 24.93044, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.93044\n",
      "\n",
      "Epoch 00039: val_loss improved from 24.93044 to 24.56764, saving model to ./ckpt_opt/0.8 90.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.56764\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.56764\n",
      "0.8 90 FD004\n",
      "Score:  1881.0105769652523\n",
      "RMSE:  20.413177858218052\n",
      "MSE:  416.69783027124373\n",
      "Penalized RMSE:  23.96786475551297\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 121.85985, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 121.85985 to 116.67544, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 116.67544 to 95.89668, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.89668 to 91.80277, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 77.21617\n",
      "\n",
      "Epoch 00013: val_loss improved from 77.21617 to 73.15625, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 73.15625 to 57.97791, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 57.97791\n",
      "\n",
      "Epoch 00016: val_loss improved from 57.97791 to 56.21986, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 56.21986 to 48.84392, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 48.84392 to 39.57481, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.57481 to 33.39342, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 33.39342\n",
      "\n",
      "Epoch 00021: val_loss improved from 33.39342 to 26.93053, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.93053 to 26.61892, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.61892 to 25.19957, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.19957 to 24.82677, saving model to ./ckpt_opt/0.8 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.82677\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.82677\n",
      "0.8 100 FD004\n",
      "Score:  3519.9848511646614\n",
      "RMSE:  22.75652908541912\n",
      "MSE:  517.8596160155264\n",
      "Penalized RMSE:  28.443048071310965\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.82855, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.82855 to 104.31470, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 104.31470 to 97.25213, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 97.25213\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 97.25213\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 97.25213\n",
      "\n",
      "Epoch 00007: val_loss improved from 97.25213 to 94.77853, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 94.77853 to 91.86827, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 91.86827 to 81.83754, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 81.83754 to 73.35038, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 73.35038\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 73.35038\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.35038 to 68.11078, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 68.11078 to 63.84503, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 63.84503 to 55.45301, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 55.45301 to 48.37369, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 48.37369 to 46.32473, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.32473 to 43.09048, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 43.09048 to 38.04474, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.04474 to 34.12415, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.12415 to 30.60771, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 30.60771 to 28.68932, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.68932 to 28.11878, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 28.11878\n",
      "\n",
      "Epoch 00025: val_loss improved from 28.11878 to 27.66981, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 27.66981 to 27.43202, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 27.43202\n",
      "\n",
      "Epoch 00028: val_loss improved from 27.43202 to 27.34527, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.34527\n",
      "\n",
      "Epoch 00030: val_loss improved from 27.34527 to 27.06400, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.06400\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.06400\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.06400\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.06400\n",
      "\n",
      "Epoch 00035: val_loss improved from 27.06400 to 26.76807, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.76807\n",
      "\n",
      "Epoch 00045: val_loss improved from 26.76807 to 26.42435, saving model to ./ckpt_opt/0.8 110.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 26.42435\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 26.42435\n",
      "0.8 110 FD004\n",
      "Score:  4526.887519319692\n",
      "RMSE:  23.76754009003206\n",
      "MSE:  564.8959619312812\n",
      "Penalized RMSE:  28.96776668568673\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.59638, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 114.59638\n",
      "\n",
      "Epoch 00003: val_loss improved from 114.59638 to 105.63123, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 105.63123\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 105.63123\n",
      "\n",
      "Epoch 00006: val_loss improved from 105.63123 to 101.41026, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 101.41026 to 89.97916, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 89.97916\n",
      "\n",
      "Epoch 00009: val_loss improved from 89.97916 to 69.72464, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 69.72464 to 67.15850, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 67.15850 to 57.27225, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 57.27225 to 55.21248, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 55.21248 to 53.15824, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 53.15824 to 51.08473, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 51.08473 to 50.61945, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 50.61945 to 45.38461, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 45.38461 to 42.65746, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 42.65746 to 40.42646, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 40.42646\n",
      "\n",
      "Epoch 00020: val_loss improved from 40.42646 to 31.10943, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 31.10943\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 31.10943\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 31.10943\n",
      "\n",
      "Epoch 00024: val_loss improved from 31.10943 to 31.07439, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 31.07439 to 30.29528, saving model to ./ckpt_opt/0.8 120.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 30.29528\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 30.29528\n",
      "0.8 120 FD004\n",
      "Score:  6755.911863755606\n",
      "RMSE:  26.11495083995355\n",
      "MSE:  681.9906573731906\n",
      "Penalized RMSE:  33.31383567031271\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.74009, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 108.74009\n",
      "\n",
      "Epoch 00003: val_loss improved from 108.74009 to 106.70597, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 106.70597 to 90.54402, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 90.54402 to 86.63859, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.63859 to 83.12071, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.12071 to 78.10078, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 78.10078 to 73.73414, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 73.73414 to 68.05386, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 68.05386\n",
      "\n",
      "Epoch 00011: val_loss improved from 68.05386 to 65.68862, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 65.68862\n",
      "\n",
      "Epoch 00013: val_loss improved from 65.68862 to 59.19358, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 59.19358\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 59.19358\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 59.19358\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 59.19358\n",
      "\n",
      "Epoch 00018: val_loss improved from 59.19358 to 54.19748, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 54.19748 to 47.81631, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 47.81631 to 33.61340, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 33.61340\n",
      "\n",
      "Epoch 00022: val_loss improved from 33.61340 to 26.96033, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.96033 to 24.26513, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 24.26513 to 23.75049, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 23.75049 to 23.62982, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.62982\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.62982\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.62982\n",
      "\n",
      "Epoch 00029: val_loss improved from 23.62982 to 23.07780, saving model to ./ckpt_opt/0.8 130.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.07780\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.07780\n",
      "0.8 130 FD004\n",
      "Score:  2613.667771942093\n",
      "RMSE:  20.1843799809767\n",
      "MSE:  407.409195216453\n",
      "Penalized RMSE:  23.96345288246048\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.71118, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 110.71118 to 106.17063, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 106.17063 to 98.82855, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.82855 to 95.00928, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 95.00928\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 95.00928\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 95.00928\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 95.00928\n",
      "\n",
      "Epoch 00009: val_loss improved from 95.00928 to 84.45411, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 84.45411\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 84.45411\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 84.45411\n",
      "\n",
      "Epoch 00013: val_loss improved from 84.45411 to 82.38879, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 82.38879\n",
      "\n",
      "Epoch 00015: val_loss improved from 82.38879 to 75.56756, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 75.56756\n",
      "\n",
      "Epoch 00017: val_loss improved from 75.56756 to 60.34956, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 60.34956 to 51.97094, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 51.97094 to 35.03942, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 35.03942 to 30.50367, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 30.50367 to 29.15820, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 29.15820\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 29.15820\n",
      "\n",
      "Epoch 00024: val_loss improved from 29.15820 to 27.36746, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.36746\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.36746\n",
      "\n",
      "Epoch 00027: val_loss improved from 27.36746 to 26.86679, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.86679\n",
      "\n",
      "Epoch 00029: val_loss improved from 26.86679 to 25.65783, saving model to ./ckpt_opt/0.9 70.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.65783\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.65783\n",
      "0.9 70 FD004\n",
      "Score:  3346.0872580085597\n",
      "RMSE:  23.223819383383788\n",
      "MSE:  539.3457867520325\n",
      "Penalized RMSE:  26.52041648228621\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.86057, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.86057 to 111.60138, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 111.60138 to 110.23918, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 110.23918 to 109.59583, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 109.59583 to 103.48669, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 103.48669 to 101.81951, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 101.81951 to 92.71549, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 92.71549\n",
      "\n",
      "Epoch 00009: val_loss improved from 92.71549 to 69.72846, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 69.72846\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 69.72846\n",
      "\n",
      "Epoch 00012: val_loss improved from 69.72846 to 62.33491, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 62.33491\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.33491 to 57.96867, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 57.96867 to 49.93171, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 49.93171\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.93171 to 38.73918, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 38.73918\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 38.73918\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.73918 to 31.41929, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.41929 to 24.83790, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 24.83790\n",
      "\n",
      "Epoch 00023: val_loss improved from 24.83790 to 24.01219, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24.01219\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.01219\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.01219 to 23.99342, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.99342\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.99342\n",
      "\n",
      "Epoch 00029: val_loss improved from 23.99342 to 23.91794, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.91794\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.91794\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.91794\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.91794\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.91794\n",
      "\n",
      "Epoch 00035: val_loss improved from 23.91794 to 23.61728, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.61728\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.61728\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.61728\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.61728\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.61728\n",
      "\n",
      "Epoch 00041: val_loss improved from 23.61728 to 23.23975, saving model to ./ckpt_opt/0.9 80.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 23.23975\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 23.23975\n",
      "0.9 80 FD004\n",
      "Score:  2087.8652966436466\n",
      "RMSE:  20.48961066390199\n",
      "MSE:  419.82414515828606\n",
      "Penalized RMSE:  25.578975787716637\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 105.19324, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 105.19324\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.19324 to 102.13938, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 102.13938\n",
      "\n",
      "Epoch 00005: val_loss improved from 102.13938 to 89.97403, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.97403 to 83.58842, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.58842 to 76.40776, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 76.40776\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 76.40776\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 76.40776\n",
      "\n",
      "Epoch 00011: val_loss improved from 76.40776 to 64.02564, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 64.02564 to 61.47269, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 61.47269 to 53.84599, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 53.84599 to 50.15732, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 50.15732\n",
      "\n",
      "Epoch 00016: val_loss improved from 50.15732 to 49.25063, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.25063 to 37.65563, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 37.65563\n",
      "\n",
      "Epoch 00019: val_loss improved from 37.65563 to 33.38063, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 33.38063 to 31.62478, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.62478 to 27.21633, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 27.21633\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.21633\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.21633\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.21633\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.21633\n",
      "\n",
      "Epoch 00027: val_loss improved from 27.21633 to 27.11500, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 27.11500 to 26.32178, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.32178\n",
      "\n",
      "Epoch 00030: val_loss improved from 26.32178 to 26.09625, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 26.09625 to 24.84884, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.84884\n",
      "\n",
      "Epoch 00033: val_loss improved from 24.84884 to 24.82775, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.82775\n",
      "\n",
      "Epoch 00049: val_loss improved from 24.82775 to 24.61168, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.61168\n",
      "\n",
      "Epoch 00051: val_loss improved from 24.61168 to 24.49007, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.49007\n",
      "\n",
      "Epoch 00053: val_loss improved from 24.49007 to 24.35032, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.35032\n",
      "\n",
      "Epoch 00061: val_loss improved from 24.35032 to 24.20490, saving model to ./ckpt_opt/0.9 90.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 24.20490\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 24.20490\n",
      "0.9 90 FD004\n",
      "Score:  2284.8865870396553\n",
      "RMSE:  21.017676336513507\n",
      "MSE:  441.7427185864398\n",
      "Penalized RMSE:  25.972066746056555\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.78391, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 110.78391 to 103.70721, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 103.70721\n",
      "\n",
      "Epoch 00004: val_loss improved from 103.70721 to 99.47115, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 99.47115\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 99.47115\n",
      "\n",
      "Epoch 00007: val_loss improved from 99.47115 to 88.42290, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 88.42290 to 78.65886, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 78.65886\n",
      "\n",
      "Epoch 00010: val_loss improved from 78.65886 to 74.13342, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 74.13342 to 70.96944, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 70.96944 to 58.45184, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 58.45184\n",
      "\n",
      "Epoch 00014: val_loss improved from 58.45184 to 53.55926, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 53.55926 to 46.96489, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 46.96489\n",
      "\n",
      "Epoch 00017: val_loss improved from 46.96489 to 45.98579, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.98579 to 42.32015, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 42.32015 to 32.53446, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 32.53446 to 28.55572, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 28.55572\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.55572 to 25.37263, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 25.37263\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.37263 to 24.82879, saving model to ./ckpt_opt/0.9 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.82879\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.82879\n",
      "0.9 100 FD004\n",
      "Score:  4578.3719894217\n",
      "RMSE:  23.143230505039956\n",
      "MSE:  535.6091182094119\n",
      "Penalized RMSE:  29.632406079780353\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 128.03144, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 128.03144 to 107.44817, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.44817 to 95.18178, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.18178 to 91.17241, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.17241 to 85.00302, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 85.00302\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.00302 to 79.72262, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 79.72262\n",
      "\n",
      "Epoch 00009: val_loss improved from 79.72262 to 73.01104, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 73.01104 to 61.47300, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 61.47300\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 61.47300\n",
      "\n",
      "Epoch 00013: val_loss improved from 61.47300 to 60.72207, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 60.72207 to 54.41870, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 54.41870 to 51.48192, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 51.48192 to 44.09952, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 44.09952\n",
      "\n",
      "Epoch 00018: val_loss improved from 44.09952 to 38.91420, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 38.91420\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.91420 to 37.17467, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.17467 to 32.12835, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 32.12835\n",
      "\n",
      "Epoch 00023: val_loss improved from 32.12835 to 27.69737, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.69737\n",
      "\n",
      "Epoch 00025: val_loss improved from 27.69737 to 26.53105, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 26.53105 to 25.71608, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.71608 to 25.58888, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.58888\n",
      "\n",
      "Epoch 00029: val_loss improved from 25.58888 to 25.33715, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 25.33715 to 24.19533, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.19533\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.19533\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.19533\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.19533\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.19533\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.19533\n",
      "\n",
      "Epoch 00037: val_loss improved from 24.19533 to 24.04064, saving model to ./ckpt_opt/0.9 110.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.04064\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.04064\n",
      "0.9 110 FD004\n",
      "Score:  2511.7847900455185\n",
      "RMSE:  19.761590823417727\n",
      "MSE:  390.5204718721877\n",
      "Penalized RMSE:  22.790968395046985\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.82000, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 114.82000 to 110.40872, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 110.40872 to 104.06588, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 104.06588 to 98.34259, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.34259 to 88.75625, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 88.75625 to 85.75532, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 85.75532\n",
      "\n",
      "Epoch 00008: val_loss improved from 85.75532 to 72.98744, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 72.98744 to 68.03139, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 68.03139\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 68.03139\n",
      "\n",
      "Epoch 00012: val_loss improved from 68.03139 to 65.75952, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 65.75952 to 64.01924, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 64.01924 to 56.97381, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 56.97381 to 55.84549, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 55.84549\n",
      "\n",
      "Epoch 00017: val_loss improved from 55.84549 to 47.42174, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 47.42174 to 43.53811, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 43.53811\n",
      "\n",
      "Epoch 00020: val_loss improved from 43.53811 to 31.66022, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.66022 to 29.37620, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 29.37620\n",
      "\n",
      "Epoch 00023: val_loss improved from 29.37620 to 26.51316, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.51316 to 25.10822, saving model to ./ckpt_opt/0.9 120.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.10822\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.10822\n",
      "0.9 120 FD004\n",
      "Score:  2428.39044818239\n",
      "RMSE:  21.233545234000363\n",
      "MSE:  450.86344320433955\n",
      "Penalized RMSE:  26.775816915819753\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 106.63483, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 106.63483 to 101.34041, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 101.34041 to 95.32077, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.32077 to 90.83322, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 90.83322 to 85.92743, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 85.92743 to 82.84189, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 82.84189 to 75.11507, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 75.11507 to 73.78695, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 73.78695\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 73.78695\n",
      "\n",
      "Epoch 00011: val_loss improved from 73.78695 to 64.99203, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 64.99203\n",
      "\n",
      "Epoch 00013: val_loss improved from 64.99203 to 62.25756, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.25756 to 54.89956, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 54.89956 to 50.63187, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 50.63187 to 49.31972, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.31972 to 39.39094, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 39.39094\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 39.39094\n",
      "\n",
      "Epoch 00020: val_loss improved from 39.39094 to 37.48921, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.48921 to 34.54228, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 34.54228\n",
      "\n",
      "Epoch 00023: val_loss improved from 34.54228 to 33.18259, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 33.18259\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 33.18259\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 33.18259\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 33.18259\n",
      "\n",
      "Epoch 00028: val_loss improved from 33.18259 to 31.18206, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 31.18206 to 30.17612, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 30.17612\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 30.17612\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 30.17612\n",
      "\n",
      "Epoch 00033: val_loss improved from 30.17612 to 30.08138, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 30.08138\n",
      "\n",
      "Epoch 00041: val_loss improved from 30.08138 to 29.41815, saving model to ./ckpt_opt/0.9 130.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 29.41815\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 29.41815\n",
      "0.9 130 FD004\n",
      "Score:  9101.62485947901\n",
      "RMSE:  25.41177191695862\n",
      "MSE:  645.7581519595268\n",
      "Penalized RMSE:  30.964486167245195\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.79549, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.79549 to 105.99227, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.99227 to 99.41829, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 99.41829 to 98.34848, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.34848 to 89.72968, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 89.72968\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 89.72968\n",
      "\n",
      "Epoch 00008: val_loss improved from 89.72968 to 82.48280, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 82.48280\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 82.48280\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 82.48280\n",
      "\n",
      "Epoch 00012: val_loss improved from 82.48280 to 73.83388, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.83388 to 68.06749, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 68.06749\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 68.06749\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 68.06749\n",
      "\n",
      "Epoch 00017: val_loss improved from 68.06749 to 48.36780, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 48.36780 to 35.30086, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 35.30086\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 35.30086\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.30086 to 28.69705, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.69705 to 26.65793, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.65793 to 26.31489, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.31489\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.31489 to 24.81709, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.81709\n",
      "\n",
      "Epoch 00037: val_loss improved from 24.81709 to 24.65789, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.65789\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.65789\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.65789\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.65789\n",
      "\n",
      "Epoch 00042: val_loss improved from 24.65789 to 24.53844, saving model to ./ckpt_opt/1.0 70.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.53844\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.53844\n",
      "1.0 70 FD004\n",
      "Score:  3954.5161446866723\n",
      "RMSE:  23.027501808042924\n",
      "MSE:  530.2658395194202\n",
      "Penalized RMSE:  28.504013514421693\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.34454, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.34454 to 105.54137, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.54137 to 96.44148, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.44148 to 92.44860, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.44860 to 86.82977, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.82977 to 80.36489, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 80.36489 to 74.73633, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 74.73633 to 69.56438, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 69.56438 to 65.07830, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 65.07830 to 58.82593, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 58.82593 to 58.74767, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 58.74767\n",
      "\n",
      "Epoch 00013: val_loss improved from 58.74767 to 58.72789, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 58.72789 to 53.56177, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 53.56177\n",
      "\n",
      "Epoch 00016: val_loss improved from 53.56177 to 47.31651, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 47.31651 to 42.87546, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 42.87546 to 37.53642, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 37.53642\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 37.53642\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.53642 to 29.00385, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 29.00385 to 27.35037, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.35037\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.35037\n",
      "\n",
      "Epoch 00025: val_loss improved from 27.35037 to 26.92049, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 26.92049 to 26.44384, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.44384\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.44384\n",
      "\n",
      "Epoch 00029: val_loss improved from 26.44384 to 25.75726, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.75726\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.75726\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.75726\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.75726\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.75726\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.75726\n",
      "\n",
      "Epoch 00036: val_loss improved from 25.75726 to 25.69422, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 25.69422 to 25.18916, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.18916\n",
      "\n",
      "Epoch 00049: val_loss improved from 25.18916 to 24.58081, saving model to ./ckpt_opt/1.0 80.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 24.58081\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 24.58081\n",
      "1.0 80 FD004\n",
      "Score:  2764.552183874106\n",
      "RMSE:  21.508102647953734\n",
      "MSE:  462.59847951491446\n",
      "Penalized RMSE:  26.5343315708778\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.59417, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.59417 to 101.03845, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 101.03845 to 93.48871, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.48871 to 89.03353, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 89.03353 to 88.92067, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 88.92067\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.92067 to 82.67060, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 82.67060\n",
      "\n",
      "Epoch 00009: val_loss improved from 82.67060 to 75.83289, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 75.83289\n",
      "\n",
      "Epoch 00011: val_loss improved from 75.83289 to 68.51155, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 68.51155\n",
      "\n",
      "Epoch 00013: val_loss improved from 68.51155 to 67.25533, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 67.25533 to 60.56966, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 60.56966 to 52.56711, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 52.56711 to 50.90369, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 50.90369\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 50.90369\n",
      "\n",
      "Epoch 00019: val_loss improved from 50.90369 to 39.81769, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 39.81769 to 37.39099, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 37.39099 to 33.30963, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 33.30963 to 31.09158, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 31.09158\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 31.09158\n",
      "\n",
      "Epoch 00025: val_loss improved from 31.09158 to 29.55953, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 29.55953\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 29.55953\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 29.55953\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 29.55953\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 29.55953\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 29.55953\n",
      "\n",
      "Epoch 00032: val_loss improved from 29.55953 to 29.54909, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 29.54909\n",
      "\n",
      "Epoch 00034: val_loss improved from 29.54909 to 29.17443, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 29.17443 to 28.90442, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 28.90442\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 28.90442\n",
      "\n",
      "Epoch 00038: val_loss improved from 28.90442 to 28.28154, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 28.28154\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 28.28154\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 28.28154\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 28.28154\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 28.28154\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 28.28154\n",
      "\n",
      "Epoch 00045: val_loss improved from 28.28154 to 27.73149, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 27.73149\n",
      "\n",
      "Epoch 00047: val_loss improved from 27.73149 to 26.97371, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.97371\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.97371\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.97371\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.97371\n",
      "\n",
      "Epoch 00052: val_loss improved from 26.97371 to 26.57667, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 26.57667\n",
      "\n",
      "Epoch 00067: val_loss improved from 26.57667 to 26.49464, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 26.49464\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 26.49464\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 26.49464\n",
      "\n",
      "Epoch 00071: val_loss improved from 26.49464 to 26.44399, saving model to ./ckpt_opt/1.0 90.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 26.44399\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 26.44399\n",
      "1.0 90 FD004\n",
      "Score:  3222.591464069632\n",
      "RMSE:  22.94828261303348\n",
      "MSE:  526.6236748876547\n",
      "Penalized RMSE:  28.720993934173276\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.35342, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 107.35342\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.35342 to 100.22769, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 100.22769 to 95.87038, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 95.87038 to 89.88337, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.88337 to 88.25519, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.25519 to 78.33496, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 78.33496 to 77.66202, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 77.66202 to 69.32300, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 69.32300\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 69.32300\n",
      "\n",
      "Epoch 00012: val_loss improved from 69.32300 to 62.01468, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 62.01468 to 55.34567, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 55.34567\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 55.34567\n",
      "\n",
      "Epoch 00016: val_loss improved from 55.34567 to 49.83545, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.83545 to 35.81657, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 35.81657 to 34.13620, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 34.13620\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 34.13620\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.13620 to 31.45508, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 31.45508\n",
      "\n",
      "Epoch 00023: val_loss improved from 31.45508 to 30.99728, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 30.99728 to 30.72717, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 30.72717\n",
      "\n",
      "Epoch 00026: val_loss improved from 30.72717 to 29.72581, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 29.72581\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 29.72581\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 29.72581\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 29.72581\n",
      "\n",
      "Epoch 00031: val_loss improved from 29.72581 to 29.63053, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 29.63053 to 28.97963, saving model to ./ckpt_opt/1.0 100.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 28.97963\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 28.97963\n",
      "1.0 100 FD004\n",
      "Score:  7768.694230810273\n",
      "RMSE:  25.03798835621382\n",
      "MSE:  626.9008609258988\n",
      "Penalized RMSE:  32.02935106996431\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.03178, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 110.03178 to 103.01796, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.01796 to 98.91535, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.91535 to 96.79610, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 96.79610 to 92.10345, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 92.10345\n",
      "\n",
      "Epoch 00007: val_loss improved from 92.10345 to 89.69426, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 89.69426\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 89.69426\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 89.69426\n",
      "\n",
      "Epoch 00011: val_loss improved from 89.69426 to 80.71136, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 80.71136 to 62.72057, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 62.72057\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.72057 to 56.36212, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 56.36212\n",
      "\n",
      "Epoch 00016: val_loss improved from 56.36212 to 46.28069, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 46.28069\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.28069 to 42.37742, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 42.37742\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 42.37742\n",
      "\n",
      "Epoch 00021: val_loss improved from 42.37742 to 37.67458, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 37.67458\n",
      "\n",
      "Epoch 00023: val_loss improved from 37.67458 to 36.70104, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 36.70104 to 34.80063, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 34.80063 to 34.33095, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 34.33095 to 33.27493, saving model to ./ckpt_opt/1.0 110.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 33.27493\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 33.27493\n",
      "1.0 110 FD004\n",
      "Score:  10192.968266537486\n",
      "RMSE:  27.46001902870145\n",
      "MSE:  754.0526450566457\n",
      "Penalized RMSE:  34.614047961936585\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 103.59764, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 103.59764\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.59764 to 98.67378, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.67378 to 94.93307, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.93307 to 91.62003, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 91.62003 to 90.08290, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 90.08290 to 76.79612, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.79612 to 76.63827, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 76.63827\n",
      "\n",
      "Epoch 00010: val_loss improved from 76.63827 to 73.75943, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 73.75943\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 73.75943\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 73.75943\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 73.75943\n",
      "\n",
      "Epoch 00015: val_loss improved from 73.75943 to 52.82437, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 52.82437\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 52.82437\n",
      "\n",
      "Epoch 00018: val_loss improved from 52.82437 to 37.64719, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 37.64719\n",
      "\n",
      "Epoch 00020: val_loss improved from 37.64719 to 35.78277, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 35.78277\n",
      "\n",
      "Epoch 00022: val_loss improved from 35.78277 to 27.75902, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.75902\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.75902\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.75902\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.75902\n",
      "\n",
      "Epoch 00027: val_loss improved from 27.75902 to 26.13215, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.13215\n",
      "\n",
      "Epoch 00029: val_loss improved from 26.13215 to 25.79357, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.79357\n",
      "\n",
      "Epoch 00031: val_loss improved from 25.79357 to 23.33122, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.33122\n",
      "\n",
      "Epoch 00039: val_loss improved from 23.33122 to 23.00991, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.00991\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.00991\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.00991\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.00991\n",
      "\n",
      "Epoch 00044: val_loss improved from 23.00991 to 22.92745, saving model to ./ckpt_opt/1.0 120.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 22.92745\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 22.92745\n",
      "1.0 120 FD004\n",
      "Score:  2622.5775440816815\n",
      "RMSE:  20.760916269851286\n",
      "MSE:  431.01564436377583\n",
      "Penalized RMSE:  26.81486805827847\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 110.09839, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 110.09839\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 110.09839\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 110.09839\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 110.09839\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 110.09839\n",
      "\n",
      "Epoch 00007: val_loss improved from 110.09839 to 104.10302, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 104.10302\n",
      "\n",
      "Epoch 00009: val_loss improved from 104.10302 to 99.44518, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 99.44518 to 70.29861, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 70.29861\n",
      "\n",
      "Epoch 00012: val_loss improved from 70.29861 to 57.46345, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 57.46345\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 57.46345\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 57.46345\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 57.46345\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.46345 to 55.37567, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 55.37567 to 45.22821, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 45.22821 to 40.48321, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 40.48321 to 38.60864, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.60864 to 31.87741, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 31.87741 to 28.45298, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.45298 to 24.56816, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24.56816\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.56816\n",
      "\n",
      "Epoch 00026: val_loss improved from 24.56816 to 24.16033, saving model to ./ckpt_opt/1.0 130.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.16033\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.16033\n",
      "1.0 130 FD004\n",
      "Score:  2879.0075316954567\n",
      "RMSE:  21.257711418780286\n",
      "MSE:  451.89029476414174\n",
      "Penalized RMSE:  26.9503674251866\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 115.75385, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 115.75385 to 95.33957, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 95.33957\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.33957 to 93.90655, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 93.90655 to 89.71700, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.71700 to 84.67417, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 84.67417 to 78.56701, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 78.56701 to 70.41654, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 70.41654\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 70.41654\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 70.41654\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 70.41654\n",
      "\n",
      "Epoch 00013: val_loss improved from 70.41654 to 69.55087, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 69.55087 to 59.70801, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 59.70801 to 48.50166, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 48.50166\n",
      "\n",
      "Epoch 00017: val_loss improved from 48.50166 to 46.75550, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 46.75550 to 40.04190, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 40.04190\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 40.04190\n",
      "\n",
      "Epoch 00021: val_loss improved from 40.04190 to 32.60654, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 32.60654 to 27.83351, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 27.83351 to 26.38598, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.38598\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.38598\n",
      "\n",
      "Epoch 00026: val_loss improved from 26.38598 to 25.94195, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.94195\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.94195\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.94195\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.94195\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.94195\n",
      "\n",
      "Epoch 00032: val_loss improved from 25.94195 to 25.66332, saving model to ./ckpt_opt/1.1 70.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.66332\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.66332\n",
      "1.1 70 FD004\n",
      "Score:  2954.169463234106\n",
      "RMSE:  22.401388842678127\n",
      "MSE:  501.8222220808641\n",
      "Penalized RMSE:  27.294391918066847\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.47736, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.47736 to 103.46725, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.46725 to 101.70975, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 101.70975 to 98.07651, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.07651 to 88.90762, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 88.90762 to 83.55806, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.55806 to 80.65552, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 80.65552 to 71.25054, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 71.25054 to 67.46670, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 67.46670 to 61.87765, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 61.87765\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 61.87765\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 61.87765\n",
      "\n",
      "Epoch 00014: val_loss improved from 61.87765 to 59.39033, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 59.39033\n",
      "\n",
      "Epoch 00016: val_loss improved from 59.39033 to 50.55373, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 50.55373 to 44.15182, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 44.15182 to 42.38247, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 42.38247 to 37.19626, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 37.19626 to 34.93975, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.93975 to 33.11916, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 33.11916 to 30.31258, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 30.31258\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 30.31258\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 30.31258\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 30.31258\n",
      "\n",
      "Epoch 00027: val_loss improved from 30.31258 to 29.27152, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 29.27152 to 27.29769, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.29769\n",
      "\n",
      "Epoch 00030: val_loss improved from 27.29769 to 27.26466, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 27.26466 to 27.12831, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.12831\n",
      "\n",
      "Epoch 00033: val_loss improved from 27.12831 to 26.79602, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.79602\n",
      "\n",
      "Epoch 00035: val_loss improved from 26.79602 to 26.24440, saving model to ./ckpt_opt/1.1 80.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 26.24440\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 26.24440\n",
      "1.1 80 FD004\n",
      "Score:  2803.757524217909\n",
      "RMSE:  21.995774804174562\n",
      "MSE:  483.81410923596053\n",
      "Penalized RMSE:  27.911428588346357\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 129.58031, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 129.58031 to 124.75169, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 124.75169 to 101.05248, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 101.05248 to 94.10563, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.10563 to 91.89148, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 91.89148 to 83.80551, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.80551 to 77.04715, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 77.04715 to 70.43596, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 70.43596\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 70.43596\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 70.43596\n",
      "\n",
      "Epoch 00012: val_loss improved from 70.43596 to 68.60344, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 68.60344\n",
      "\n",
      "Epoch 00014: val_loss improved from 68.60344 to 62.78829, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 62.78829 to 55.84780, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 55.84780\n",
      "\n",
      "Epoch 00017: val_loss improved from 55.84780 to 40.99850, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 40.99850\n",
      "\n",
      "Epoch 00019: val_loss improved from 40.99850 to 33.38228, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 33.38228\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 33.38228\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 33.38228\n",
      "\n",
      "Epoch 00023: val_loss improved from 33.38228 to 29.08479, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 29.08479 to 27.13919, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.13919\n",
      "\n",
      "Epoch 00026: val_loss improved from 27.13919 to 25.47174, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.47174\n",
      "\n",
      "Epoch 00038: val_loss improved from 25.47174 to 25.39329, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 25.39329 to 24.92034, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 24.92034 to 24.63296, saving model to ./ckpt_opt/1.1 90.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.63296\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.63296\n",
      "1.1 90 FD004\n",
      "Score:  2278.129691450024\n",
      "RMSE:  21.205670565465475\n",
      "MSE:  449.68046413104884\n",
      "Penalized RMSE:  26.944973279733453\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.33507, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 107.33507 to 93.61216, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 93.61216\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.61216 to 92.21955, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.21955 to 88.64425, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 88.64425 to 86.60756, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 86.60756\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 86.60756\n",
      "\n",
      "Epoch 00009: val_loss improved from 86.60756 to 77.20053, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 77.20053\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 77.20053\n",
      "\n",
      "Epoch 00012: val_loss improved from 77.20053 to 76.41624, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 76.41624 to 70.86023, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 70.86023\n",
      "\n",
      "Epoch 00015: val_loss improved from 70.86023 to 54.15421, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 54.15421 to 52.35014, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 52.35014 to 51.93796, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 51.93796 to 47.53115, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.53115 to 43.80884, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 43.80884\n",
      "\n",
      "Epoch 00021: val_loss improved from 43.80884 to 28.66865, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 28.66865\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.66865 to 27.16310, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.16310 to 25.65379, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 25.65379 to 25.51191, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.51191\n",
      "\n",
      "Epoch 00027: val_loss improved from 25.51191 to 24.59045, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.59045\n",
      "\n",
      "Epoch 00036: val_loss improved from 24.59045 to 24.42374, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.42374\n",
      "\n",
      "Epoch 00038: val_loss improved from 24.42374 to 24.37430, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.37430\n",
      "\n",
      "Epoch 00040: val_loss improved from 24.37430 to 24.23944, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 24.23944 to 24.18026, saving model to ./ckpt_opt/1.1 100.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.18026\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.18026\n",
      "1.1 100 FD004\n",
      "Score:  3037.88937230837\n",
      "RMSE:  21.455551564067033\n",
      "MSE:  460.3406929183393\n",
      "Penalized RMSE:  27.70286060860836\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.41793, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.41793 to 98.22628, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 98.22628 to 92.76511, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 92.76511 to 88.31284, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 88.31284 to 82.89733, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 82.89733 to 76.36486, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 76.36486 to 71.08335, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 71.08335 to 66.40236, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 66.40236 to 65.21515, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 65.21515 to 60.13352, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 60.13352 to 58.35695, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 58.35695 to 53.26223, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 53.26223\n",
      "\n",
      "Epoch 00014: val_loss improved from 53.26223 to 50.80327, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 50.80327 to 44.26247, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 44.26247 to 44.18531, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 44.18531\n",
      "\n",
      "Epoch 00018: val_loss improved from 44.18531 to 37.49543, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 37.49543 to 36.07903, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 36.07903 to 31.41775, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 31.41775\n",
      "\n",
      "Epoch 00022: val_loss improved from 31.41775 to 29.44191, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 29.44191 to 27.64594, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 27.64594 to 26.62013, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.62013 to 26.56122, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.56122\n",
      "\n",
      "Epoch 00027: val_loss improved from 26.56122 to 24.93696, saving model to ./ckpt_opt/1.1 110.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.93696\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.93696\n",
      "1.1 110 FD004\n",
      "Score:  2794.7264442326145\n",
      "RMSE:  21.739475812419485\n",
      "MSE:  472.60480859877185\n",
      "Penalized RMSE:  28.08565344151604\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 106.67948, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 106.67948 to 98.17734, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 98.17734\n",
      "\n",
      "Epoch 00004: val_loss improved from 98.17734 to 94.01682, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.01682 to 93.47597, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 93.47597\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 93.47597\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 93.47597\n",
      "\n",
      "Epoch 00009: val_loss improved from 93.47597 to 84.54782, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 84.54782\n",
      "\n",
      "Epoch 00011: val_loss improved from 84.54782 to 77.44942, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 77.44942 to 71.59504, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 71.59504\n",
      "\n",
      "Epoch 00014: val_loss improved from 71.59504 to 62.16740, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 62.16740\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 62.16740\n",
      "\n",
      "Epoch 00017: val_loss improved from 62.16740 to 44.30520, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 44.30520\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.30520 to 35.86783, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 35.86783\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.86783 to 35.15600, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 35.15600 to 30.09511, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.09511 to 26.94072, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.94072 to 26.14779, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.14779\n",
      "\n",
      "Epoch 00038: val_loss improved from 26.14779 to 26.03878, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.03878\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.03878\n",
      "\n",
      "Epoch 00041: val_loss improved from 26.03878 to 25.92399, saving model to ./ckpt_opt/1.1 120.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 25.92399\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 25.92399\n",
      "1.1 120 FD004\n",
      "Score:  3323.4509724767786\n",
      "RMSE:  22.309030028557135\n",
      "MSE:  497.69282081506395\n",
      "Penalized RMSE:  28.416761410984723\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.32592, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 107.32592 to 104.56438, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 104.56438 to 96.09004, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.09004 to 94.97946, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.97946 to 90.47022, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 90.47022 to 88.56246, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.56246 to 88.42469, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 88.42469 to 78.57896, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 78.57896 to 77.10498, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 77.10498 to 71.13096, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 71.13096 to 68.75477, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 68.75477 to 66.37174, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 66.37174\n",
      "\n",
      "Epoch 00014: val_loss improved from 66.37174 to 56.58920, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 56.58920 to 51.57159, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 51.57159 to 45.55973, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 45.55973\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.55973 to 39.80780, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 39.80780\n",
      "\n",
      "Epoch 00020: val_loss improved from 39.80780 to 35.80027, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 35.80027\n",
      "\n",
      "Epoch 00022: val_loss improved from 35.80027 to 29.97750, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 29.97750\n",
      "\n",
      "Epoch 00024: val_loss improved from 29.97750 to 27.10072, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 27.10072 to 26.31017, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.31017\n",
      "\n",
      "Epoch 00037: val_loss improved from 26.31017 to 26.18904, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.18904\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.18904\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.18904\n",
      "\n",
      "Epoch 00041: val_loss improved from 26.18904 to 26.05314, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 26.05314 to 25.94338, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 25.94338 to 25.78710, saving model to ./ckpt_opt/1.1 130.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 25.78710\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 25.78710\n",
      "1.1 130 FD004\n",
      "Score:  3103.076819023246\n",
      "RMSE:  21.79120410134446\n",
      "MSE:  474.85657618645155\n",
      "Penalized RMSE:  27.17375436967838\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 119.64034, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 119.64034 to 105.27233, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 105.27233 to 102.00246, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 102.00246 to 97.09882, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 97.09882\n",
      "\n",
      "Epoch 00006: val_loss improved from 97.09882 to 92.45151, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 92.45151 to 82.00658, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 82.00658\n",
      "\n",
      "Epoch 00009: val_loss improved from 82.00658 to 70.83513, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 70.83513\n",
      "\n",
      "Epoch 00011: val_loss improved from 70.83513 to 68.06224, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 68.06224\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 68.06224\n",
      "\n",
      "Epoch 00014: val_loss improved from 68.06224 to 67.18961, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 67.18961 to 56.20632, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 56.20632\n",
      "\n",
      "Epoch 00017: val_loss improved from 56.20632 to 39.17678, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 39.17678\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.17678 to 33.14976, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 33.14976 to 29.57413, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 29.57413 to 26.64321, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.64321 to 25.50306, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 25.50306 to 25.43682, saving model to ./ckpt_opt/1.2 70.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.43682\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.43682\n",
      "1.2 70 FD004\n",
      "Score:  3433.0230856472876\n",
      "RMSE:  23.740984180377026\n",
      "MSE:  563.6343298529122\n",
      "Penalized RMSE:  31.393105978512565\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.03701, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.03701 to 94.14421, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 94.14421 to 93.13278, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.13278 to 89.96759, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 89.96759 to 88.70098, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 88.70098\n",
      "\n",
      "Epoch 00007: val_loss improved from 88.70098 to 86.73059, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 86.73059\n",
      "\n",
      "Epoch 00009: val_loss improved from 86.73059 to 69.11287, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 69.11287\n",
      "\n",
      "Epoch 00017: val_loss improved from 69.11287 to 67.56751, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 67.56751\n",
      "\n",
      "Epoch 00019: val_loss improved from 67.56751 to 58.42348, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 58.42348 to 55.61214, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 55.61214 to 46.73433, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 46.73433 to 35.52123, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 35.52123\n",
      "\n",
      "Epoch 00024: val_loss improved from 35.52123 to 34.70320, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 34.70320 to 33.61797, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 33.61797\n",
      "\n",
      "Epoch 00027: val_loss improved from 33.61797 to 31.19890, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 31.19890\n",
      "\n",
      "Epoch 00029: val_loss improved from 31.19890 to 31.17651, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 31.17651 to 30.84322, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 30.84322\n",
      "\n",
      "Epoch 00040: val_loss improved from 30.84322 to 30.64806, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 30.64806\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 30.64806\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 30.64806\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 30.64806\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 30.64806\n",
      "\n",
      "Epoch 00046: val_loss improved from 30.64806 to 30.64034, saving model to ./ckpt_opt/1.2 80.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 30.64034\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 30.64034\n",
      "1.2 80 FD004\n",
      "Score:  7145.714736380587\n",
      "RMSE:  24.98959013412052\n",
      "MSE:  624.4796150713336\n",
      "Penalized RMSE:  31.227040672805792\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.91019, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 111.91019\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 111.91019\n",
      "\n",
      "Epoch 00004: val_loss improved from 111.91019 to 105.33031, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 105.33031\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 105.33031\n",
      "\n",
      "Epoch 00007: val_loss improved from 105.33031 to 83.79377, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.79377 to 77.23158, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 77.23158\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 77.23158\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 77.23158\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 77.23158\n",
      "\n",
      "Epoch 00013: val_loss improved from 77.23158 to 62.90869, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 62.90869\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 62.90869\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 62.90869\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 62.90869\n",
      "\n",
      "Epoch 00018: val_loss improved from 62.90869 to 61.82970, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 61.82970\n",
      "\n",
      "Epoch 00020: val_loss improved from 61.82970 to 47.03161, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 47.03161 to 32.88477, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 32.88477\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 32.88477\n",
      "\n",
      "Epoch 00024: val_loss improved from 32.88477 to 29.03663, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 29.03663 to 28.85154, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 28.85154 to 26.57196, saving model to ./ckpt_opt/1.2 90.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.57196\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.57196\n",
      "1.2 90 FD004\n",
      "Score:  3154.046548818543\n",
      "RMSE:  21.95444352224673\n",
      "MSE:  481.9975903715214\n",
      "Penalized RMSE:  28.599336906671965\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 113.58627, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 113.58627 to 103.05340, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.05340 to 102.42699, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 102.42699\n",
      "\n",
      "Epoch 00005: val_loss improved from 102.42699 to 102.29159, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 102.29159 to 92.49147, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 92.49147\n",
      "\n",
      "Epoch 00008: val_loss improved from 92.49147 to 78.25389, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 78.25389\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 78.25389\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 78.25389\n",
      "\n",
      "Epoch 00012: val_loss improved from 78.25389 to 73.77735, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.77735 to 67.30352, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 67.30352 to 66.94922, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.94922 to 61.95500, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 61.95500 to 48.37254, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 48.37254\n",
      "\n",
      "Epoch 00018: val_loss improved from 48.37254 to 46.36163, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 46.36163 to 31.54496, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 31.54496\n",
      "\n",
      "Epoch 00021: val_loss improved from 31.54496 to 30.05420, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 30.05420 to 24.29957, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24.29957\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24.29957\n",
      "\n",
      "Epoch 00025: val_loss improved from 24.29957 to 23.83727, saving model to ./ckpt_opt/1.2 100.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.83727\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.83727\n",
      "1.2 100 FD004\n",
      "Score:  2742.0339966796287\n",
      "RMSE:  21.28719950658494\n",
      "MSE:  453.1448628331501\n",
      "Penalized RMSE:  27.976675967229163\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.67810, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 109.67810\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 109.67810\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 109.67810\n",
      "\n",
      "Epoch 00005: val_loss improved from 109.67810 to 104.65600, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 104.65600 to 92.68548, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 92.68548 to 92.22121, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 92.22121 to 74.65324, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 74.65324 to 73.94188, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 73.94188\n",
      "\n",
      "Epoch 00011: val_loss improved from 73.94188 to 72.00929, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 72.00929\n",
      "\n",
      "Epoch 00013: val_loss improved from 72.00929 to 65.60160, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 65.60160 to 49.21247, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 49.21247\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 49.21247\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.21247 to 43.43843, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 43.43843 to 33.03062, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 33.03062\n",
      "\n",
      "Epoch 00020: val_loss improved from 33.03062 to 28.20085, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 28.20085 to 26.68126, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 26.68126\n",
      "\n",
      "Epoch 00023: val_loss improved from 26.68126 to 26.55870, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.55870\n",
      "\n",
      "Epoch 00032: val_loss improved from 26.55870 to 24.98993, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.98993\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.98993\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.98993\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.98993\n",
      "\n",
      "Epoch 00037: val_loss improved from 24.98993 to 24.92874, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.92874\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.92874\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.92874\n",
      "\n",
      "Epoch 00041: val_loss improved from 24.92874 to 23.87154, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.87154\n",
      "\n",
      "Epoch 00051: val_loss improved from 23.87154 to 23.30591, saving model to ./ckpt_opt/1.2 110.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 23.30591\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 23.30591\n",
      "1.2 110 FD004\n",
      "Score:  2143.0098651736585\n",
      "RMSE:  19.712229931762245\n",
      "MSE:  388.57200888266334\n",
      "Penalized RMSE:  26.26148243772092\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.54570, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 109.54570\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 109.54570\n",
      "\n",
      "Epoch 00004: val_loss improved from 109.54570 to 97.15968, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 97.15968 to 90.06240, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 90.06240 to 90.02229, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 90.02229\n",
      "\n",
      "Epoch 00008: val_loss improved from 90.02229 to 83.70466, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 83.70466\n",
      "\n",
      "Epoch 00010: val_loss improved from 83.70466 to 77.19334, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 77.19334 to 73.62990, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 73.62990\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 73.62990\n",
      "\n",
      "Epoch 00014: val_loss improved from 73.62990 to 71.39586, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 71.39586 to 67.38569, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 67.38569 to 58.94985, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 58.94985 to 45.06969, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 45.06969\n",
      "\n",
      "Epoch 00019: val_loss improved from 45.06969 to 38.55660, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.55660 to 36.20646, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 36.20646 to 34.11906, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 34.11906 to 27.33868, saving model to ./ckpt_opt/1.2 120.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 27.33868\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 27.33868\n",
      "1.2 120 FD004\n",
      "Score:  7826.387000954877\n",
      "RMSE:  24.475000070461057\n",
      "MSE:  599.0256284490688\n",
      "Penalized RMSE:  31.36209515981856\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.76356, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 107.76356 to 101.97239, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 101.97239 to 95.66495, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 95.66495 to 92.52157, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 92.52157 to 90.88694, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 90.88694 to 87.89061, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 87.89061 to 79.17900, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 79.17900 to 69.98640, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 69.98640\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 69.98640\n",
      "\n",
      "Epoch 00011: val_loss improved from 69.98640 to 69.80882, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 69.80882 to 67.38642, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 67.38642\n",
      "\n",
      "Epoch 00014: val_loss improved from 67.38642 to 66.10378, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 66.10378 to 64.03015, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 64.03015 to 56.31059, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 56.31059 to 54.76399, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 54.76399 to 49.28523, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 49.28523 to 38.18409, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.18409 to 35.43692, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.43692 to 30.09963, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 30.09963 to 28.06323, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 28.06323\n",
      "\n",
      "Epoch 00024: val_loss improved from 28.06323 to 25.57722, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.57722\n",
      "\n",
      "Epoch 00032: val_loss improved from 25.57722 to 25.57219, saving model to ./ckpt_opt/1.2 130.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.57219\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.57219\n",
      "1.2 130 FD004\n",
      "Score:  4340.883703278659\n",
      "RMSE:  23.465918343210213\n",
      "MSE:  550.6493236902095\n",
      "Penalized RMSE:  30.221069693439116\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.44992, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 107.44992 to 102.21665, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 102.21665 to 94.94975, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 94.94975 to 91.68441, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.68441 to 86.83619, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 86.83619\n",
      "\n",
      "Epoch 00007: val_loss improved from 86.83619 to 81.19445, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 81.19445 to 73.65515, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 73.65515\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 73.65515\n",
      "\n",
      "Epoch 00011: val_loss improved from 73.65515 to 67.24969, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 67.24969\n",
      "\n",
      "Epoch 00014: val_loss improved from 65.46535 to 57.42084, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 57.42084\n",
      "\n",
      "Epoch 00016: val_loss improved from 57.42084 to 49.87193, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.87193 to 39.76546, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 39.76546\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 39.76546\n",
      "\n",
      "Epoch 00020: val_loss improved from 39.76546 to 30.45177, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 30.45177 to 28.93251, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.93251 to 28.71089, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 28.71089\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 28.71089\n",
      "\n",
      "Epoch 00025: val_loss improved from 28.71089 to 27.45301, saving model to ./ckpt_opt/1.3 70.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 27.45301\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 27.45301\n",
      "1.3 70 FD004\n",
      "Score:  3625.048667170785\n",
      "RMSE:  22.799620548272735\n",
      "MSE:  519.8226971452203\n",
      "Penalized RMSE:  30.251532693272033\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 114.54273, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 114.54273\n",
      "\n",
      "Epoch 00003: val_loss improved from 114.54273 to 101.64862, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 101.64862\n",
      "\n",
      "Epoch 00005: val_loss improved from 101.64862 to 89.96681, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.96681 to 84.07395, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 84.07395 to 76.87658, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.87658 to 74.57657, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 74.57657 to 73.99589, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 73.99589\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 73.99589\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 73.99589\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.99589 to 60.57432, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 60.57432\n",
      "\n",
      "Epoch 00015: val_loss improved from 60.57432 to 60.13471, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 60.13471 to 40.21636, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 40.21636 to 36.14169, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 36.14169 to 34.73622, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 34.73622\n",
      "\n",
      "Epoch 00020: val_loss improved from 34.73622 to 30.11287, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 30.11287 to 28.38640, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.38640 to 27.07089, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 27.07089\n",
      "\n",
      "Epoch 00036: val_loss improved from 27.07089 to 26.78780, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.78780\n",
      "\n",
      "Epoch 00038: val_loss improved from 26.78780 to 26.62592, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 26.62592\n",
      "\n",
      "Epoch 00058: val_loss improved from 26.62592 to 26.55592, saving model to ./ckpt_opt/1.3 80.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 26.55592\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 26.55592\n",
      "1.3 80 FD004\n",
      "Score:  2428.6023103893485\n",
      "RMSE:  21.842352851974344\n",
      "MSE:  477.0883781101517\n",
      "Penalized RMSE:  27.952364860588165\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 106.21204, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 106.21204 to 102.15360, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 102.15360 to 98.70893, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 98.70893\n",
      "\n",
      "Epoch 00005: val_loss improved from 98.70893 to 93.09539, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 93.09539 to 92.52179, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 92.52179\n",
      "\n",
      "Epoch 00008: val_loss improved from 92.52179 to 78.41026, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 78.41026 to 71.72259, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 71.72259\n",
      "\n",
      "Epoch 00011: val_loss improved from 71.72259 to 65.72292, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 65.72292 to 64.06915, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 64.06915\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 64.06915\n",
      "\n",
      "Epoch 00015: val_loss improved from 64.06915 to 57.02667, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 57.02667\n",
      "\n",
      "Epoch 00017: val_loss improved from 57.02667 to 42.29041, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 42.29041 to 42.28999, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 42.28999\n",
      "\n",
      "Epoch 00020: val_loss improved from 42.28999 to 38.90131, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.90131 to 36.14104, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 36.14104\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 36.14104\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 36.14104\n",
      "\n",
      "Epoch 00025: val_loss improved from 36.14104 to 32.20107, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 32.20107 to 29.15904, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 29.15904 to 26.51797, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.51797\n",
      "\n",
      "Epoch 00040: val_loss improved from 26.51797 to 26.51029, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.51029\n",
      "\n",
      "Epoch 00042: val_loss improved from 26.51029 to 26.19048, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.19048\n",
      "\n",
      "Epoch 00044: val_loss improved from 26.19048 to 26.02783, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.02783\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.02783\n",
      "\n",
      "Epoch 00047: val_loss improved from 26.02783 to 25.83972, saving model to ./ckpt_opt/1.3 90.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 25.83972\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 25.83972\n",
      "1.3 90 FD004\n",
      "Score:  3091.915055210467\n",
      "RMSE:  22.67326355487375\n",
      "MSE:  514.0768802287662\n",
      "Penalized RMSE:  27.53091773350896\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.01694, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 111.01694\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 111.01694\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 111.01694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 111.01694\n",
      "\n",
      "Epoch 00006: val_loss improved from 111.01694 to 108.12231, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 108.12231 to 104.88530, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 104.88530 to 94.29977, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 94.29977 to 91.72355, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 91.72355 to 83.01727, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 83.01727\n",
      "\n",
      "Epoch 00012: val_loss improved from 83.01727 to 67.03306, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 67.03306 to 63.49946, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 63.49946 to 55.39251, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 55.39251 to 53.68732, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 53.68732 to 50.33827, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 50.33827 to 43.43428, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 43.43428 to 39.40376, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 39.40376 to 31.72169, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 31.72169 to 30.35241, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 30.35241 to 27.47201, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 27.47201\n",
      "\n",
      "Epoch 00023: val_loss improved from 27.47201 to 24.77325, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24.77325\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.77325\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.77325\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.77325\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.77325\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.77325\n",
      "\n",
      "Epoch 00030: val_loss improved from 24.77325 to 24.72636, saving model to ./ckpt_opt/1.3 100.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.72636\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.72636\n",
      "1.3 100 FD004\n",
      "Score:  2772.9664560926394\n",
      "RMSE:  22.018274743769766\n",
      "MSE:  484.80442269212955\n",
      "Penalized RMSE:  28.462495498017883\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.73341, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.73341 to 106.17614, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 106.17614 to 96.68246, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.68246 to 91.79192, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.79192 to 86.69714, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.69714 to 81.70026, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 81.70026 to 77.18067, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 77.18067 to 72.69465, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 72.69465 to 68.94702, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 68.94702\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 68.94702\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 68.94702\n",
      "\n",
      "Epoch 00013: val_loss improved from 68.94702 to 62.39112, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 62.39112 to 59.27456, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 59.27456\n",
      "\n",
      "Epoch 00016: val_loss improved from 59.27456 to 49.29557, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.29557 to 46.87323, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 46.87323\n",
      "\n",
      "Epoch 00019: val_loss improved from 46.87323 to 38.97407, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.97407 to 32.24573, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.24573 to 29.71071, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 29.71071 to 28.53443, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 28.53443 to 26.48623, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.48623\n",
      "\n",
      "Epoch 00031: val_loss improved from 26.48623 to 26.45053, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.45053\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.45053\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 26.45053\n",
      "\n",
      "Epoch 00035: val_loss improved from 26.45053 to 25.65642, saving model to ./ckpt_opt/1.3 110.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.65642\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.65642\n",
      "1.3 110 FD004\n",
      "Score:  3140.736341568969\n",
      "RMSE:  21.73664617981968\n",
      "MSE:  472.4817871466695\n",
      "Penalized RMSE:  28.638262751889624\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.18026, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.18026 to 95.86000, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 95.86000 to 93.41777, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.41777 to 88.92028, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 88.92028 to 84.27362, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 84.27362 to 79.82529, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 79.82529 to 77.25793, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 77.25793 to 72.98573, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 72.98573\n",
      "\n",
      "Epoch 00010: val_loss improved from 72.98573 to 69.03520, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 69.03520 to 65.67637, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 65.67637 to 55.91608, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 55.91608 to 55.01208, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 55.01208 to 50.10350, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 50.10350 to 39.33853, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 39.33853 to 34.67181, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 34.67181 to 34.58872, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 34.58872 to 32.53920, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 32.53920\n",
      "\n",
      "Epoch 00020: val_loss improved from 32.53920 to 29.95386, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 29.95386 to 28.25454, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 28.25454\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 28.25454\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 28.25454\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 28.25454\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 28.25454\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 28.25454\n",
      "\n",
      "Epoch 00028: val_loss improved from 28.25454 to 27.89782, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.89782\n",
      "\n",
      "Epoch 00030: val_loss improved from 27.89782 to 27.33980, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 27.33980\n",
      "\n",
      "Epoch 00046: val_loss improved from 27.33980 to 27.18071, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 27.18071\n",
      "\n",
      "Epoch 00048: val_loss improved from 27.18071 to 27.08157, saving model to ./ckpt_opt/1.3 120.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 27.08157\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 27.08157\n",
      "1.3 120 FD004\n",
      "Score:  2730.0722530478492\n",
      "RMSE:  21.97571081925661\n",
      "MSE:  482.931866011592\n",
      "Penalized RMSE:  26.70019463977454\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 106.54080, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 106.54080 to 101.63371, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 101.63371 to 93.27546, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.27546 to 90.28830, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 90.28830 to 89.13495, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 89.13495 to 83.61805, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 83.61805 to 76.06064, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.06064 to 70.09584, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 70.09584 to 66.55438, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 66.55438 to 60.43053, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 60.43053\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 60.43053\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 60.43053\n",
      "\n",
      "Epoch 00014: val_loss improved from 60.43053 to 53.56862, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 53.56862 to 53.10984, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 53.10984 to 45.71097, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 45.71097\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.71097 to 42.39195, saving model to ./ckpt_opt/1.3 130.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 26.35541\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 26.35541\n",
      "1.3 130 FD004\n",
      "Score:  4597.86582428704\n",
      "RMSE:  23.276005303347215\n",
      "MSE:  541.7724228814477\n",
      "Penalized RMSE:  31.312865163480865\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.84884, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.84884 to 111.65370, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 111.65370 to 100.18556, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 100.18556 to 94.27890, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 94.27890 to 88.52790, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 88.52790 to 81.77411, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 81.77411 to 76.21676, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 76.21676 to 70.64185, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 70.64185\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 70.64185\n",
      "\n",
      "Epoch 00011: val_loss improved from 70.64185 to 69.21316, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 69.21316\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 69.21316\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 69.21316\n",
      "\n",
      "Epoch 00015: val_loss improved from 69.21316 to 61.51780, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 61.51780\n",
      "\n",
      "Epoch 00017: val_loss improved from 61.51780 to 52.91536, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 52.91536 to 47.71816, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.71816 to 38.73790, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 38.73790\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.73790 to 27.37405, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 27.37405\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 27.37405\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 27.37405\n",
      "\n",
      "Epoch 00025: val_loss improved from 27.37405 to 26.48171, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.48171\n",
      "\n",
      "Epoch 00027: val_loss improved from 26.48171 to 26.05533, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 26.05533\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.05533\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 26.05533\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 26.05533\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.05533\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.05533\n",
      "\n",
      "Epoch 00034: val_loss improved from 26.05533 to 25.64882, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.64882\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.64882\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.64882\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.64882\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 25.64882\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 25.64882\n",
      "\n",
      "Epoch 00041: val_loss improved from 25.64882 to 25.24732, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 25.24732\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 25.24732\n",
      "\n",
      "Epoch 00044: val_loss improved from 25.24732 to 24.97915, saving model to ./ckpt_opt/1.4 70.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 24.97915\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 24.97915\n",
      "1.4 70 FD004\n",
      "Score:  2521.1579353820075\n",
      "RMSE:  21.56741057486309\n",
      "MSE:  465.1531989047162\n",
      "Penalized RMSE:  27.214314216080055\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 121.73431, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 121.73431 to 120.35576, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 120.35576\n",
      "\n",
      "Epoch 00004: val_loss improved from 120.35576 to 114.06676, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 114.06676 to 97.17894, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 97.17894\n",
      "\n",
      "Epoch 00007: val_loss improved from 97.17894 to 83.54478, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 83.54478\n",
      "\n",
      "Epoch 00009: val_loss improved from 83.54478 to 80.31026, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 80.31026\n",
      "\n",
      "Epoch 00011: val_loss improved from 80.31026 to 65.68304, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 65.68304\n",
      "\n",
      "Epoch 00013: val_loss improved from 65.68304 to 57.79179, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 57.79179\n",
      "\n",
      "Epoch 00015: val_loss improved from 57.79179 to 52.11041, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 52.11041 to 47.99247, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 47.99247 to 35.23926, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 35.23926 to 35.23370, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 35.23370 to 33.02982, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 33.02982\n",
      "\n",
      "Epoch 00021: val_loss improved from 33.02982 to 30.13793, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 30.13793\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.13793 to 25.25141, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 25.25141\n",
      "\n",
      "Epoch 00039: val_loss improved from 25.25141 to 24.63170, saving model to ./ckpt_opt/1.4 80.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 24.63170\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 24.63170\n",
      "1.4 80 FD004\n",
      "Score:  2314.745749489627\n",
      "RMSE:  20.840676819119007\n",
      "MSE:  434.33381027896434\n",
      "Penalized RMSE:  27.345692375805303\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 111.97797, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 111.97797 to 98.54719, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 98.54719 to 94.65605, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 94.65605 to 91.89376, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.89376 to 91.34923, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 91.34923\n",
      "\n",
      "Epoch 00007: val_loss improved from 91.34923 to 84.94715, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 84.94715 to 69.86541, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 69.86541 to 69.02600, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 69.02600\n",
      "\n",
      "Epoch 00011: val_loss improved from 69.02600 to 60.91647, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 60.91647\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 60.91647\n",
      "\n",
      "Epoch 00014: val_loss improved from 60.91647 to 55.12253, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 55.12253\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 55.12253\n",
      "\n",
      "Epoch 00017: val_loss improved from 55.12253 to 45.70447, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.70447 to 40.52625, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 40.52625 to 38.27093, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 38.27093 to 33.20390, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 33.20390 to 30.14667, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 30.14667\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 30.14667\n",
      "\n",
      "Epoch 00024: val_loss improved from 30.14667 to 27.58035, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 27.58035\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 27.58035\n",
      "\n",
      "Epoch 00027: val_loss improved from 27.58035 to 27.02001, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 27.02001\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.02001\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 27.02001\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.02001\n",
      "\n",
      "Epoch 00032: val_loss improved from 27.02001 to 26.99981, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.99981\n",
      "\n",
      "Epoch 00034: val_loss improved from 26.99981 to 26.65562, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 26.65562\n",
      "\n",
      "Epoch 00042: val_loss improved from 26.65562 to 26.34932, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 26.34932\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.34932\n",
      "\n",
      "Epoch 00045: val_loss improved from 26.34932 to 25.44978, saving model to ./ckpt_opt/1.4 90.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 25.44978\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 25.44978\n",
      "1.4 90 FD004\n",
      "Score:  2379.785674590681\n",
      "RMSE:  21.668930176764928\n",
      "MSE:  469.54253500551374\n",
      "Penalized RMSE:  29.142689950710412\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 108.09963, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 108.09963 to 103.49338, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.49338 to 100.61974, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 100.61974 to 95.06283, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 95.06283\n",
      "\n",
      "Epoch 00006: val_loss improved from 95.06283 to 94.79376, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 94.79376 to 88.05169, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 88.05169 to 81.76187, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 81.76187 to 73.59741, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 73.59741\n",
      "\n",
      "Epoch 00011: val_loss improved from 73.59741 to 73.51660, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 73.51660\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.51660 to 71.37067, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 71.37067 to 62.30169, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 62.30169\n",
      "\n",
      "Epoch 00016: val_loss improved from 62.30169 to 58.24881, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 58.24881 to 51.05894, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 51.05894\n",
      "\n",
      "Epoch 00019: val_loss improved from 51.05894 to 41.49476, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 41.49476 to 35.08787, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.08787 to 30.57356, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 30.57356 to 30.10794, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.10794 to 25.84368, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 25.84368 to 24.06395, saving model to ./ckpt_opt/1.4 100.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.06395\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.06395\n",
      "1.4 100 FD004\n",
      "Score:  3404.2304816235655\n",
      "RMSE:  21.880348095379773\n",
      "MSE:  478.74963277498927\n",
      "Penalized RMSE:  28.94752501898274\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 112.74255, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 112.74255 to 107.60751, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 107.60751 to 96.49358, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 96.49358\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 96.49358\n",
      "\n",
      "Epoch 00006: val_loss improved from 96.49358 to 89.17680, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 89.17680\n",
      "\n",
      "Epoch 00008: val_loss improved from 89.17680 to 75.17669, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 75.17669\n",
      "\n",
      "Epoch 00010: val_loss improved from 75.17669 to 67.98096, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 67.98096\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 67.98096\n",
      "\n",
      "Epoch 00013: val_loss improved from 67.98096 to 62.17664, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 62.17664\n",
      "\n",
      "Epoch 00015: val_loss improved from 62.17664 to 59.27008, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 59.27008 to 48.69785, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 48.69785 to 44.80955, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 44.80955\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.80955 to 35.61976, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 35.61976 to 35.38770, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 35.38770 to 30.24196, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 30.24196\n",
      "\n",
      "Epoch 00023: val_loss improved from 30.24196 to 30.18748, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 30.18748\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 30.18748\n",
      "\n",
      "Epoch 00026: val_loss improved from 30.18748 to 28.67931, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 28.67931\n",
      "\n",
      "Epoch 00035: val_loss improved from 28.67931 to 27.97952, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 27.97952\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 27.97952\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 27.97952\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 27.97952\n",
      "\n",
      "Epoch 00040: val_loss improved from 27.97952 to 27.18016, saving model to ./ckpt_opt/1.4 110.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 27.18016\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 27.18016\n",
      "1.4 110 FD004\n",
      "Score:  5019.121918570771\n",
      "RMSE:  24.196677650590193\n",
      "MSE:  585.4792093265709\n",
      "Penalized RMSE:  33.33426834792392\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.48695, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.48695 to 108.89174, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 108.89174 to 96.68518, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 96.68518 to 91.51953, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 91.51953 to 86.43656, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 86.43656 to 83.20894, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 83.20894\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 83.20894\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 83.20894\n",
      "\n",
      "Epoch 00010: val_loss improved from 83.20894 to 78.44022, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 78.44022 to 73.20272, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 73.20272\n",
      "\n",
      "Epoch 00013: val_loss improved from 73.20272 to 71.02514, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 71.02514 to 56.46383, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 56.46383 to 51.69851, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 51.69851 to 51.58207, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 51.58207 to 38.55662, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 38.55662\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 38.55662\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 38.55662\n",
      "\n",
      "Epoch 00021: val_loss improved from 38.55662 to 32.05645, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 32.05645\n",
      "\n",
      "Epoch 00023: val_loss improved from 32.05645 to 28.40353, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 28.40353\n",
      "\n",
      "Epoch 00025: val_loss improved from 28.40353 to 24.03559, saving model to ./ckpt_opt/1.4 120.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 24.03559\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 24.03559\n",
      "1.4 120 FD004\n",
      "Score:  2825.639417883176\n",
      "RMSE:  21.437153241062337\n",
      "MSE:  459.55153908078944\n",
      "Penalized RMSE:  29.278018225073907\n",
      "---------------\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 109.30606, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 109.30606 to 109.17899, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 109.17899 to 105.91992, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 105.91992 to 103.72894, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 103.72894 to 103.56422, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 103.56422 to 85.94646, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 85.94646 to 83.60291, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 83.60291 to 81.83964, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 81.83964 to 79.75138, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 79.75138\n",
      "\n",
      "Epoch 00011: val_loss improved from 79.75138 to 62.77162, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 62.77162\n",
      "\n",
      "Epoch 00013: val_loss improved from 62.77162 to 60.87413, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 60.87413\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 60.87413\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 60.87413\n",
      "\n",
      "Epoch 00017: val_loss improved from 60.87413 to 52.67071, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 52.67071 to 44.02684, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 44.02684 to 41.17573, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 41.17573 to 34.19194, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 34.19194 to 29.40094, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 29.40094\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 29.40094\n",
      "\n",
      "Epoch 00024: val_loss improved from 29.40094 to 28.92750, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 28.92750\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 28.92750\n",
      "\n",
      "Epoch 00027: val_loss improved from 28.92750 to 28.88617, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 28.88617 to 27.32759, saving model to ./ckpt_opt/1.4 130.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 27.32759\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 27.32759\n",
      "1.4 130 FD004\n",
      "Score:  3627.996049985224\n",
      "RMSE:  21.768447121276548\n",
      "MSE:  473.8652900718132\n",
      "Penalized RMSE:  29.93998098559429\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "file_list = ['FD004']\n",
    "seq_set = [70, 80, 90, 100, 110, 120, 130]\n",
    "lambda_set = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4]\n",
    "result = []\n",
    "for lambda_ in lambda_set:\n",
    "    def score(y_true, y_pred):\n",
    "            diff = y_pred - y_true\n",
    "            positive = K.maximum(diff, 0)\n",
    "            negative = K.minimum(diff, 0)\n",
    "            return K.sqrt(K.mean(K.square(positive) * (1 + lambda_) + K.square(negative)))\n",
    "\n",
    "    def compute_penalized_rmse(y_true, y_pred):\n",
    "        diff = y_pred - y_true\n",
    "        positive = np.maximum(diff, 0)\n",
    "        negative = np.minimum(diff, 0)\n",
    "        return np.sqrt(np.mean(np.square(positive) * (1 + lambda_) + np.square(negative)))\n",
    "\n",
    "    for sequence_length in seq_set:\n",
    "        for index, file in enumerate(file_list):\n",
    "            sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "            upper = 160\n",
    "\n",
    "            train_df, test_df, truth_df = read_data(file)\n",
    "            train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "            seq_array, label_array = split(train_df, sequence_length, sequence_cols, upper)\n",
    "\n",
    "            model = build_model(seq_array.shape[2], sequence_length)\n",
    "            history = model.fit(\n",
    "                x=seq_array, \n",
    "                y={'output': label_array, 'reconstruction': seq_array},\n",
    "                epochs=200, \n",
    "                batch_size=128 * 8,\n",
    "                verbose=0, \n",
    "                validation_split=0.2,\n",
    "                callbacks = [\n",
    "                    keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min'),\n",
    "                    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-8),\n",
    "                    keras.callbacks.ModelCheckpoint(filepath='./ckpt_opt/' + str(lambda_) + ' ' + str(sequence_length) + '.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "                ])\n",
    "        \n",
    "            print(lambda_, sequence_length, file)\n",
    "            a, b, c = evaluate(test_df, model, upper=upper, plot_=False)\n",
    "            \n",
    "            result.append('{},{},{},{},{},{},{} \\n'.format(\n",
    "                str(lambda_),\n",
    "                str(sequence_length),\n",
    "                str(file),\n",
    "                str(a),\n",
    "                str(b),\n",
    "                str(c),\n",
    "                str(np.sqrt(a) + b)\n",
    "            ))\n",
    "        print('---------------')\n",
    "\n",
    "np.save('./ckpt_opt/effect_on_all', np.array(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,70,FD004,4893.575301817399,24.220345346451616,24.220345346451616,94.17443959273143 \\n',\n",
       " '0,80,FD004,9563.09340836566,26.601231566172746,26.601231566172746,124.39230175341254 \\n',\n",
       " '0,90,FD004,5099.826330692138,25.586444883044635,25.586444883044635,96.99951322975649 \\n',\n",
       " '0,100,FD004,3638.3206083619048,22.57275625885059,22.57275625885059,82.89124934647592 \\n',\n",
       " '0,110,FD004,3815.9335531830525,22.45795794981618,22.45795794981618,84.23120096961442 \\n',\n",
       " '0,120,FD004,4747.827078910243,24.71377367727564,24.71377367727564,93.6182516080331 \\n',\n",
       " '0,130,FD004,8321.657999621886,26.005529493276228,26.005529493276228,117.22865156011784 \\n',\n",
       " '0.1,70,FD004,8022.641847200835,25.626640145733322,26.456053019895506,115.19584158909127 \\n',\n",
       " '0.1,80,FD004,7406.941167296629,25.342659288906514,26.397364897707938,111.40624721839403 \\n',\n",
       " '0.1,90,FD004,3525.10505311057,22.94364180766466,23.811462531998036,82.31623694305158 \\n',\n",
       " '0.1,100,FD004,3127.241744644385,22.06898927841287,22.872212304798147,77.99073589495183 \\n',\n",
       " '0.1,110,FD004,4931.713500919781,23.792921835108555,24.787574386634283,94.01908149744365 \\n',\n",
       " '0.1,120,FD004,5279.564451928365,25.309334330816927,26.36473013664614,97.96994575081783 \\n',\n",
       " '0.1,130,FD004,6101.094183202994,25.57087172116414,26.620584318934327,103.68037295630835 \\n',\n",
       " '0.2,70,FD004,5119.815912065412,24.017060167851824,25.692127412801586,95.5699490827336 \\n',\n",
       " '0.2,80,FD004,3217.8383720461497,22.887591185161213,24.41350880807115,79.61358498030023 \\n',\n",
       " '0.2,90,FD004,2857.1603998116148,21.757249094305166,23.290027592031347,75.20966157317851 \\n',\n",
       " '0.2,100,FD004,7136.339635866233,24.719996620980886,26.27146031723295,109.1968552028803 \\n',\n",
       " '0.2,110,FD004,4134.092427965053,22.009762636366002,23.408666963970393,86.3066710221123 \\n',\n",
       " '0.2,120,FD004,3510.991231528282,22.42717515878641,24.028263329178614,81.68079303591085 \\n',\n",
       " '0.2,130,FD004,5950.997866243049,24.351879995844747,26.000361166421712,101.4945910335251 \\n',\n",
       " '0.3,70,FD004,10258.187055567296,26.478628626943138,28.556600169706137,127.76133719838339 \\n',\n",
       " '0.3,80,FD004,3186.4366049527807,22.057558254233076,24.39130415279883,78.50608883699299 \\n',\n",
       " '0.3,90,FD004,4250.104312084414,23.60483042716591,26.03763388821508,88.79765451145002 \\n',\n",
       " '0.3,100,FD004,2591.4236090906566,21.647743717783253,23.604000048839765,72.55377095514147 \\n',\n",
       " '0.3,110,FD004,5643.76036592402,23.67128376157739,26.20881308171959,98.79624875939965 \\n',\n",
       " '0.3,120,FD004,4100.298320372536,23.507458886879196,25.376155884276127,87.54103070957652 \\n',\n",
       " '0.3,130,FD004,5923.940327479948,24.077440036766212,26.444679385371376,101.04457800798059 \\n',\n",
       " '0.4,70,FD004,3559.6481972546408,21.726601088014284,24.416308142692294,81.38938846217776 \\n',\n",
       " '0.4,80,FD004,2339.627111910886,20.57187238574384,22.72573723435312,68.9415644657115 \\n',\n",
       " '0.4,90,FD004,2848.6237945872817,22.87034915948898,25.601694792931823,76.24284951863726 \\n',\n",
       " '0.4,100,FD004,6212.826827769533,25.306457659568494,28.707573979054303,104.12794475394863 \\n',\n",
       " '0.4,110,FD004,4400.702325406384,22.660274493010387,25.775886701395613,88.99806406582013 \\n',\n",
       " '0.4,120,FD004,61702.435010485264,36.29381929642309,40.41373740156005,284.6935677070953 \\n',\n",
       " '0.4,130,FD004,6928.8290520595065,24.07370073095018,27.29202612382833,107.31328898113019 \\n',\n",
       " '0.5,70,FD004,3187.51455888659,23.24896927333665,26.27656569985296,79.70704716203731 \\n',\n",
       " '0.5,80,FD004,3519.849821595079,22.30978234225866,25.66605392336557,81.6381045972751 \\n',\n",
       " '0.5,90,FD004,2281.9568994709834,21.040202985902926,24.263130561489202,68.81003551094935 \\n',\n",
       " '0.5,100,FD004,4183.960303802108,22.866125198240933,26.61683638162501,87.54966486485247 \\n',\n",
       " '0.5,110,FD004,3194.30881976004,21.19039172133856,24.59820003883878,77.70860842852103 \\n',\n",
       " '0.5,120,FD004,2749.9519780603696,21.581733378228957,24.930969504243766,74.02171791350861 \\n',\n",
       " '0.5,130,FD004,4525.6270072055395,23.93322375340353,27.314691244751064,91.20600433519263 \\n',\n",
       " '0.6,70,FD004,4038.0074023006946,23.335462149973274,27.3800197200981,86.88077985553436 \\n',\n",
       " '0.6,80,FD004,3500.315509574628,22.162491667131917,26.42873122367775,81.32595598063057 \\n',\n",
       " '0.6,90,FD004,2085.8728585206854,21.139999583628228,24.586410396692756,66.81135667448645 \\n',\n",
       " '0.6,100,FD004,2984.908072247332,21.199364316297896,24.24543902982106,75.83367654082701 \\n',\n",
       " '0.6,110,FD004,11205.751491366282,29.53777728435464,33.15447765467337,135.39499948183288 \\n',\n",
       " '0.6,120,FD004,5631.391932816781,24.863136904095235,29.783341202148094,99.90573769069366 \\n',\n",
       " '0.6,130,FD004,3166.030812222558,22.306211658910843,26.30853763199902,78.5737050473901 \\n',\n",
       " '0.7,70,FD004,2974.0201466787075,22.250508139324346,25.867573825207153,76.78508567210356 \\n',\n",
       " '0.7,80,FD004,2381.949413367797,21.005456826698495,24.819934226345286,69.81067594686839 \\n',\n",
       " '0.7,90,FD004,2620.670556329551,22.192517523287222,26.888072236965805,73.38500287347853 \\n',\n",
       " '0.7,100,FD004,3045.1416406463013,21.98277154800836,26.18710496648829,77.16557358493198 \\n',\n",
       " '0.7,110,FD004,6904.071248143001,22.688761199947287,28.181243438066176,105.7795022493747 \\n',\n",
       " '0.7,120,FD004,2999.559018614382,21.512227017676317,24.37072541170815,76.28045702937544 \\n',\n",
       " '0.7,130,FD004,4208.645161124887,22.345166109376606,27.208802975687068,87.21923767209638 \\n',\n",
       " '0.8,70,FD004,4616.8051061591195,24.52560352293316,29.07508139979696,92.47267929635493 \\n',\n",
       " '0.8,80,FD004,2985.1379606494365,22.35833829749845,27.791071226954006,76.99475436452683 \\n',\n",
       " '0.8,90,FD004,1881.0105769652523,20.413177858218052,23.96786475551297,63.78379667978034 \\n',\n",
       " '0.8,100,FD004,3519.9848511646614,22.75652908541912,28.443048071310965,82.08598931526329 \\n',\n",
       " '0.8,110,FD004,4526.887519319692,23.76754009003206,28.96776668568673,91.04968868324876 \\n',\n",
       " '0.8,120,FD004,6755.911863755606,26.11495083995355,33.31383567031271,108.30930504847329 \\n',\n",
       " '0.8,130,FD004,2613.667771942093,20.1843799809767,23.96345288246048,71.30842297389827 \\n',\n",
       " '0.9,70,FD004,3346.0872580085597,23.223819383383788,26.52041648228621,81.06919307919942 \\n',\n",
       " '0.9,80,FD004,2087.8652966436466,20.48961066390199,25.578975787716637,66.18277532387762 \\n',\n",
       " '0.9,90,FD004,2284.8865870396553,21.017676336513507,25.972066746056555,68.81816364615979 \\n',\n",
       " '0.9,100,FD004,4578.3719894217,23.143230505039956,29.632406079780353,90.80689866914187 \\n',\n",
       " '0.9,110,FD004,2511.7847900455185,19.761590823417727,22.790968395046985,69.8793001689726 \\n',\n",
       " '0.9,120,FD004,2428.39044818239,21.233545234000363,26.775816915819753,70.51224700446495 \\n',\n",
       " '0.9,130,FD004,9101.62485947901,25.41177191695862,30.964486167245195,120.81420825628388 \\n',\n",
       " '1.0,70,FD004,3954.5161446866723,23.027501808042924,28.504013514421693,85.91244551229276 \\n',\n",
       " '1.0,80,FD004,2764.552183874106,21.508102647953734,26.5343315708778,74.08711161421428 \\n',\n",
       " '1.0,90,FD004,3222.591464069632,22.94828261303348,28.720993934173276,79.716156133799 \\n',\n",
       " '1.0,100,FD004,7768.694230810273,25.03798835621382,32.02935106996431,113.1781848094228 \\n',\n",
       " '1.0,110,FD004,10192.968266537486,27.46001902870145,34.614047961936585,128.42025014243 \\n',\n",
       " '1.0,120,FD004,2622.5775440816815,20.760916269851286,26.81486805827847,71.97202389389787 \\n',\n",
       " '1.0,130,FD004,2879.0075316954567,21.257711418780286,26.9503674251866,74.9140953045348 \\n',\n",
       " '1.1,70,FD004,2954.169463234106,22.401388842678127,27.294391918066847,76.75366076606791 \\n',\n",
       " '1.1,80,FD004,2803.757524217909,21.995774804174562,27.911428588346357,74.94629438819668 \\n',\n",
       " '1.1,90,FD004,2278.129691450024,21.205670565465475,26.944973279733453,68.9354274415488 \\n',\n",
       " '1.1,100,FD004,3037.88937230837,21.455551564067033,27.70286060860836,76.57260312080734 \\n',\n",
       " '1.1,110,FD004,2794.7264442326145,21.739475812419485,28.08565344151604,74.60464813061603 \\n',\n",
       " '1.1,120,FD004,3323.4509724767786,22.309030028557135,28.416761410984723,79.9584096650873 \\n',\n",
       " '1.1,130,FD004,3103.076819023246,21.79120410134446,27.17375436967838,77.49647152753357 \\n',\n",
       " '1.2,70,FD004,3433.0230856472876,23.740984180377026,31.393105978512565,82.33298948525875 \\n',\n",
       " '1.2,80,FD004,7145.714736380587,24.98959013412052,31.227040672805792,109.52191966774089 \\n',\n",
       " '1.2,90,FD004,3154.046548818543,21.95444352224673,28.599336906671965,78.115342275904 \\n',\n",
       " '1.2,100,FD004,2742.0339966796287,21.28719950658494,27.976675967229163,73.65163397006052 \\n',\n",
       " '1.2,110,FD004,2143.0098651736585,19.712229931762245,26.26148243772092,66.00488448055279 \\n',\n",
       " '1.2,120,FD004,7826.387000954877,24.475000070461057,31.36209515981856,112.94186958077478 \\n',\n",
       " '1.2,130,FD004,4340.883703278659,23.465918343210213,30.221069693439116,89.3513014207841 \\n',\n",
       " '1.3,70,FD004,3625.048667170785,22.799620548272735,30.251532693272033,83.00799759973282 \\n',\n",
       " '1.3,80,FD004,2428.6023103893485,21.842352851974344,27.952364860588165,71.12320420814866 \\n',\n",
       " '1.3,90,FD004,3091.915055210467,22.67326355487375,27.53091773350896,78.27825483452838 \\n',\n",
       " '1.3,100,FD004,2772.9664560926394,22.018274743769766,28.462495498017883,74.67723842605884 \\n',\n",
       " '1.3,110,FD004,3140.736341568969,21.73664617981968,28.638262751889624,77.77891898855728 \\n',\n",
       " '1.3,120,FD004,2730.0722530478492,21.97571081925661,26.70019463977454,74.22580414977469 \\n',\n",
       " '1.3,130,FD004,4597.86582428704,23.276005303347215,31.312865163480865,91.08356995720453 \\n',\n",
       " '1.4,70,FD004,2521.1579353820075,21.56741057486309,27.214314216080055,71.7785441547974 \\n',\n",
       " '1.4,80,FD004,2314.745749489627,20.840676819119007,27.345692375805303,68.95248149832042 \\n',\n",
       " '1.4,90,FD004,2379.785674590681,21.668930176764928,29.142689950710412,70.45197717662627 \\n',\n",
       " '1.4,100,FD004,3404.2304816235655,21.880348095379773,28.94752501898274,80.22613184728053 \\n',\n",
       " '1.4,110,FD004,5019.121918570771,24.196677650590193,33.33426834792392,95.04243912275429 \\n',\n",
       " '1.4,120,FD004,2825.639417883176,21.437153241062337,29.278018225073907,74.59389710186683 \\n',\n",
       " '1.4,130,FD004,3627.996049985224,21.768447121276548,29.93998098559429,82.00129571724803 \\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 噪音Simga对结果的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2941.83, 21.93\n",
      "0.04, \t 2200.27, 21.36\n",
      "0.09, \t 5165.33, 23.88\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 1844.83, 20.93\n",
      "0.04, \t 2430.52, 21.75\n",
      "0.09, \t 5394.32, 23.82\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2580.55, 22.06\n",
      "0.04, \t 2686.95, 21.45\n",
      "0.09, \t 2493.33, 22.01\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2171.59, 21.61\n",
      "0.04, \t 2451.25, 22.1\n",
      "0.09, \t 4074.32, 23.15\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2243.59, 21.58\n",
      "0.04, \t 2121.03, 21.42\n",
      "0.09, \t 9603.54, 22.79\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 3880.68, 22.05\n",
      "0.04, \t 3157.11, 22.62\n",
      "0.09, \t 4557.47, 24.12\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2755.6, 21.9\n",
      "0.04, \t 2911.19, 22.39\n",
      "0.09, \t 3714.96, 21.68\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2680.35, 22.11\n",
      "0.04, \t 2221.59, 21.14\n",
      "0.09, \t 6289.59, 23.26\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2771.8, 22.39\n",
      "0.04, \t 3000.67, 22.06\n",
      "0.09, \t 4963.93, 23.11\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2482.63, 21.83\n",
      "0.04, \t 2969.08, 22.25\n",
      "0.09, \t 2502.86, 22.35\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2499.47, 22.14\n",
      "0.04, \t 2253.22, 21.82\n",
      "0.09, \t 2789.57, 22.67\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 1942.41, 20.64\n",
      "0.04, \t 2313.64, 21.53\n",
      "0.09, \t 2656.8, 22.67\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2152.67, 21.66\n",
      "0.04, \t 2234.32, 21.48\n",
      "0.09, \t 2993.64, 22.91\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2446.82, 22.39\n",
      "0.04, \t 2613.76, 22.17\n",
      "0.09, \t 4569.51, 22.97\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2298.93, 22.1\n",
      "0.04, \t 2350.24, 21.62\n",
      "0.09, \t 2917.17, 22.31\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2718.59, 21.92\n",
      "0.04, \t 2217.71, 21.34\n",
      "0.09, \t 3734.66, 22.2\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2874.4, 22.04\n",
      "0.04, \t 2991.42, 22.3\n",
      "0.09, \t 4545.73, 23.33\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2431.68, 21.6\n",
      "0.04, \t 2604.69, 22.25\n",
      "0.09, \t 3317.77, 23.65\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 2632.12, 21.91\n",
      "0.04, \t 2342.64, 21.87\n",
      "0.09, \t 2787.18, 22.4\n",
      "===============\n",
      "0.0, \t 2027.12, 20.88\n",
      "0.01, \t 3432.81, 22.49\n",
      "0.04, \t 3792.89, 21.81\n",
      "0.09, \t 3081.84, 22.62\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "filename = 'FD004'\n",
    "lambda_ = 0.8\n",
    "sequence_length = 90\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "model4 = load_model_from_disk_opt(\"0.8 90\")\n",
    "\n",
    "all_rmse_mean = []\n",
    "all_score_mean = []\n",
    "\n",
    "all_rmse_std = []\n",
    "all_score_std = []\n",
    "\n",
    "for i in range(20):\n",
    "    loss_list = []\n",
    "    score_list = []\n",
    "    for sigma in [0.0, 0.01, 0.04, 0.09]:\n",
    "        score_, rmse_, _ =  evaluate(test_df, model4, upper=160, plot_=False, noise=True, sigma=sigma, debug=False)\n",
    "        loss_list.append(rmse_)\n",
    "        score_list.append(score_)\n",
    "\n",
    "        print(\"{}, \\t {}, {}\".format(round(sigma, 2), round(score_, 2), round(rmse_, 2)))\n",
    "        \n",
    "    print('===============')\n",
    "    all_rmse_mean.append(np.mean(loss_list))\n",
    "    all_score_mean.append(np.mean(score_list))\n",
    "    \n",
    "    all_rmse_std.append(np.std(loss_list))\n",
    "    all_score_std.append(np.std(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01, \t 2436.68, 21.74, 71.0\n",
      "0.04, \t 2401.12, 21.47, 70.0\n",
      "0.09, \t 3527.87, 23.16, 83.0\n",
      "0.16, \t 20763.55, 28.63, 173.0\n",
      "===============\n",
      "0.01, \t 2895.93, 21.62, 75.0\n",
      "0.04, \t 1939.27, 21.06, 65.0\n",
      "0.09, \t 4094.18, 23.11, 87.0\n",
      "0.16, \t 17925.94, 26.53, 160.0\n",
      "===============\n",
      "0.01, \t 2373.78, 21.67, 70.0\n",
      "0.04, \t 2407.98, 22.14, 71.0\n",
      "0.09, \t 3332.6, 22.47, 80.0\n",
      "0.16, \t 62751.99, 27.3, 278.0\n",
      "===============\n",
      "0.01, \t 2298.64, 21.04, 69.0\n",
      "0.04, \t 2762.36, 22.17, 75.0\n",
      "0.09, \t 2639.53, 22.2, 74.0\n",
      "0.16, \t 14037.25, 26.75, 145.0\n",
      "===============\n",
      "0.01, \t 2855.07, 21.53, 75.0\n",
      "0.04, \t 2407.57, 21.87, 71.0\n",
      "0.09, \t 5615.9, 23.32, 98.0\n",
      "0.16, \t 6794.52, 25.0, 107.0\n",
      "===============\n",
      "0.01, \t 2690.13, 21.43, 73.0\n",
      "0.04, \t 3272.65, 22.33, 80.0\n",
      "0.09, \t 6062.56, 23.73, 102.0\n",
      "0.16, \t 9719.09, 25.88, 124.0\n",
      "===============\n",
      "0.01, \t 2637.26, 21.77, 73.0\n",
      "0.04, \t 7021.37, 22.95, 107.0\n",
      "0.09, \t 11260.21, 24.82, 131.0\n",
      "0.16, \t 6296.85, 24.7, 104.0\n",
      "===============\n",
      "0.01, \t 2142.43, 21.6, 68.0\n",
      "0.04, \t 3214.22, 22.42, 79.0\n",
      "0.09, \t 3368.39, 23.04, 81.0\n",
      "0.16, \t 11769.25, 25.55, 134.0\n",
      "===============\n",
      "0.01, \t 2351.05, 21.55, 70.0\n",
      "0.04, \t 2218.3, 21.36, 68.0\n",
      "0.09, \t 2569.27, 22.0, 73.0\n",
      "0.16, \t 43969.81, 26.85, 237.0\n",
      "===============\n",
      "0.01, \t 2343.49, 21.6, 70.0\n",
      "0.04, \t 2072.88, 21.29, 67.0\n",
      "0.09, \t 3216.1, 22.69, 79.0\n",
      "0.16, \t 15245.17, 27.84, 151.0\n",
      "===============\n",
      "0.01, \t 2220.09, 21.67, 69.0\n",
      "0.04, \t 2400.4, 21.31, 70.0\n",
      "0.09, \t 3081.47, 22.95, 78.0\n",
      "0.16, \t 9291.02, 26.25, 123.0\n",
      "===============\n",
      "0.01, \t 2334.26, 21.37, 70.0\n",
      "0.04, \t 2751.65, 22.25, 75.0\n",
      "0.09, \t 4875.78, 24.0, 94.0\n",
      "0.16, \t 23191.51, 26.61, 179.0\n",
      "===============\n",
      "0.01, \t 2164.85, 21.64, 68.0\n",
      "0.04, \t 3535.17, 22.12, 82.0\n",
      "0.09, \t 3112.77, 23.07, 79.0\n",
      "0.16, \t 11065.08, 26.2, 131.0\n",
      "===============\n",
      "0.01, \t 2396.84, 21.82, 71.0\n",
      "0.04, \t 2318.0, 21.43, 70.0\n",
      "0.09, \t 3144.18, 23.23, 79.0\n",
      "0.16, \t 4872.9, 23.66, 93.0\n",
      "===============\n",
      "0.01, \t 2875.6, 21.85, 75.0\n",
      "0.04, \t 2646.58, 22.05, 73.0\n",
      "0.09, \t 3183.18, 22.66, 79.0\n",
      "0.16, \t 240238.11, 29.57, 520.0\n",
      "===============\n",
      "0.01, \t 2403.56, 21.75, 71.0\n",
      "0.04, \t 3022.06, 21.78, 77.0\n",
      "0.09, \t 2692.02, 22.15, 74.0\n",
      "0.16, \t 54715.99, 27.32, 261.0\n",
      "===============\n",
      "0.01, \t 3025.56, 22.4, 77.0\n",
      "0.04, \t 2565.7, 21.5, 72.0\n",
      "0.09, \t 2654.98, 22.56, 74.0\n",
      "0.16, \t 13000.37, 27.11, 141.0\n",
      "===============\n",
      "0.01, \t 2707.72, 22.19, 74.0\n",
      "0.04, \t 2254.47, 21.61, 69.0\n",
      "0.09, \t 2511.24, 21.59, 72.0\n",
      "0.16, \t 8770.94, 25.0, 119.0\n",
      "===============\n",
      "0.01, \t 2330.0, 21.5, 70.0\n",
      "0.04, \t 2787.93, 22.16, 75.0\n",
      "0.09, \t 3298.87, 22.18, 80.0\n",
      "0.16, \t 36709.31, 25.84, 217.0\n",
      "===============\n",
      "0.01, \t 2301.89, 21.62, 70.0\n",
      "0.04, \t 2139.83, 21.85, 68.0\n",
      "0.09, \t 3365.98, 22.09, 80.0\n",
      "0.16, \t 19219.8, 26.14, 165.0\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "filename = 'FD004'\n",
    "lambda_ = 0.8\n",
    "sequence_length = 90\n",
    "train_df, test_df, truth_df = read_data(filename)\n",
    "train_df, test_df = data_handle(train_df, test_df, truth_df)\n",
    "sequence_cols = ['s' + str(i) for i in set(range(1, 22))] + ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "model4 = load_model_from_disk_opt(\"0.8 90\")\n",
    "\n",
    "all_rmse_mean = []\n",
    "all_score_mean = []\n",
    "\n",
    "all_rmse_std = []\n",
    "all_score_std = []\n",
    "\n",
    "for i in range(20):\n",
    "    loss_list = []\n",
    "    score_list = []\n",
    "    for sigma in [0.01, 0.04, 0.09, 0.16]:\n",
    "        score_, rmse_, _ =  evaluate(test_df, model4, upper=160, plot_=False, noise=True, sigma=sigma, debug=False)\n",
    "        loss_list.append(rmse_)\n",
    "        score_list.append(score_)\n",
    "\n",
    "        print(\"{}, \\t {}, {}, {}\".format(round(sigma, 2), round(score_, 2), round(rmse_, 2), round(np.sqrt(score_) + rmse_), 2))\n",
    "        \n",
    "    print('===============')\n",
    "    all_rmse_mean.append(np.mean(loss_list))\n",
    "    all_score_mean.append(np.mean(score_list))\n",
    "    \n",
    "    all_rmse_std.append(np.std(loss_list))\n",
    "    all_score_std.append(np.std(score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise Linear Degradation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 612x295.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    }
   ],
   "source": [
    "max_degradation = 130\n",
    "actual_rul = list(range(250, 0, -1))\n",
    "degradation_rul = list(range(250, 0, -1))\n",
    "degradation_rul[:(250 - max_degradation)] = [max_degradation] * (250 - max_degradation)\n",
    "\n",
    "# 画图\n",
    "fig = plt.figure(figsize = (8.5, 4.1))\n",
    "plt.grid(True)\n",
    "plt.plot(list(range(0, 250)), degradation_rul, 'b-')\n",
    "plt.plot(list(range(0, 250)), actual_rul, 'red', linestyle='--')\n",
    "plt.xlabel('Time Cycles', fontsize=16)\n",
    "# plt.xticks(fontsize=12)\n",
    "plt.xticks([])\n",
    "plt.ylabel('RUL' , fontsize=16)\n",
    "# plt.yticks(fontsize=12)\n",
    "plt.yticks([])\n",
    "plt.legend(['Point-wise Linear Degradation RUL', 'Actual Linear RUL', ],loc='best', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 保存\n",
    "fig.savefig('./plot/point-wise-linear-degradation.eps', dpi=600, format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.x",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}